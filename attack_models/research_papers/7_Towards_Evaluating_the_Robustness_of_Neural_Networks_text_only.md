# 7_Towards_Evaluating_the_Robustness_of_Neural_Networks

## Document Information

- **Source**: attack_models/research_papers/7_Towards_Evaluating_the_Robustness_of_Neural_Networks.pdf
- **Pages**: 19
- **Tables**: 26



## Page 1

Towards Evaluating the Robustness
of Neural Networks
Nicholas Carlini
David Wagner
University of California, Berkeley
ABSTRACT
Neural networks provide state-of-the-art results for most
machine learning tasks. Unfortunately, neural networks are
vulnerable to adversarial examples: given an input x and any
target classiﬁcation t, it is possible to ﬁnd a new input x′
that is similar to x but classiﬁed as t. This makes it difﬁcult
to apply neural networks in security-critical areas. Defensive
distillation is a recently proposed approach that can take an
arbitrary neural network, and increase its robustness, reducing
the success rate of current attacks’ ability to ﬁnd adversarial
examples from 95% to 0.5%.
In this paper, we demonstrate that defensive distillation does
not signiﬁcantly increase the robustness of neural networks
by introducing three new attack algorithms that are successful
on both distilled and undistilled neural networks with 100%
probability. Our attacks are tailored to three distance metrics
used previously in the literature, and when compared to pre-
vious adversarial example generation algorithms, our attacks
are often much more effective (and never worse). Furthermore,
we propose using high-conﬁdence adversarial examples in
a simple transferability test we show can also be used to
break defensive distillation. We hope our attacks will be used
as a benchmark in future defense attempts to create neural
networks that resist adversarial examples.
I. INTRODUCTION
Deep neural networks have become increasingly effective
at many difﬁcult machine-learning tasks. In the image recog-
nition domain, they are able to recognize images with near-
human accuracy [27], [25]. They are also used for speech
recognition [18], natural language processing [1], and playing
games [43], [32].
However, researchers have discovered that existing neural
networks are vulnerable to attack. Szegedy et al. [46] ﬁrst
noticed the existence of adversarial examples in the image
classiﬁcation domain: it is possible to transform an image by
a small amount and thereby change how the image is classiﬁed.
Often, the total amount of change required can be so small as
to be undetectable.
The degree to which attackers can ﬁnd adversarial examples
limits the domains in which neural networks can be used.
For example, if we use neural networks in self-driving cars,
adversarial examples could allow an attacker to cause the car
to take unwanted actions.
The existence of adversarial examples has inspired research
on how to harden neural networks against these kinds of
Original
Adversarial
Original
Adversarial
Fig. 1.
An illustration of our attacks on a defensively distilled network.
The leftmost column contains the starting image. The next three columns
show adversarial examples generated by our L2, L∞, and L0 algorithms,
respectively. All images start out classiﬁed correctly with label l, and the three
misclassiﬁed instances share the same misclassiﬁed label of l+1 (mod 10).
Images were chosen as the ﬁrst of their class from the test set.
attacks. Many early attempts to secure neural networks failed
or provided only marginal robustness improvements [15], [2],
[20], [42].
Defensive distillation [39] is one such recent defense pro-
posed for hardening neural networks against adversarial exam-
ples. Initial analysis proved to be very promising: defensive
distillation defeats existing attack algorithms and reduces their
success probability from 95% to 0.5%. Defensive distillation
can be applied to any feed-forward neural network and only
requires a single re-training step, and is currently one of
the only defenses giving strong security guarantees against
adversarial examples.
In general, there are two different approaches one can take
to evaluate the robustness of a neural network: attempt to prove
a lower bound, or construct attacks that demonstrate an upper
bound. The former approach, while sound, is substantially
more difﬁcult to implement in practice, and all attempts have
required approximations [2], [21]. On the other hand, if the
arXiv:1608.04644v2  [cs.CR]  22 Mar 2017


**Table 1 from page 1**

| 0                                                               | 1                                                                                  |
|:----------------------------------------------------------------|:-----------------------------------------------------------------------------------|
|                                                                 | University of California, Berkeley                                                 |
| ABSTRACT                                                        | Original                                                                           |
|                                                                 | Adversarial                                                                        |
|                                                                 | Original                                                                           |
|                                                                 | Adversarial                                                                        |
| Neural                                                          |                                                                                    |
| networks                                                        |                                                                                    |
| provide                                                         |                                                                                    |
| state-of-the-art                                                |                                                                                    |
| results                                                         |                                                                                    |
| for most                                                        |                                                                                    |
| machine                                                         |                                                                                    |
| learning                                                        |                                                                                    |
| tasks. Unfortunately,                                           |                                                                                    |
| neural                                                          |                                                                                    |
| networks                                                        |                                                                                    |
| are                                                             |                                                                                    |
| vulnerable to adversarial examples: given an input x and any    |                                                                                    |
| target                                                          |                                                                                    |
| classiﬁcation t,                                                |                                                                                    |
| it                                                              |                                                                                    |
| is possible                                                     |                                                                                    |
| to ﬁnd a new input x(cid:48)                                    |                                                                                    |
| that                                                            |                                                                                    |
| is similar                                                      |                                                                                    |
| to x but classiﬁed as t. This makes it difﬁcult                 |                                                                                    |
| to apply neural networks in security-critical areas. Defensive  |                                                                                    |
| distillation is a recently proposed approach that can take an   |                                                                                    |
| arbitrary neural network, and increase its robustness, reducing |                                                                                    |
| the success rate of current attacks’ ability to ﬁnd adversarial |                                                                                    |
| examples from 95% to 0.5%.                                      |                                                                                    |
| In this paper, we demonstrate that defensive distillation does  |                                                                                    |
| not                                                             |                                                                                    |
| signiﬁcantly increase                                           |                                                                                    |
| the                                                             |                                                                                    |
| robustness of neural networks                                   |                                                                                    |
| by introducing three new attack algorithms that are successful  |                                                                                    |
| on both distilled and undistilled neural networks with 100%     |                                                                                    |
| probability. Our attacks are tailored to three distance metrics |                                                                                    |
| used previously in the literature, and when compared to pre-    |                                                                                    |
| vious adversarial example generation algorithms, our attacks    |                                                                                    |
| are often much more effective (and never worse). Furthermore,   |                                                                                    |
| we                                                              |                                                                                    |
| propose                                                         |                                                                                    |
| using                                                           |                                                                                    |
| high-conﬁdence                                                  |                                                                                    |
| adversarial                                                     |                                                                                    |
| examples                                                        |                                                                                    |
| in                                                              |                                                                                    |
| a                                                               |                                                                                    |
| simple                                                          |                                                                                    |
| transferability                                                 |                                                                                    |
| test we                                                         |                                                                                    |
| show can                                                        |                                                                                    |
| also                                                            |                                                                                    |
| be                                                              |                                                                                    |
| used                                                            |                                                                                    |
| to                                                              |                                                                                    |
|                                                                 | Fig. 1.                                                                            |
|                                                                 | An illustration of our                                                             |
|                                                                 | attacks on a defensively distilled network.                                        |
| break defensive distillation. We hope our attacks will be used  |                                                                                    |
|                                                                 | The                                                                                |
|                                                                 | leftmost                                                                           |
|                                                                 | column contains                                                                    |
|                                                                 | the                                                                                |
|                                                                 | starting image. The next                                                           |
|                                                                 | three                                                                              |
|                                                                 | columns                                                                            |
| as                                                              | algorithms,                                                                        |
| a benchmark in future defense                                   | show adversarial examples generated by our L2, L∞, and L0                          |
| attempts                                                        |                                                                                    |
| to create neural                                                |                                                                                    |
|                                                                 | respectively. All images start out classiﬁed correctly with label l, and the three |
| networks that                                                   |                                                                                    |
| resist adversarial examples.                                    |                                                                                    |
|                                                                 | misclassiﬁed instances share the same misclassiﬁed label of l + 1 (mod 10).        |
|                                                                 | Images were chosen as the ﬁrst of                                                  |
|                                                                 | their class from the test set.                                                     |
| I.                                                              |                                                                                    |
| INTRODUCTION                                                    |                                                                                    |
| Deep neural networks have become increasingly effective         |                                                                                    |
| at many difﬁcult machine-learning tasks.                        |                                                                                    |
| In the image recog-                                             |                                                                                    |
|                                                                 | attacks. Many early attempts to secure neural networks failed                      |
| nition domain,                                                  |                                                                                    |
| they are able to recognize images with near-                    |                                                                                    |
|                                                                 | or provided only marginal robustness improvements [15], [2],                       |
| human                                                           |                                                                                    |
| accuracy                                                        |                                                                                    |
| [27],                                                           |                                                                                    |
| [25]. They                                                      |                                                                                    |
| are                                                             |                                                                                    |
| also                                                            |                                                                                    |
| used                                                            |                                                                                    |
| for                                                             |                                                                                    |
| speech                                                          |                                                                                    |
|                                                                 | [20],                                                                              |
|                                                                 | [42].                                                                              |
| recognition [18], natural                                       |                                                                                    |
| language processing [1], and playing                            |                                                                                    |
|                                                                 | Defensive distillation [39]                                                        |
|                                                                 | is one such recent defense pro-                                                    |
| games [43],                                                     |                                                                                    |
| [32].                                                           |                                                                                    |
| However,                                                        | posed for hardening neural networks against adversarial exam-                      |
| researchers have discovered that existing neural                |                                                                                    |
| networks                                                        | ples.                                                                              |
| are vulnerable                                                  | Initial analysis proved to be very promising: defensive                            |
| to attack. Szegedy et al.                                       |                                                                                    |
| [46] ﬁrst                                                       |                                                                                    |
| examples                                                        | distillation defeats existing attack algorithms and reduces their                  |
| noticed the                                                     |                                                                                    |
| existence of adversarial                                        |                                                                                    |
| in the                                                          |                                                                                    |
| image                                                           |                                                                                    |
| classiﬁcation domain:                                           | success probability from 95% to 0.5%. Defensive distillation                       |
| it                                                              |                                                                                    |
| is possible to transform an image by                            |                                                                                    |
| a small amount and thereby change how the image is classiﬁed.   | can be applied to any feed-forward neural network and only                         |
| Often,                                                          | requires                                                                           |
| the total amount of change required can be so small as          | a                                                                                  |
|                                                                 | single                                                                             |
|                                                                 | re-training                                                                        |
|                                                                 | step,                                                                              |
|                                                                 | and                                                                                |
|                                                                 | is                                                                                 |
|                                                                 | currently                                                                          |
|                                                                 | one                                                                                |
|                                                                 | of                                                                                 |
| to be undetectable.                                             | the only defenses giving strong security guarantees                                |
|                                                                 | against                                                                            |
| The degree to which attackers can ﬁnd adversarial examples      | adversarial examples.                                                              |
| limits                                                          | In general,                                                                        |
| the                                                             | there are two different approaches one can take                                    |
| domains                                                         |                                                                                    |
| in which                                                        |                                                                                    |
| neural                                                          |                                                                                    |
| networks                                                        |                                                                                    |
| can                                                             |                                                                                    |
| be                                                              |                                                                                    |
| used.                                                           |                                                                                    |
| For example,                                                    | to evaluate the robustness of a neural network: attempt to prove                   |
| if we use neural networks                                       |                                                                                    |
| in self-driving cars,                                           |                                                                                    |
| adversarial examples could allow an attacker                    | a lower bound, or construct attacks that demonstrate an upper                      |
| to cause the car                                                |                                                                                    |
| to take unwanted actions.                                       | bound. The                                                                         |
|                                                                 | former                                                                             |
|                                                                 | approach, while                                                                    |
|                                                                 | sound,                                                                             |
|                                                                 | is                                                                                 |
|                                                                 | substantially                                                                      |
| The existence of adversarial examples has inspired research     | more difﬁcult                                                                      |
|                                                                 | to implement                                                                       |
|                                                                 | in practice, and all attempts have                                                 |
| on                                                              | required approximations                                                            |
| how to                                                          | [2],                                                                               |
| harden                                                          | [21]. On the other hand,                                                           |
| neural                                                          | if                                                                                 |
| networks                                                        | the                                                                                |
| against                                                         |                                                                                    |
| these                                                           |                                                                                    |
| kinds                                                           |                                                                                    |
| of                                                              |                                                                                    |



## Page 2

attacks used in the the latter approach are not sufﬁciently
strong and fail often, the upper bound may not be useful.
In this paper we create a set of attacks that can be used
to construct an upper bound on the robustness of neural
networks. As a case study, we use these attacks to demon-
strate that defensive distillation does not actually eliminate
adversarial examples. We construct three new attacks (under
three previously used distance metrics: L0, L2, and L∞) that
succeed in ﬁnding adversarial examples for 100% of images
on defensively distilled networks. While defensive distillation
stops previously published attacks, it cannot resist the more
powerful attack techniques we introduce in this paper.
This case study illustrates the general need for better
techniques to evaluate the robustness of neural networks:
while distillation was shown to be secure against the current
state-of-the-art attacks, it fails against our stronger attacks.
Furthermore, when comparing our attacks against the current
state-of-the-art on standard unsecured models, our methods
generate adversarial examples with less total distortion in
every case. We suggest that our attacks are a better baseline
for evaluating candidate defenses: before placing any faith in a
new possible defense, we suggest that designers at least check
whether it can resist our attacks.
We additionally propose using high-conﬁdence adversarial
examples to evaluate the robustness of defenses. Transfer-
ability [46], [11] is the well-known property that adversarial
examples on one model are often also adversarial on another
model. We demonstrate that adversarial examples from our
attacks are transferable from the unsecured model to the
defensively distilled (secured) model. In general, we argue
that any defense must demonstrate it is able to break the
transferability property.
We evaluate our attacks on three standard datasets: MNIST
[28], a digit-recognition task (0-9); CIFAR-10 [24], a small-
image recognition task, also with 10 classes; and ImageNet
[9], a large-image recognition task with 1000 classes.
Figure 1 shows examples of adversarial examples our tech-
niques generate on defensively distilled networks trained on
the MNIST and CIFAR datasets.
In one extreme example for the ImageNet classiﬁcation task,
we can cause the Inception v3 [45] network to incorrectly
classify images by changing only the lowest order bit of each
pixel. Such changes are impossible to detect visually.
To enable others to more easily use our work to evaluate
the robustness of other defenses, all of our adversarial example
generation algorithms (along with code to train the models we
use, to reproduce the results we present) are available online
at http://nicholas.carlini.com/code/nn robust attacks.
This paper makes the following contributions:
• We introduce three new attacks for the L0, L2, and L∞
distance metrics. Our attacks are signiﬁcantly more effec-
tive than previous approaches. Our L0 attack is the ﬁrst
published attack that can cause targeted misclassiﬁcation
on the ImageNet dataset.
• We apply these attacks to defensive distillation and dis-
cover that distillation provides little security beneﬁt over
un-distilled networks.
• We propose using high-conﬁdence adversarial examples
in a simple transferability test to evaluate defenses, and
show this test breaks defensive distillation.
• We systematically evaluate the choice of the objective
function for ﬁnding adversarial examples, and show that
the choice can dramatically impact the efﬁcacy of an
attack.
II. BACKGROUND
A. Threat Model
Machine learning is being used in an increasing array of
settings to make potentially security critical decisions: self-
driving cars [3], [4], drones [10], robots [33], [22], anomaly
detection [6], malware classiﬁcation [8], [40], [48], speech
recognition and recognition of voice commands [17], [13],
NLP [1], and many more. Consequently, understanding the
security properties of deep learning has become a crucial
question in this area. The extent to which we can construct
adversarial examples inﬂuences the settings in which we may
want to (or not want to) use neural networks.
In the speech recognition domain, recent work has shown
[5] it is possible to generate audio that sounds like speech to
machine learning algorithms but not to humans. This can be
used to control user’s devices without their knowledge. For
example, by playing a video with a hidden voice command,
it may be possible to cause a smart phone to visit a malicious
webpage to cause a drive-by download. This work focused
on conventional techniques (Gaussian Mixture Models and
Hidden Markov Models), but as speech recognition is increas-
ingly using neural networks, the study of adversarial examples
becomes relevant in this domain. 1
In the space of malware classiﬁcation, the existence of
adversarial examples not only limits their potential application
settings, but entirely defeats its purpose: an adversary who is
able to make only slight modiﬁcations to a malware ﬁle that
cause it to remain malware, but become classiﬁed as benign,
has entirely defeated the malware classiﬁer [8], [14].
Turning back to the threat to self-driving cars introduced
earlier, this is not an unrealistic attack: it has been shown that
adversarial examples are possible in the physical world [26]
after taking pictures of them.
The key question then becomes exactly how much distortion
we must add to cause the classiﬁcation to change. In each
domain, the distance metric that we must use is different. In
the space of images, which we focus on in this paper, we
rely on previous work that suggests that various Lp norms are
reasonable approximations of human perceptual distance (see
Section II-D for more information).
We assume in this paper that the adversary has complete
access to a neural network, including the architecture and all
paramaters, and can use this in a white-box manner. This is a
conservative and realistic assumption: prior work has shown it
1Strictly speaking, hidden voice commands are not adversarial examples
because they are not similar to the original input [5].
2


**Table 2 from page 2**

| 0                                                                | 1                                                                      |
|:-----------------------------------------------------------------|:-----------------------------------------------------------------------|
| attacks                                                          | un-distilled networks.                                                 |
| used                                                             |                                                                        |
| in                                                               |                                                                        |
| the                                                              |                                                                        |
| the                                                              |                                                                        |
| latter                                                           |                                                                        |
| approach                                                         |                                                                        |
| are                                                              |                                                                        |
| not                                                              |                                                                        |
| sufﬁciently                                                      |                                                                        |
| strong and fail often,                                           | • We propose using high-conﬁdence adversarial examples                 |
| the upper bound may not be useful.                               |                                                                        |
| In this paper we                                                 | in a simple transferability test                                       |
| create a set of attacks                                          | to evaluate defenses, and                                              |
| that can be used                                                 |                                                                        |
| to                                                               | show this test breaks defensive distillation.                          |
| construct                                                        |                                                                        |
| an                                                               |                                                                        |
| upper                                                            |                                                                        |
| bound                                                            |                                                                        |
| on                                                               |                                                                        |
| the                                                              |                                                                        |
| robustness                                                       |                                                                        |
| of                                                               |                                                                        |
| neural                                                           |                                                                        |
| networks. As                                                     | • We                                                                   |
| a                                                                | systematically evaluate                                                |
| case                                                             | the                                                                    |
| study, we use                                                    | choice of                                                              |
| these                                                            | the objective                                                          |
| attacks                                                          |                                                                        |
| to demon-                                                        |                                                                        |
| strate                                                           | function for ﬁnding adversarial examples, and show that                |
| that defensive distillation does not                             |                                                                        |
| actually eliminate                                               |                                                                        |
| adversarial examples. We construct                               | the                                                                    |
| three new attacks                                                | choice                                                                 |
| (under                                                           | can                                                                    |
|                                                                  | dramatically                                                           |
|                                                                  | impact                                                                 |
|                                                                  | the                                                                    |
|                                                                  | efﬁcacy                                                                |
|                                                                  | of                                                                     |
|                                                                  | an                                                                     |
| that                                                             | attack.                                                                |
| three previously used distance metrics: L0, L2, and L∞)          |                                                                        |
| succeed in ﬁnding adversarial examples                           |                                                                        |
| for 100% of                                                      |                                                                        |
| images                                                           |                                                                        |
|                                                                  | II. BACKGROUND                                                         |
| on defensively distilled networks. While defensive distillation  |                                                                        |
| stops previously published attacks,                              | A. Threat Model                                                        |
| it cannot                                                        |                                                                        |
| resist                                                           |                                                                        |
| the more                                                         |                                                                        |
| powerful attack techniques we introduce in this paper.           |                                                                        |
|                                                                  | Machine learning is being used in an increasing array of               |
| This                                                             |                                                                        |
| case                                                             |                                                                        |
| study                                                            |                                                                        |
| illustrates                                                      |                                                                        |
| the                                                              |                                                                        |
| general                                                          |                                                                        |
| need                                                             |                                                                        |
| for                                                              |                                                                        |
| better                                                           |                                                                        |
|                                                                  | settings                                                               |
|                                                                  | to make potentially security critical decisions:                       |
|                                                                  | self-                                                                  |
| techniques                                                       |                                                                        |
| to                                                               |                                                                        |
| evaluate                                                         |                                                                        |
| the                                                              |                                                                        |
| robustness                                                       |                                                                        |
| of                                                               |                                                                        |
| neural                                                           |                                                                        |
| networks:                                                        |                                                                        |
|                                                                  | driving cars                                                           |
|                                                                  | [3],                                                                   |
|                                                                  | [4], drones                                                            |
|                                                                  | [10],                                                                  |
|                                                                  | robots                                                                 |
|                                                                  | [33],                                                                  |
|                                                                  | [22], anomaly                                                          |
| while distillation was shown to be secure against                |                                                                        |
| the current                                                      |                                                                        |
|                                                                  | detection                                                              |
|                                                                  | [6], malware                                                           |
|                                                                  | classiﬁcation                                                          |
|                                                                  | [8],                                                                   |
|                                                                  | [40],                                                                  |
|                                                                  | [48],                                                                  |
|                                                                  | speech                                                                 |
| state-of-the-art                                                 |                                                                        |
| attacks,                                                         |                                                                        |
| it                                                               |                                                                        |
| fails                                                            |                                                                        |
| against                                                          |                                                                        |
| our                                                              |                                                                        |
| stronger                                                         |                                                                        |
| attacks.                                                         |                                                                        |
|                                                                  | recognition                                                            |
|                                                                  | and                                                                    |
|                                                                  | recognition                                                            |
|                                                                  | of                                                                     |
|                                                                  | voice                                                                  |
|                                                                  | commands                                                               |
|                                                                  | [17],                                                                  |
|                                                                  | [13],                                                                  |
| Furthermore, when comparing our attacks against                  |                                                                        |
| the current                                                      |                                                                        |
|                                                                  | NLP [1],                                                               |
|                                                                  | and many more. Consequently, understanding the                         |
| state-of-the-art on standard unsecured models, our methods       |                                                                        |
|                                                                  | security                                                               |
|                                                                  | properties                                                             |
|                                                                  | of                                                                     |
|                                                                  | deep                                                                   |
|                                                                  | learning                                                               |
|                                                                  | has                                                                    |
|                                                                  | become                                                                 |
|                                                                  | a                                                                      |
|                                                                  | crucial                                                                |
| generate                                                         |                                                                        |
| adversarial                                                      |                                                                        |
| examples with                                                    |                                                                        |
| less                                                             |                                                                        |
| total                                                            |                                                                        |
| distortion                                                       |                                                                        |
| in                                                               |                                                                        |
|                                                                  | question in this area. The extent                                      |
|                                                                  | to which we can construct                                              |
| every case. We suggest                                           |                                                                        |
| that our attacks are a better baseline                           |                                                                        |
|                                                                  | adversarial examples inﬂuences the settings in which we may            |
| for evaluating candidate defenses: before placing any faith in a |                                                                        |
|                                                                  | want                                                                   |
|                                                                  | to (or not want                                                        |
|                                                                  | to) use neural networks.                                               |
| new possible defense, we suggest                                 |                                                                        |
| that designers at                                                |                                                                        |
| least check                                                      |                                                                        |
|                                                                  | In the speech recognition domain,                                      |
|                                                                  | recent work has                                                        |
|                                                                  | shown                                                                  |
| whether                                                          |                                                                        |
| it can resist our attacks.                                       |                                                                        |
|                                                                  | [5]                                                                    |
|                                                                  | it                                                                     |
|                                                                  | is possible to generate audio that sounds like speech to               |
| We additionally propose using high-conﬁdence adversarial         |                                                                        |
|                                                                  | machine learning algorithms but not                                    |
|                                                                  | to humans. This can be                                                 |
| examples                                                         |                                                                        |
| to                                                               |                                                                        |
| evaluate                                                         |                                                                        |
| the                                                              |                                                                        |
| robustness                                                       |                                                                        |
| of                                                               |                                                                        |
| defenses. Transfer-                                              |                                                                        |
|                                                                  | used to control user’s devices without                                 |
|                                                                  | their knowledge. For                                                   |
| ability [46],                                                    |                                                                        |
| [11]                                                             |                                                                        |
| is                                                               |                                                                        |
| the well-known property that adversarial                         |                                                                        |
|                                                                  | example, by playing a video with a hidden voice command,               |
| examples on one model are often also adversarial on another      |                                                                        |
|                                                                  | it may be possible to cause a smart phone to visit a malicious         |
| model. We demonstrate                                            |                                                                        |
| that                                                             |                                                                        |
| adversarial                                                      |                                                                        |
| examples                                                         |                                                                        |
| from our                                                         |                                                                        |
|                                                                  | webpage                                                                |
|                                                                  | to cause                                                               |
|                                                                  | a drive-by download. This work focused                                 |
| attacks                                                          |                                                                        |
| are                                                              |                                                                        |
| transferable                                                     |                                                                        |
| from the                                                         |                                                                        |
| unsecured model                                                  |                                                                        |
| to                                                               |                                                                        |
| the                                                              |                                                                        |
|                                                                  | on                                                                     |
|                                                                  | conventional                                                           |
|                                                                  | techniques                                                             |
|                                                                  | (Gaussian Mixture Models                                               |
|                                                                  | and                                                                    |
| defensively                                                      |                                                                        |
| distilled                                                        |                                                                        |
| (secured) model.                                                 |                                                                        |
| In                                                               |                                                                        |
| general, we                                                      |                                                                        |
| argue                                                            |                                                                        |
|                                                                  | Hidden Markov Models), but as speech recognition is increas-           |
| that                                                             |                                                                        |
| any                                                              |                                                                        |
| defense must                                                     |                                                                        |
| demonstrate                                                      |                                                                        |
| it                                                               |                                                                        |
| is                                                               |                                                                        |
| able                                                             |                                                                        |
| to                                                               |                                                                        |
| break                                                            |                                                                        |
| the                                                              |                                                                        |
|                                                                  | ingly using neural networks, the study of adversarial examples         |
| transferability property.                                        |                                                                        |
|                                                                  | becomes relevant                                                       |
|                                                                  | in this domain. 1                                                      |
| We evaluate our attacks on three standard datasets: MNIST        |                                                                        |
|                                                                  | In                                                                     |
|                                                                  | the                                                                    |
|                                                                  | space                                                                  |
|                                                                  | of malware                                                             |
|                                                                  | classiﬁcation,                                                         |
|                                                                  | the                                                                    |
|                                                                  | existence                                                              |
|                                                                  | of                                                                     |
| [28], a digit-recognition task (0-9); CIFAR-10 [24], a small-    |                                                                        |
|                                                                  | adversarial examples not only limits their potential application       |
| image recognition task, also with 10 classes; and ImageNet       |                                                                        |
|                                                                  | settings, but entirely defeats its purpose: an adversary who is        |
| [9], a large-image recognition task with 1000 classes.           |                                                                        |
|                                                                  | able to make only slight modiﬁcations to a malware ﬁle that            |
| Figure 1 shows examples of adversarial examples our tech-        |                                                                        |
|                                                                  | cause it                                                               |
|                                                                  | to remain malware, but become classiﬁed as benign,                     |
| niques generate on defensively distilled networks                |                                                                        |
| trained on                                                       |                                                                        |
|                                                                  | has entirely defeated the malware classiﬁer                            |
|                                                                  | [8],                                                                   |
|                                                                  | [14].                                                                  |
| the MNIST and CIFAR datasets.                                    |                                                                        |
|                                                                  | Turning back to the                                                    |
|                                                                  | threat                                                                 |
|                                                                  | to self-driving cars                                                   |
|                                                                  | introduced                                                             |
| In one extreme example for the ImageNet classiﬁcation task,      |                                                                        |
|                                                                  | earlier,                                                               |
|                                                                  | this is not an unrealistic attack:                                     |
|                                                                  | it has been shown that                                                 |
| we                                                               |                                                                        |
| can cause                                                        |                                                                        |
| the                                                              |                                                                        |
| Inception v3 [45] network to incorrectly                         |                                                                        |
|                                                                  | adversarial examples are possible in the physical world [26]           |
| classify images by changing only the lowest order bit of each    |                                                                        |
|                                                                  | after                                                                  |
|                                                                  | taking pictures of                                                     |
|                                                                  | them.                                                                  |
| pixel. Such changes are impossible to detect visually.           |                                                                        |
|                                                                  | The key question then becomes exactly how much distortion              |
| To enable others                                                 |                                                                        |
| to more easily use our work to evaluate                          |                                                                        |
|                                                                  | we must                                                                |
|                                                                  | add to cause                                                           |
|                                                                  | the                                                                    |
|                                                                  | classiﬁcation to change.                                               |
|                                                                  | In each                                                                |
| the robustness of other defenses, all of our adversarial example |                                                                        |
|                                                                  | domain,                                                                |
|                                                                  | the distance metric that we must use is different.                     |
|                                                                  | In                                                                     |
| generation algorithms (along with code to train the models we    |                                                                        |
|                                                                  | the                                                                    |
|                                                                  | space of                                                               |
|                                                                  | images, which we                                                       |
|                                                                  | focus on in this paper, we                                             |
| use,                                                             |                                                                        |
| to reproduce the results we present) are available online        |                                                                        |
|                                                                  | rely on previous work that suggests that various Lp norms are          |
| attacks.                                                         |                                                                        |
| at http://nicholas.carlini.com/code/nn robust                    |                                                                        |
|                                                                  | reasonable approximations of human perceptual distance (see            |
| This paper makes the following contributions:                    |                                                                        |
|                                                                  | Section II-D for more information).                                    |
| • We introduce three new attacks for                             |                                                                        |
| the L0, L2, and L∞                                               |                                                                        |
|                                                                  | We assume in this paper                                                |
|                                                                  | that                                                                   |
|                                                                  | the adversary has complete                                             |
| distance metrics. Our attacks are signiﬁcantly more effec-       |                                                                        |
|                                                                  | access to a neural network,                                            |
|                                                                  | including the architecture and all                                     |
| tive than previous approaches. Our L0 attack is the ﬁrst         |                                                                        |
|                                                                  | paramaters, and can use this in a white-box manner. This is a          |
| published attack that can cause targeted misclassiﬁcation        |                                                                        |
|                                                                  | conservative and realistic assumption: prior work has shown it         |
| on the ImageNet dataset.                                         |                                                                        |
| • We apply these attacks to defensive distillation and dis-      |                                                                        |
|                                                                  | 1Strictly speaking, hidden voice commands are not adversarial examples |
| cover that distillation provides little security beneﬁt over     | because they are not similar                                           |
|                                                                  | to the original                                                        |
|                                                                  | input                                                                  |
|                                                                  | [5].                                                                   |



## Page 3

is possible to train a substitute model given black-box access
to a target model, and by attacking the substitute model, we
can then transfer these attacks to the target model. [37]
Given these threats, there have been various attempts [15],
[2], [20], [42], [39] at constructing defenses that increase the
robustness of a neural network, deﬁned as a measure of how
easy it is to ﬁnd adversarial examples that are close to their
original input.
In this paper we study one of these, distillation as a defense
[39], that hopes to secure an arbitrary neural network. This
type of defensive distillation was shown to make generating
adversarial examples nearly impossible for existing attack
techniques [39]. We ﬁnd that although the current state-of-the-
art fails to ﬁnd adversarial examples for defensively distilled
networks, the stronger attacks we develop in this paper are
able to construct adversarial examples.
B. Neural Networks and Notation
A neural network is a function F(x) = y that accepts an
input x ∈Rn and produces an output y ∈Rm. The model F
also implicitly depends on some model parameters θ; in our
work the model is ﬁxed, so for convenience we don’t show
the dependence on θ.
In this paper we focus on neural networks used as an m-
class classiﬁer. The output of the network is computed using
the softmax function, which ensures that the output vector y
satisﬁes 0 ≤yi ≤1 and y1+· · ·+ym = 1. The output vector y
is thus treated as a probability distribution, i.e., yi is treated as
the probability that input x has class i. The classiﬁer assigns
the label C(x) = arg maxi F(x)i to the input x. Let C∗(x)
be the correct label of x. The inputs to the softmax function
are called logits.
We use the notation from Papernot et al. [39]: deﬁne F
to be the full neural network including the softmax function,
Z(x) = z to be the output of all layers except the softmax (so
z are the logits), and
F(x) = softmax(Z(x)) = y.
A neural network typically 2 consists of layers
F = softmax ◦Fn ◦Fn−1 ◦· · · ◦F1
where
Fi(x) = σ(θi · x) + ˆθi
for some non-linear activation function σ, some matrix θi of
model weights, and some vector ˆθi of model biases. Together
θ and ˆθ make up the model parameters. Common choices of σ
are tanh [31], sigmoid, ReLU [29], or ELU [7]. In this paper
we focus primarily on networks that use a ReLU activation
function, as it currently is the most widely used activation
function [45], [44], [31], [39].
We use image classiﬁcation as our primary evaluation
domain. An h×w-pixel grey-scale image is a two-dimensional
2Most simple networks have this simple linear structure, however other
more sophisticated networks have more complicated structures (e.g., ResNet
[16] and Inception [45]). The network architecture does not impact our attacks.
vector x ∈Rhw, where xi denotes the intensity of pixel i
and is scaled to be in the range [0, 1]. A color RGB image
is a three-dimensional vector x ∈R3hw. We do not convert
RGB images to HSV, HSL, or other cylindrical coordinate
representations of color images: the neural networks act on
raw pixel values.
C. Adversarial Examples
Szegedy et al. [46] ﬁrst pointed out the existence of
adversarial examples: given a valid input x and a target
t ̸= C∗(x), it is often possible to ﬁnd a similar input x′
such that C(x′) = t yet x, x′ are close according to some
distance metric. An example x′ with this property is known
as a targeted adversarial example.
A less powerful attack also discussed in the literature
instead asks for untargeted adversarial examples: instead of
classifying x as a given target class, we only search for an
input x′ so that C(x′) ̸= C∗(x) and x, x′ are close. Untargeted
attacks are strictly less powerful than targeted attacks and we
do not consider them in this paper. 3
Instead, we consider three different approaches for how to
choose the target class, in a targeted attack:
• Average Case: select the target class uniformly at random
among the labels that are not the correct label.
• Best Case: perform the attack against all incorrect classes,
and report the target class that was least difﬁcult to attack.
• Worst Case: perform the attack against all incorrect
classes, and report the target class that was most difﬁcult
to attack.
In all of our evaluations we perform all three types of
attacks: best-case, average-case, and worst-case. Notice that
if a classiﬁer is only accurate 80% of the time, then the best
case attack will require a change of 0 in 20% of cases.
On ImageNet, we approximate the best-case and worst-case
attack by sampling 100 random target classes out of the 1,000
possible for efﬁciency reasons.
D. Distance Metrics
In our deﬁnition of adversarial examples, we require use
of a distance metric to quantify similarity. There are three
widely-used distance metrics in the literature for generating
adversarial examples, all of which are Lp norms.
The Lp distance is written ∥x −x′∥p, where the p-norm
∥· ∥p is deﬁned as
∥v∥p =
 n
X
i=1
|vi|p
! 1
p
.
In more detail:
3An untargeted attack is simply a more efﬁcient (and often less accurate)
method of running a targeted attack for each target and taking the closest.
In this paper we focus on identifying the most accurate attacks, and do not
consider untargeted attacks.
3


**Table 3 from page 3**

| 0                                                                               | 1                                                                                            |
|:--------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------|
| is possible to train a substitute model given black-box access                  | i                                                                                            |
|                                                                                 | denotes                                                                                      |
|                                                                                 | the                                                                                          |
|                                                                                 | intensity of pixel                                                                           |
|                                                                                 | vector x ∈ Rhw, where xi                                                                     |
| to a target model, and by attacking the substitute model, we                    | and is                                                                                       |
|                                                                                 | scaled to be in the range [0, 1]. A color RGB image                                          |
| can then transfer                                                               | is a three-dimensional vector x ∈ R3hw. We do not convert                                    |
| these attacks to the target model.                                              |                                                                                              |
| [37]                                                                            |                                                                                              |
| Given these threats,                                                            | RGB images                                                                                   |
| there have been various attempts [15],                                          | to HSV, HSL, or other                                                                        |
|                                                                                 | cylindrical                                                                                  |
|                                                                                 | coordinate                                                                                   |
| [2],                                                                            | representations of                                                                           |
| [20],                                                                           | color                                                                                        |
| [42],                                                                           | images:                                                                                      |
| [39] at constructing defenses that                                              | the neural networks                                                                          |
| increase the                                                                    | act on                                                                                       |
| robustness of a neural network, deﬁned as a measure of how                      | raw pixel values.                                                                            |
| easy it                                                                         |                                                                                              |
| is                                                                              |                                                                                              |
| to ﬁnd adversarial examples                                                     |                                                                                              |
| that are close to their                                                         |                                                                                              |
| original                                                                        |                                                                                              |
| input.                                                                          |                                                                                              |
|                                                                                 | C. Adversarial Examples                                                                      |
| In this paper we study one of these, distillation as a defense                  |                                                                                              |
|                                                                                 | et                                                                                           |
|                                                                                 | al.                                                                                          |
|                                                                                 | Szegedy                                                                                      |
|                                                                                 | [46]                                                                                         |
|                                                                                 | ﬁrst                                                                                         |
|                                                                                 | pointed                                                                                      |
|                                                                                 | out                                                                                          |
|                                                                                 | the                                                                                          |
|                                                                                 | existence                                                                                    |
|                                                                                 | of                                                                                           |
| [39],                                                                           |                                                                                              |
| that hopes                                                                      |                                                                                              |
| to secure an arbitrary neural network. This                                     |                                                                                              |
|                                                                                 | adversarial                                                                                  |
|                                                                                 | x                                                                                            |
|                                                                                 | examples:                                                                                    |
|                                                                                 | given                                                                                        |
|                                                                                 | a                                                                                            |
|                                                                                 | valid                                                                                        |
|                                                                                 | input                                                                                        |
|                                                                                 | and                                                                                          |
|                                                                                 | a                                                                                            |
|                                                                                 | target                                                                                       |
| type of defensive distillation was                                              |                                                                                              |
| shown to make generating                                                        |                                                                                              |
|                                                                                 | t                                                                                            |
|                                                                                 | (cid:54)= C ∗(x),                                                                            |
|                                                                                 | it                                                                                           |
|                                                                                 | is                                                                                           |
|                                                                                 | often                                                                                        |
|                                                                                 | possible                                                                                     |
|                                                                                 | to ﬁnd                                                                                       |
|                                                                                 | a                                                                                            |
|                                                                                 | similar                                                                                      |
|                                                                                 | input x(cid:48)                                                                              |
| adversarial                                                                     |                                                                                              |
| examples                                                                        |                                                                                              |
| nearly                                                                          |                                                                                              |
| impossible                                                                      |                                                                                              |
| for                                                                             |                                                                                              |
| existing                                                                        |                                                                                              |
| attack                                                                          |                                                                                              |
|                                                                                 | such that C(x(cid:48)) = t yet x, x(cid:48)                                                  |
|                                                                                 | are                                                                                          |
|                                                                                 | close                                                                                        |
|                                                                                 | according to some                                                                            |
| techniques [39]. We ﬁnd that although the current state-of-the-                 |                                                                                              |
|                                                                                 | distance metric. An example x(cid:48) with this property is known                            |
| art                                                                             |                                                                                              |
| fails to ﬁnd adversarial examples for defensively distilled                     |                                                                                              |
|                                                                                 | as a targeted adversarial example.                                                           |
| networks,                                                                       |                                                                                              |
| the stronger attacks we develop in this paper are                               |                                                                                              |
|                                                                                 | A less                                                                                       |
|                                                                                 | powerful                                                                                     |
|                                                                                 | attack                                                                                       |
|                                                                                 | also                                                                                         |
|                                                                                 | discussed                                                                                    |
|                                                                                 | in                                                                                           |
|                                                                                 | the                                                                                          |
|                                                                                 | literature                                                                                   |
| able to construct adversarial examples.                                         |                                                                                              |
|                                                                                 | instead asks                                                                                 |
|                                                                                 | for untargeted adversarial examples:                                                         |
|                                                                                 | instead of                                                                                   |
| B. Neural Networks and Notation                                                 |                                                                                              |
|                                                                                 | classifying x as a given target class, we only search for an                                 |
| A neural network is a function F (x) = y that accepts an                        | input x(cid:48) so that C(x(cid:48)) (cid:54)= C ∗(x) and x, x(cid:48) are close. Untargeted |
| input x ∈ Rn and produces an output y ∈ Rm. The model F                         | attacks are strictly less powerful                                                           |
|                                                                                 | than targeted attacks and we                                                                 |
| also implicitly depends on some model parameters θ;                             | do not consider                                                                              |
| in our                                                                          | them in this paper. 3                                                                        |
| work the model                                                                  | Instead, we consider                                                                         |
| is ﬁxed,                                                                        | three different approaches for how to                                                        |
| so for convenience we don’t                                                     |                                                                                              |
| show                                                                            |                                                                                              |
| the dependence on θ.                                                            | choose the target class,                                                                     |
|                                                                                 | in a targeted attack:                                                                        |
| In this paper we focus on neural networks used as an m-                         |                                                                                              |
|                                                                                 | • Average Case: select the target class uniformly at random                                  |
| class classiﬁer. The output of                                                  |                                                                                              |
| the network is computed using                                                   |                                                                                              |
|                                                                                 | among the labels that are not                                                                |
|                                                                                 | the correct                                                                                  |
|                                                                                 | label.                                                                                       |
| the softmax function, which ensures that                                        |                                                                                              |
| the output vector y                                                             |                                                                                              |
|                                                                                 | • Best Case: perform the attack against all incorrect classes,                               |
| satisﬁes 0 ≤ yi ≤ 1 and y1+· · ·+ym = 1. The output vector y                    |                                                                                              |
|                                                                                 | and report the target class that was least difﬁcult to attack.                               |
| is thus treated as a probability distribution,                                  |                                                                                              |
| is treated as                                                                   |                                                                                              |
| i.e., yi                                                                        |                                                                                              |
|                                                                                 | • Worst Case:                                                                                |
|                                                                                 | perform the                                                                                  |
|                                                                                 | attack                                                                                       |
|                                                                                 | against                                                                                      |
|                                                                                 | all                                                                                          |
|                                                                                 | incorrect                                                                                    |
| the probability that                                                            |                                                                                              |
| input x has class i. The classiﬁer assigns                                      |                                                                                              |
|                                                                                 | classes, and report                                                                          |
|                                                                                 | the target class that was most difﬁcult                                                      |
| to the input x. Let C ∗(x)                                                      |                                                                                              |
| the label C(x) = arg maxi F (x)i                                                |                                                                                              |
|                                                                                 | to attack.                                                                                   |
| be the correct                                                                  |                                                                                              |
| label of x. The inputs to the softmax function                                  |                                                                                              |
|                                                                                 | In                                                                                           |
|                                                                                 | all                                                                                          |
|                                                                                 | of                                                                                           |
|                                                                                 | our                                                                                          |
|                                                                                 | evaluations we                                                                               |
|                                                                                 | perform all                                                                                  |
|                                                                                 | three                                                                                        |
|                                                                                 | types                                                                                        |
|                                                                                 | of                                                                                           |
| are called logits.                                                              |                                                                                              |
|                                                                                 | attacks: best-case,                                                                          |
|                                                                                 | average-case,                                                                                |
|                                                                                 | and worst-case. Notice                                                                       |
|                                                                                 | that                                                                                         |
| We use                                                                          |                                                                                              |
| the notation from Papernot                                                      |                                                                                              |
| et                                                                              |                                                                                              |
| al.                                                                             |                                                                                              |
| [39]: deﬁne F                                                                   |                                                                                              |
|                                                                                 | if a classiﬁer                                                                               |
|                                                                                 | is only accurate 80% of                                                                      |
|                                                                                 | the time,                                                                                    |
|                                                                                 | then the best                                                                                |
| to be the full neural network including the softmax function,                   |                                                                                              |
|                                                                                 | case attack will                                                                             |
|                                                                                 | require a change of 0 in 20% of cases.                                                       |
| Z(x) = z to be the output of all                                                |                                                                                              |
| layers except                                                                   |                                                                                              |
| the softmax (so                                                                 |                                                                                              |
|                                                                                 | On ImageNet, we approximate the best-case and worst-case                                     |
| z are the logits), and                                                          |                                                                                              |
|                                                                                 | attack by sampling 100 random target classes out of the 1,000                                |
| F (x) = softmax(Z(x)) = y.                                                      | possible for efﬁciency reasons.                                                              |
| A neural network typically 2 consists of                                        |                                                                                              |
| layers                                                                          |                                                                                              |
|                                                                                 | D. Distance Metrics                                                                          |
| F = softmax ◦ Fn ◦ Fn−1 ◦ · · · ◦ F1                                            |                                                                                              |
|                                                                                 | In our deﬁnition of                                                                          |
|                                                                                 | adversarial                                                                                  |
|                                                                                 | examples, we                                                                                 |
|                                                                                 | require use                                                                                  |
| where                                                                           | of                                                                                           |
|                                                                                 | a distance metric                                                                            |
|                                                                                 | to quantify similarity. There                                                                |
|                                                                                 | are                                                                                          |
|                                                                                 | three                                                                                        |
| Fi(x) = σ(θi · x) + ˆθi                                                         | widely-used distance metrics                                                                 |
|                                                                                 | in the literature for generating                                                             |
|                                                                                 | adversarial examples, all of which are Lp norms.                                             |
| for some non-linear activation function σ, some matrix θi of                    |                                                                                              |
|                                                                                 | distance                                                                                     |
|                                                                                 | the p-norm                                                                                   |
|                                                                                 | The Lp                                                                                       |
|                                                                                 | is written (cid:107)x − x(cid:48)(cid:107)p, where                                           |
| model weights, and some vector ˆθi of model biases. Together                    |                                                                                              |
|                                                                                 | (cid:107) · (cid:107)p is deﬁned as                                                          |
| θ and ˆθ make up the model parameters. Common choices of σ                      |                                                                                              |
| are tanh [31], sigmoid, ReLU [29], or ELU [7].                                  |                                                                                              |
| In this paper                                                                   |                                                                                              |
|                                                                                 | (cid:32) n                                                                                   |
|                                                                                 | (cid:33) 1                                                                                   |
| we focus primarily on networks                                                  |                                                                                              |
| that use a ReLU activation                                                      |                                                                                              |
|                                                                                 | (cid:88) i                                                                                   |
|                                                                                 | .                                                                                            |
|                                                                                 | (cid:107)v(cid:107)p =                                                                       |
|                                                                                 | |vi|p                                                                                        |
| function,                                                                       |                                                                                              |
| as                                                                              |                                                                                              |
| it                                                                              |                                                                                              |
| currently is                                                                    |                                                                                              |
| the most widely used activation                                                 |                                                                                              |
|                                                                                 | =1                                                                                           |
| function [45],                                                                  |                                                                                              |
| [44],                                                                           |                                                                                              |
| [31],                                                                           |                                                                                              |
| [39].                                                                           |                                                                                              |
| We                                                                              | In more detail:                                                                              |
| use                                                                             |                                                                                              |
| image                                                                           |                                                                                              |
| classiﬁcation                                                                   |                                                                                              |
| as                                                                              |                                                                                              |
| our                                                                             |                                                                                              |
| primary                                                                         |                                                                                              |
| evaluation                                                                      |                                                                                              |
| domain. An h×w-pixel grey-scale image is a two-dimensional                      |                                                                                              |
|                                                                                 | 3An untargeted attack is simply a more efﬁcient                                              |
|                                                                                 | (and often less accurate)                                                                    |
| 2Most                                                                           | method of                                                                                    |
| simple networks have                                                            | running a targeted attack for each target and taking the closest.                            |
| this                                                                            |                                                                                              |
| simple                                                                          |                                                                                              |
| linear                                                                          |                                                                                              |
| structure, however other                                                        |                                                                                              |
| more sophisticated networks have more complicated structures (e.g., ResNet      | In this paper we focus on identifying the most accurate attacks, and do not                  |
| [16] and Inception [45]). The network architecture does not impact our attacks. | consider untargeted attacks.                                                                 |



## Page 4

1) L0 distance measures the number of coordinates i such
that xi ̸= x′
i. Thus, the L0 distance corresponds to the
number of pixels that have been altered in an image.4
Papernot et al. argue for the use of the L0 distance
metric, and it is the primary distance metric under which
defensive distillation’s security is argued [39].
2) L2 distance measures the standard Euclidean (root-
mean-square) distance between x and x′. The L2 dis-
tance can remain small when there are many small
changes to many pixels.
This distance metric was used in the initial adversarial
example work [46].
3) L∞distance measures the maximum change to any of
the coordinates:
∥x −x′∥∞= max(|x1 −x′
1|, . . . , |xn −x′
n|).
For images, we can imagine there is a maximum budget,
and each pixel is allowed to be changed by up to this
limit, with no limit on the number of pixels that are
modiﬁed.
Goodfellow et al. argue that L∞is the optimal distance
metric to use [47] and in a follow-up paper Papernot et
al. argue distillation is secure under this distance metric
[36].
No distance metric is a perfect measure of human perceptual
similarity, and we pass no judgement on exactly which dis-
tance metric is optimal. We believe constructing and evaluating
a good distance metric is an important research question we
leave to future work.
However, since most existing work has picked one of these
three distance metrics, and since defensive distillation argued
security against two of these, we too use these distance metrics
and construct attacks that perform superior to the state-of-the-
art for each of these distance metrics.
When reporting all numbers in this paper, we report using
the distance metric as deﬁned above, on the range [0, 1]. (That
is, changing a pixel in a greyscale image from full-on to full-
off will result in L2 change of 1.0 and a L∞change of 1.0,
not 255.)
E. Defensive Distillation
We brieﬂy provide a high-level overview of defensive distil-
lation. We provide a complete description later in Section VIII.
To defensively distill a neural network, begin by ﬁrst
training a network with identical architecture on the training
data in a standard manner. When we compute the softmax
while training this network, replace it with a more-smooth
version of the softmax (by dividing the logits by some constant
T). At the end of training, generate the soft training labels by
evaluating this network on each of the training instances and
taking the output labels of the network.
4In RGB images, there are three channels that each can change. We count
the number of pixels that are different, where two pixels are considered
different if any of the three colors are different. We do not consider a
distance metric where an attacker can change one color plane but not another
meaningful. We relax this requirement when comparing to other L0 attacks
that do not make this assumption to provide for a fair comparison.
Then, throw out the ﬁrst network and use only the soft
training labels. With those, train a second network where
instead of training it on the original training labels, use the
soft labels. This trains the second model to behave like the ﬁrst
model, and the soft labels convey additional hidden knowledge
learned by the ﬁrst model.
The key insight here is that by training to match the ﬁrst
network, we will hopefully avoid over-ﬁtting against any of the
training data. If the reason that neural networks exist is because
neural networks are highly non-linear and have “blind spots”
[46] where adversarial examples lie, then preventing this type
of over-ﬁtting might remove those blind spots.
In fact, as we will see later, defensive distillation does not
remove adversarial examples. One potential reason this may
occur is that others [11] have argued the reason adversarial
examples exist is not due to blind spots in a highly non-linear
neural network, but due only to the locally-linear nature of
neural networks. This so-called linearity hypothesis appears
to be true [47], and under this explanation it is perhaps less
surprising that distillation does not increase the robustness of
neural networks.
F. Organization
The remainder of this paper is structured as follows. In
the next section, we survey existing attacks that have been
proposed in the literature for generating adversarial examples,
for the L2, L∞, and L0 distance metrics. We then describe
our attack algorithms that target the same three distance
metrics and provide superior results to the prior work. Having
developed these attacks, we review defensive distillation in
more detail and discuss why the existing attacks fail to ﬁnd ad-
versarial examples on defensively distilled networks. Finally,
we attack defensive distillation with our new algorithms and
show that it provides only limited value.
III. ATTACK ALGORITHMS
A. L-BFGS
Szegedy et al. [46] generated adversarial examples using
box-constrained L-BFGS. Given an image x, their method
ﬁnds a different image x′ that is similar to x under L2 distance,
yet is labeled differently by the classiﬁer. They model the
problem as a constrained minimization problem:
minimize ∥x −x′∥2
2
such that C(x′) = l
x′ ∈[0, 1]n
This problem can be very difﬁcult to solve, however, so
Szegedy et al. instead solve the following problem:
minimize c · ∥x −x′∥2
2 + lossF,l(x′)
such that x′ ∈[0, 1]n
where lossF,l is a function mapping an image to a positive real
number. One common loss function to use is cross-entropy.
Line search is performed to ﬁnd the constant c > 0 that yields
an adversarial example of minimum distance: in other words,
4


**Table 4 from page 4**

| 0                                                                            | 1                                                                  |
|:-----------------------------------------------------------------------------|:-------------------------------------------------------------------|
| 1) L0 distance measures the number of coordinates i such                     | Then,                                                              |
|                                                                              | throw out                                                          |
|                                                                              | the ﬁrst network and use only the                                  |
|                                                                              | soft                                                               |
| (cid:54)= x(cid:48)                                                          | training                                                           |
| that xi                                                                      | labels. With                                                       |
| the L0 distance corresponds to the                                           | those,                                                             |
|                                                                              | train                                                              |
|                                                                              | a                                                                  |
|                                                                              | second                                                             |
|                                                                              | network where                                                      |
| i. Thus,                                                                     |                                                                    |
| number of pixels that have been altered in an image.4                        | instead of                                                         |
|                                                                              | training it on the original                                        |
|                                                                              | training labels, use the                                           |
| et                                                                           | soft labels. This trains the second model to behave like the ﬁrst  |
| al.                                                                          |                                                                    |
| Papernot                                                                     |                                                                    |
| argue                                                                        |                                                                    |
| for                                                                          |                                                                    |
| the                                                                          |                                                                    |
| use                                                                          |                                                                    |
| of                                                                           |                                                                    |
| distance                                                                     |                                                                    |
| the L0                                                                       |                                                                    |
| metric, and it is the primary distance metric under which                    | model, and the soft labels convey additional hidden knowledge      |
| defensive distillation’s security is argued [39].                            | learned by the ﬁrst model.                                         |
| distance measures                                                            | The key insight here is                                            |
| the                                                                          | that by training to match the ﬁrst                                 |
| standard                                                                     |                                                                    |
| Euclidean                                                                    |                                                                    |
| (root-                                                                       |                                                                    |
| 2) L2                                                                        |                                                                    |
| mean-square) distance between x and x(cid:48). The L2 dis-                   | network, we will hopefully avoid over-ﬁtting against any of the    |
| tance                                                                        | training data. If the reason that neural networks exist is because |
| can                                                                          |                                                                    |
| remain                                                                       |                                                                    |
| small when                                                                   |                                                                    |
| there                                                                        |                                                                    |
| are many                                                                     |                                                                    |
| small                                                                        |                                                                    |
| changes to many pixels.                                                      | neural networks are highly non-linear and have “blind spots”       |
| This distance metric was used in the initial adversarial                     | [46] where adversarial examples lie,                               |
|                                                                              | then preventing this type                                          |
| example work [46].                                                           | of over-ﬁtting might                                               |
|                                                                              | remove those blind spots.                                          |
| 3) L∞ distance measures the maximum change to any of                         | In fact, as we will see later, defensive distillation does not     |
| the coordinates:                                                             | remove adversarial examples. One potential                         |
|                                                                              | reason this may                                                    |
|                                                                              | occur                                                              |
|                                                                              | is                                                                 |
|                                                                              | that others                                                        |
|                                                                              | [11] have argued the reason adversarial                            |
| (cid:107)x − x(cid:48)(cid:107)∞ = max(|x1 − x(cid:48)                       |                                                                    |
| 1|, . . . , |xn − x(cid:48)                                                  |                                                                    |
| n|).                                                                         |                                                                    |
|                                                                              | examples exist                                                     |
|                                                                              | is not due to blind spots in a highly non-linear                   |
| For images, we can imagine there is a maximum budget,                        | neural network, but due only to the                                |
|                                                                              | locally-linear nature of                                           |
| and each pixel                                                               | neural networks. This                                              |
| is allowed to be changed by up to this                                       | so-called linearity hypothesis                                     |
|                                                                              | appears                                                            |
| limit, with no limit on the number of pixels                                 | to be true [47], and under                                         |
| that                                                                         | this explanation it                                                |
| are                                                                          | is perhaps                                                         |
|                                                                              | less                                                               |
| modiﬁed.                                                                     | surprising that distillation does not                              |
|                                                                              | increase the robustness of                                         |
| Goodfellow et al. argue that L∞ is the optimal distance                      | neural networks.                                                   |
| metric to use [47] and in a follow-up paper Papernot et                      |                                                                    |
|                                                                              | F                                                                  |
|                                                                              | . Organization                                                     |
| al. argue distillation is secure under this distance metric                  |                                                                    |
| [36].                                                                        | The                                                                |
|                                                                              | remainder                                                          |
|                                                                              | of                                                                 |
|                                                                              | this                                                               |
|                                                                              | paper                                                              |
|                                                                              | is                                                                 |
|                                                                              | structured                                                         |
|                                                                              | as                                                                 |
|                                                                              | follows.                                                           |
|                                                                              | In                                                                 |
|                                                                              | the next                                                           |
|                                                                              | section, we                                                        |
|                                                                              | survey existing attacks                                            |
|                                                                              | that have been                                                     |
| No distance metric is a perfect measure of human perceptual                  |                                                                    |
|                                                                              | proposed in the literature for generating adversarial examples,    |
| similarity, and we pass no judgement on exactly which dis-                   |                                                                    |
|                                                                              | for                                                                |
|                                                                              | the L2, L∞, and L0 distance metrics. We then describe              |
| tance metric is optimal. We believe constructing and evaluating              |                                                                    |
|                                                                              | our                                                                |
|                                                                              | attack                                                             |
|                                                                              | algorithms                                                         |
|                                                                              | that                                                               |
|                                                                              | target                                                             |
|                                                                              | the                                                                |
|                                                                              | same                                                               |
|                                                                              | three                                                              |
|                                                                              | distance                                                           |
| a good distance metric is an important                                       |                                                                    |
| research question we                                                         |                                                                    |
|                                                                              | metrics and provide superior results to the prior work. Having     |
| leave to future work.                                                        |                                                                    |
|                                                                              | developed these                                                    |
|                                                                              | attacks, we                                                        |
|                                                                              | review defensive distillation in                                   |
| However, since most existing work has picked one of these                    |                                                                    |
|                                                                              | more detail and discuss why the existing attacks fail to ﬁnd ad-   |
| three distance metrics, and since defensive distillation argued              |                                                                    |
|                                                                              | versarial examples on defensively distilled networks. Finally,     |
| security against two of these, we too use these distance metrics             |                                                                    |
|                                                                              | we attack defensive distillation with our new algorithms and       |
| and construct attacks that perform superior to the state-of-the-             |                                                                    |
|                                                                              | show that                                                          |
|                                                                              | it provides only limited value.                                    |
| art                                                                          |                                                                    |
| for each of                                                                  |                                                                    |
| these distance metrics.                                                      |                                                                    |
| When reporting all numbers in this paper, we report using                    |                                                                    |
|                                                                              | III. ATTACK ALGORITHMS                                             |
| the distance metric as deﬁned above, on the range [0, 1]. (That              |                                                                    |
|                                                                              | A. L-BFGS                                                          |
| is, changing a pixel                                                         |                                                                    |
| in a greyscale image from full-on to full-                                   |                                                                    |
| off will                                                                     | Szegedy et al.                                                     |
| result                                                                       | [46] generated adversarial                                         |
| in L2 change of 1.0 and a L∞ change of 1.0,                                  | examples using                                                     |
| not 255.)                                                                    | box-constrained L-BFGS. Given                                      |
|                                                                              | an                                                                 |
|                                                                              | image x,                                                           |
|                                                                              | their method                                                       |
|                                                                              | ﬁnds a different image x(cid:48)                                   |
|                                                                              | that is similar to x under L2 distance,                            |
| E. Defensive Distillation                                                    |                                                                    |
|                                                                              | yet                                                                |
|                                                                              | is                                                                 |
|                                                                              | labeled                                                            |
|                                                                              | differently by the                                                 |
|                                                                              | classiﬁer. They model                                              |
|                                                                              | the                                                                |
| We brieﬂy provide a high-level overview of defensive distil-                 |                                                                    |
|                                                                              | problem as a constrained minimization problem:                     |
| lation. We provide a complete description later in Section VIII.             |                                                                    |
| To                                                                           | (cid:107)x − x(cid:48)(cid:107)2                                   |
| defensively                                                                  | minimize                                                           |
| distill                                                                      | 2                                                                  |
| a                                                                            |                                                                    |
| neural                                                                       |                                                                    |
| network,                                                                     |                                                                    |
| begin                                                                        |                                                                    |
| by                                                                           |                                                                    |
| ﬁrst                                                                         |                                                                    |
| training a network with identical architecture on the training               | such that C(x(cid:48)) = l                                         |
| data                                                                         |                                                                    |
| in a                                                                         |                                                                    |
| standard manner. When we                                                     |                                                                    |
| compute                                                                      |                                                                    |
| the                                                                          |                                                                    |
| softmax                                                                      |                                                                    |
|                                                                              | x(cid:48) ∈ [0, 1]n                                                |
| while                                                                        |                                                                    |
| training this network,                                                       |                                                                    |
| replace                                                                      |                                                                    |
| it with a more-smooth                                                        |                                                                    |
| version of the softmax (by dividing the logits by some constant              | This                                                               |
|                                                                              | problem can                                                        |
|                                                                              | be                                                                 |
|                                                                              | very                                                               |
|                                                                              | difﬁcult                                                           |
|                                                                              | to                                                                 |
|                                                                              | solve,                                                             |
|                                                                              | however,                                                           |
|                                                                              | so                                                                 |
| T ). At                                                                      | Szegedy et al.                                                     |
| the end of training, generate the soft                                       | instead solve the following problem:                               |
| training labels by                                                           |                                                                    |
| evaluating this network on each of                                           |                                                                    |
| the training instances and                                                   |                                                                    |
|                                                                              | c · (cid:107)x − x(cid:48)(cid:107)2                               |
|                                                                              | minimize                                                           |
|                                                                              | 2 + lossF,l(x(cid:48))                                             |
| taking the output                                                            |                                                                    |
| labels of                                                                    |                                                                    |
| the network.                                                                 |                                                                    |
|                                                                              | x(cid:48) ∈ [0, 1]n                                                |
|                                                                              | such that                                                          |
| 4In RGB images,                                                              |                                                                    |
| there are three channels that each can change. We count                      |                                                                    |
| pixels                                                                       | is a function mapping an image to a positive real                  |
| the                                                                          | where lossF,l                                                      |
| number                                                                       |                                                                    |
| of                                                                           |                                                                    |
| that                                                                         |                                                                    |
| are                                                                          |                                                                    |
| different, where                                                             |                                                                    |
| two                                                                          |                                                                    |
| pixels                                                                       |                                                                    |
| are                                                                          |                                                                    |
| considered                                                                   |                                                                    |
| any                                                                          |                                                                    |
| different                                                                    |                                                                    |
| if                                                                           |                                                                    |
| of                                                                           |                                                                    |
| the                                                                          |                                                                    |
| three                                                                        |                                                                    |
| colors                                                                       |                                                                    |
| are                                                                          |                                                                    |
| different. We                                                                |                                                                    |
| do                                                                           |                                                                    |
| not                                                                          |                                                                    |
| consider                                                                     |                                                                    |
| a                                                                            |                                                                    |
|                                                                              | number. One common loss                                            |
|                                                                              | function to use is cross-entropy.                                  |
| distance metric where an attacker can change one color plane but not another |                                                                    |
|                                                                              | Line search is performed to ﬁnd the constant c > 0 that yields     |
| meaningful. We relax this requirement when comparing to other L0 attacks     |                                                                    |
| that do not make this assumption to provide for a fair comparison.           | an adversarial example of minimum distance:                        |
|                                                                              | in other words,                                                    |



## Page 5

we repeatedly solve this optimization problem for multiple
values of c, adaptively updating c using bisection search or
any other method for one-dimensional optimization.
B. Fast Gradient Sign
The fast gradient sign [11] method has two key differences
from the L-BFGS method: ﬁrst, it is optimized for the L∞
distance metric, and second, it is designed primarily to be fast
instead of producing very close adversarial examples. Given
an image x the fast gradient sign method sets
x′ = x −ϵ · sign(∇lossF,t(x)),
where ϵ is chosen to be sufﬁciently small so as to be
undetectable, and t is the target label. Intuitively, for each
pixel, the fast gradient sign method uses the gradient of
the loss function to determine in which direction the pixel’s
intensity should be changed (whether it should be increased
or decreased) to minimize the loss function; then, it shifts all
pixels simultaneously.
It is important to note that the fast gradient sign attack was
designed to be fast, rather than optimal. It is not meant to
produce the minimal adversarial perturbations.
Iterative Gradient Sign: Kurakin et al. introduce a simple
reﬁnement of the fast gradient sign method [26] where instead
of taking a single step of size ϵ in the direction of the gradient-
sign, multiple smaller steps α are taken, and the result is
clipped by the same ϵ. Speciﬁcally, begin by setting
x′
0 = 0
and then on each iteration
x′
i = x′
i−1 −clipϵ(α · sign(∇lossF,t(x′
i−1)))
Iterative gradient sign was found to produce superior results
to fast gradient sign [26].
C. JSMA
Papernot et al. introduced an attack optimized under L0
distance [38] known as the Jacobian-based Saliency Map
Attack (JSMA). We give a brief summary of their attack
algorithm; for a complete description and motivation, we
encourage the reader to read their original paper [38].
At a high level, the attack is a greedy algorithm that
picks pixels to modify one at a time, increasing the target
classiﬁcation on each iteration. They use the gradient ∇Z(x)l
to compute a saliency map, which models the impact each
pixel has on the resulting classiﬁcation. A large value indicates
that changing it will signiﬁcantly increase the likelihood of
the model labeling the image as the target class l. Given the
saliency map, it picks the most important pixel and modify
it to increase the likelihood of class l. This is repeated until
either more than a set threshold of pixels are modiﬁed which
makes the attack detectable, or it succeeds in changing the
classiﬁcation.
In more detail, we begin by deﬁning the saliency map in
terms of a pair of pixels p, q. Deﬁne
αpq =
X
i∈{p,q}
∂Z(x)t
∂xi
βpq =

X
i∈{p,q}
X
j
∂Z(x)j
∂xi

−αpq
so that αpq represents how much changing both pixels p and
q will change the target classiﬁcation, and βpq represents how
much changing p and q will change all other outputs. Then
the algorithm picks
(p∗, q∗) = arg max
(p,q) (−αpq · βpq) · (αpq > 0) · (βpq < 0)
so that αpq > 0 (the target class is more likely), βpq < 0 (the
other classes become less likely), and −αpq · βpq is largest.
Notice that JSMA uses the output of the second-to-last layer
Z, the logits, in the calculation of the gradient: the output of
the softmax F is not used. We refer to this as the JSMA-Z
attack.
However, when the authors apply this attack to their defen-
sively distilled networks, they modify the attack so it uses F
instead of Z. In other words, their computation uses the output
of the softmax (F) instead of the logits (Z). We refer to this
modiﬁcation as the JSMA-F attack.5
When an image has multiple color channels (e.g., RGB),
this attack considers the L0 difference to be 1 for each color
channel changed independently (so that if all three color
channels of one pixel change change, the L0 norm would be
3). While we do not believe this is a meaningful threat model,
when comparing to this attack, we evaluate under both models.



D. Deepfool
Deepfool [34] is an untargeted attack technique optimized
for the L2 distance metric. It is efﬁcient and produces closer
adversarial examples than the L-BFGS approach discussed
earlier.
The authors construct Deepfool by imagining that the neural
networks are totally linear, with a hyperplane separating each
class from another. From this, they analytically derive the
optimal solution to this simpliﬁed problem, and construct the
adversarial example.
Then, since neural networks are not actually linear, they take
a step towards that solution, and repeat the process a second
time. The search terminates when a true adversarial example
is found.
The exact formulation used is rather sophisticated; inter-
ested readers should refer to the original work [34].
IV. EXPERIMENTAL SETUP
Before we develop our attack algorithms to break distilla-
tion, we describe how we train the models on which we will
evaluate our attacks.
5We veriﬁed this via personal communication with the authors.
5


**Table 5 from page 5**

| 0                                                                          | 1                                                               |
|:---------------------------------------------------------------------------|:----------------------------------------------------------------|
| we                                                                         | In more detail, we begin by deﬁning the saliency map in         |
| repeatedly solve                                                           |                                                                 |
| this optimization problem for multiple                                     |                                                                 |
| values of                                                                  | terms of a pair of pixels p, q. Deﬁne                           |
| c, adaptively updating c using bisection search or                         |                                                                 |
| any other method for one-dimensional optimization.                         |                                                                 |
|                                                                            | ∂Z(x)t                                                          |
|                                                                            | (cid:88)                                                        |
|                                                                            | αpq =                                                           |
|                                                                            | ∂xi                                                             |
| B. Fast Gradient Sign                                                      | i∈{p,q}                                                         |
| The fast gradient sign [11] method has two key differences                 |                                                                 |
|                                                                            |                                                               |
|                                                                            |                                                               |
|                                                                            | ∂Z(x)j                                                          |
|                                                                            | (cid:88)                                                        |
|                                                                            | βpq =                                                           |
|                                                                            | − αpq                                                           |
| from the L-BFGS method: ﬁrst,                                              | (cid:88) j                                                      |
| it                                                                         |                                                                 |
| is optimized for                                                           |                                                                 |
| the L∞                                                                     |                                                                 |
|                                                                            | ∂xi                                                             |
|                                                                            | i∈{p,q}                                                         |
| distance metric, and second,                                               |                                                                 |
| it                                                                         |                                                                 |
| is designed primarily to be fast                                           |                                                                 |
| instead of producing very close adversarial examples. Given                |                                                                 |
|                                                                            | represents how much changing both pixels p and                  |
|                                                                            | so that αpq                                                     |
| an image x the fast gradient sign method sets                              |                                                                 |
|                                                                            | represents how                                                  |
|                                                                            | q will change the target classiﬁcation, and βpq                 |
|                                                                            | much changing p and q will change all other outputs. Then       |
| x(cid:48) = x − (cid:15) · sign(∇lossF,t(x)),                              |                                                                 |
|                                                                            | the algorithm picks                                             |
| (cid:15)                                                                   |                                                                 |
| where                                                                      |                                                                 |
| is                                                                         |                                                                 |
| chosen                                                                     |                                                                 |
| to                                                                         |                                                                 |
| be                                                                         |                                                                 |
| sufﬁciently                                                                |                                                                 |
| small                                                                      |                                                                 |
| so                                                                         |                                                                 |
| as                                                                         |                                                                 |
| to                                                                         |                                                                 |
| be                                                                         |                                                                 |
|                                                                            | (p∗, q∗) = arg max                                              |
|                                                                            | (−αpq · βpq) · (αpq > 0) · (βpq < 0)                            |
|                                                                            | (p,q)                                                           |
| undetectable,                                                              |                                                                 |
| and t                                                                      |                                                                 |
| is                                                                         |                                                                 |
| the                                                                        |                                                                 |
| target                                                                     |                                                                 |
| label.                                                                     |                                                                 |
| Intuitively,                                                               |                                                                 |
| for                                                                        |                                                                 |
| each                                                                       |                                                                 |
| pixel,                                                                     |                                                                 |
| the                                                                        |                                                                 |
| fast                                                                       |                                                                 |
| gradient                                                                   |                                                                 |
| sign method                                                                |                                                                 |
| uses                                                                       |                                                                 |
| the                                                                        |                                                                 |
| gradient                                                                   |                                                                 |
| of                                                                         |                                                                 |
|                                                                            | so that αpq > 0 (the target class is more likely), βpq < 0 (the |
| the loss                                                                   |                                                                 |
| function to determine in which direction the pixel’s                       |                                                                 |
|                                                                            | is largest.                                                     |
|                                                                            | other classes become less likely), and −αpq · βpq               |
| intensity should be changed (whether                                       |                                                                 |
| it                                                                         |                                                                 |
| should be increased                                                        |                                                                 |
|                                                                            | Notice that JSMA uses the output of the second-to-last layer    |
| or decreased)                                                              |                                                                 |
| to minimize the loss function;                                             |                                                                 |
| then,                                                                      |                                                                 |
| it shifts all                                                              |                                                                 |
|                                                                            | Z,                                                              |
|                                                                            | the logits,                                                     |
|                                                                            | in the calculation of                                           |
|                                                                            | the gradient:                                                   |
|                                                                            | the output of                                                   |
| pixels simultaneously.                                                     |                                                                 |
|                                                                            | the softmax F is not used. We refer                             |
|                                                                            | to this as                                                      |
|                                                                            | the JSMA-Z                                                      |
| It                                                                         |                                                                 |
| is important                                                               |                                                                 |
| to note that                                                               |                                                                 |
| the fast gradient sign attack was                                          |                                                                 |
|                                                                            | attack.                                                         |
| designed to be                                                             |                                                                 |
| fast,                                                                      |                                                                 |
| rather                                                                     |                                                                 |
| than optimal.                                                              |                                                                 |
| It                                                                         |                                                                 |
| is not meant                                                               |                                                                 |
| to                                                                         |                                                                 |
|                                                                            | However, when the authors apply this attack to their defen-     |
| produce the minimal adversarial perturbations.                             |                                                                 |
|                                                                            | sively distilled networks,                                      |
|                                                                            | they modify the attack so it uses F                             |
| Iterative Gradient Sign: Kurakin et al. introduce a simple                 | instead of Z. In other words, their computation uses the output |
| reﬁnement of the fast gradient sign method [26] where instead              | of                                                              |
|                                                                            | the softmax (F )                                                |
|                                                                            | instead of                                                      |
|                                                                            | the logits (Z). We refer                                        |
|                                                                            | to this                                                         |
| of taking a single step of size (cid:15) in the direction of the gradient- | modiﬁcation as the JSMA-F attack.5                              |
| sign, multiple                                                             | When an image has multiple color channels                       |
| smaller                                                                    | (e.g., RGB),                                                    |
| steps α are                                                                |                                                                 |
| taken,                                                                     |                                                                 |
| and                                                                        |                                                                 |
| the                                                                        |                                                                 |
| result                                                                     |                                                                 |
| is                                                                         |                                                                 |
| clipped by the same (cid:15). Speciﬁcally, begin by setting                |                                                                 |
|                                                                            | this attack considers the L0 difference to be 1 for each color  |
|                                                                            | channel                                                         |
|                                                                            | changed                                                         |
|                                                                            | independently                                                   |
|                                                                            | (so                                                             |
|                                                                            | that                                                            |
|                                                                            | if                                                              |
|                                                                            | all                                                             |
|                                                                            | three                                                           |
|                                                                            | color                                                           |
| x(cid:48)                                                                  |                                                                 |
| 0 = 0                                                                      |                                                                 |
|                                                                            | channels of one pixel change change,                            |
|                                                                            | the L0 norm would be                                            |
|                                                                            | 3). While we do not believe this is a meaningful                |
|                                                                            | threat model,                                                   |
| and then on each iteration                                                 |                                                                 |
|                                                                            | when comparing to this attack, we evaluate under both models.   |
| x(cid:48)                                                                  |                                                                 |
| i = x(cid:48)                                                              |                                                                 |
| i−1)))                                                                     |                                                                 |
| i−1 − clip(cid:15)(α · sign(∇lossF,t(x(cid:48)                             |                                                                 |
|                                                                            | D. Deepfool                                                     |
| Iterative gradient                                                         | Deepfool                                                        |
| sign was                                                                   | [34]                                                            |
| found to produce superior                                                  | is an untargeted attack technique optimized                     |
| results                                                                    |                                                                 |
| to fast gradient sign [26].                                                | for                                                             |
|                                                                            | It                                                              |
|                                                                            | is efﬁcient and produces closer                                 |
|                                                                            | the L2 distance metric.                                         |
|                                                                            | adversarial                                                     |
|                                                                            | examples                                                        |
|                                                                            | than                                                            |
|                                                                            | the L-BFGS approach                                             |
|                                                                            | discussed                                                       |
| C.                                                                         | earlier.                                                        |
| JSMA                                                                       |                                                                 |
|                                                                            | The authors construct Deepfool by imagining that the neural     |
| et al.                                                                     |                                                                 |
| Papernot                                                                   |                                                                 |
| introduced an attack optimized under L0                                    |                                                                 |
|                                                                            | networks are totally linear, with a hyperplane separating each  |
| distance                                                                   |                                                                 |
| [38]                                                                       |                                                                 |
| known                                                                      |                                                                 |
| as                                                                         |                                                                 |
| the                                                                        |                                                                 |
| Jacobian-based                                                             |                                                                 |
| Saliency Map                                                               |                                                                 |
|                                                                            | class                                                           |
|                                                                            | from another. From this,                                        |
|                                                                            | they                                                            |
|                                                                            | analytically                                                    |
|                                                                            | derive                                                          |
|                                                                            | the                                                             |
| Attack                                                                     |                                                                 |
| (JSMA). We                                                                 |                                                                 |
| give                                                                       |                                                                 |
| a                                                                          |                                                                 |
| brief                                                                      |                                                                 |
| summary                                                                    |                                                                 |
| of                                                                         |                                                                 |
| their                                                                      |                                                                 |
| attack                                                                     |                                                                 |
|                                                                            | optimal solution to this simpliﬁed problem, and construct       |
|                                                                            | the                                                             |
| algorithm;                                                                 |                                                                 |
| for                                                                        |                                                                 |
| a                                                                          |                                                                 |
| complete                                                                   |                                                                 |
| description                                                                |                                                                 |
| and motivation, we                                                         |                                                                 |
|                                                                            | adversarial example.                                            |
| encourage the reader                                                       |                                                                 |
| to read their original paper                                               |                                                                 |
| [38].                                                                      |                                                                 |
|                                                                            | Then, since neural networks are not actually linear, they take  |
| At                                                                         |                                                                 |
| a                                                                          |                                                                 |
| high                                                                       |                                                                 |
| level,                                                                     |                                                                 |
| the                                                                        |                                                                 |
| attack                                                                     |                                                                 |
| is                                                                         |                                                                 |
| a                                                                          |                                                                 |
| greedy                                                                     |                                                                 |
| algorithm that                                                             |                                                                 |
|                                                                            | a step towards that solution, and repeat                        |
|                                                                            | the process a second                                            |
| picks pixels                                                               |                                                                 |
| to modify one                                                              |                                                                 |
| at                                                                         |                                                                 |
| a                                                                          |                                                                 |
| time,                                                                      |                                                                 |
| increasing the                                                             |                                                                 |
| target                                                                     |                                                                 |
|                                                                            | time. The search terminates when a true adversarial example     |
| classiﬁcation on each iteration. They use the gradient ∇Z(x)l              |                                                                 |
|                                                                            | is found.                                                       |
| to compute                                                                 |                                                                 |
| a                                                                          |                                                                 |
| saliency map, which models                                                 |                                                                 |
| the                                                                        |                                                                 |
| impact                                                                     |                                                                 |
| each                                                                       |                                                                 |
|                                                                            | The                                                             |
|                                                                            | exact                                                           |
|                                                                            | formulation used is                                             |
|                                                                            | rather                                                          |
|                                                                            | sophisticated;                                                  |
|                                                                            | inter-                                                          |
| pixel has on the resulting classiﬁcation. A large value indicates          |                                                                 |
|                                                                            | ested readers should refer                                      |
|                                                                            | to the original work [34].                                      |
| that                                                                       |                                                                 |
| changing it will                                                           |                                                                 |
| signiﬁcantly increase                                                      |                                                                 |
| the                                                                        |                                                                 |
| likelihood of                                                              |                                                                 |
| the model                                                                  | IV. EXPERIMENTAL SETUP                                          |
| labeling the image as the target class l. Given the                        |                                                                 |
| saliency map,                                                              |                                                                 |
| it picks                                                                   |                                                                 |
| the most                                                                   |                                                                 |
| important pixel and modify                                                 |                                                                 |
|                                                                            | Before we develop our attack algorithms                         |
|                                                                            | to break distilla-                                              |
| it                                                                         |                                                                 |
| to increase the likelihood of class                                        |                                                                 |
| l. This                                                                    |                                                                 |
| is                                                                         |                                                                 |
| repeated until                                                             |                                                                 |
|                                                                            | tion, we describe how we train the models on which we will      |
| either more than a set                                                     |                                                                 |
| threshold of pixels are modiﬁed which                                      |                                                                 |
|                                                                            | evaluate our attacks.                                           |
| makes                                                                      |                                                                 |
| the                                                                        |                                                                 |
| attack detectable, or                                                      |                                                                 |
| it                                                                         |                                                                 |
| succeeds                                                                   |                                                                 |
| in changing the                                                            |                                                                 |
| classiﬁcation.                                                             | 5We veriﬁed this via personal communication with the authors.   |



## Page 6

Layer Type
MNIST Model
CIFAR Model
Convolution + ReLU
3×3×32
3×3×64
Convolution + ReLU
3×3×32
3×3×64
Max Pooling
2×2
2×2
Convolution + ReLU
3×3×64
3×3×128
Convolution + ReLU
3×3×64
3×3×128
Max Pooling
2×2
2×2
Fully Connected + ReLU
200
256
Fully Connected + ReLU
200
256
Softmax
10
10
TABLE I
MODEL ARCHITECTURES FOR THE MNIST AND CIFAR MODELS. THIS
ARCHITECTURE IS IDENTICAL TO THAT OF THE ORIGINAL DEFENSIVE
DISTILLATION WORK. [39]
Parameter
MNIST Model
CIFAR Model
Learning Rate
0.1
0.01 (decay 0.5)
Momentum
0.9
0.9 (decay 0.5)
Delay Rate
-
10 epochs
Dropout
0.5
0.5
Batch Size
128
128
Epochs
50
50
TABLE II
MODEL PARAMETERS FOR THE MNIST AND CIFAR MODELS. THESE
PARAMETERS ARE IDENTICAL TO THAT OF THE ORIGINAL DEFENSIVE
DISTILLATION WORK. [39]
We train two networks for the MNIST [28] and CIFAR-10
[24] classiﬁcation tasks, and use one pre-trained network for
the ImageNet classiﬁcation task [41]. Our models and training
approaches are identical to those presented in [39]. We achieve
99.5% accuracy on MNIST, comparable to the state of the
art. On CIFAR-10, we achieve 80% accuracy, identical to the
accuracy given in the distillation work. 6
MNIST and CIFAR-10. The model architecture is given in
Table I and the hyperparameters selected in Table II. We use
a momentum-based SGD optimizer during training.
The CIFAR-10 model signiﬁcantly overﬁts the training data
even with dropout: we obtain a ﬁnal training cross-entropy
loss of 0.05 with accuracy 98%, compared to a validation
loss of 1.2 with validation accuracy 80%. We do not alter
the network by performing image augmentation or adding
additional dropout as that was not done in [39].
ImageNet. Along with considering MNIST and CIFAR,
which are both relatively small datasets, we also consider
the ImageNet dataset. Instead of training our own ImageNet
model, we use the pre-trained Inception v3 network [45],
which achieves 96% top-5 accuracy (that is, the probability
that the correct class is one of the ﬁve most likely as reported
by the network is 96%). Inception takes images as 299×299×3
dimensional vectors.
6This is compared to the state-of-the-art result of 95% [12], [44], [31].
However, in order to provide the most accurate comparison to the original
work, we feel it is important to reproduce their model architectures.
V. OUR APPROACH
We now turn to our approach for constructing adversarial
examples. To begin, we rely on the initial formulation of
adversarial examples [46] and formally deﬁne the problem of
ﬁnding an adversarial instance for an image x as follows:
minimize D(x, x + δ)
such that C(x + δ) = t
x + δ ∈[0, 1]n
where x is ﬁxed, and the goal is to ﬁnd δ that minimizes
D(x, x+δ). That is, we want to ﬁnd some small change δ that
we can make to an image x that will change its classiﬁcation,
but so that the result is still a valid image. Here D is some
distance metric; for us, it will be either L0, L2, or L∞as
discussed earlier.
We solve this problem by formulating it as an appropriate
optimization instance that can be solved by existing optimiza-
tion algorithms. There are many possible ways to do this;
we explore the space of formulations and empirically identify
which ones lead to the most effective attacks.
A. Objective Function
The above formulation is difﬁcult for existing algorithms
to solve directly, as the constraint C(x + δ) = t is highly
non-linear. Therefore, we express it in a different form that is
better suited for optimization. We deﬁne an objective function
f such that C(x + δ) = t if and only if f(x + δ) ≤0. There
are many possible choices for f:
f1(x′) = −lossF,t(x′) + 1
f2(x′) = (max
i̸=t (F(x′)i) −F(x′)t)+
f3(x′) = softplus(max
i̸=t (F(x′)i) −F(x′)t) −log(2)
f4(x′) = (0.5 −F(x′)t)+
f5(x′) = −log(2F(x′)t −2)
f6(x′) = (max
i̸=t (Z(x′)i) −Z(x′)t)+
f7(x′) = softplus(max
i̸=t (Z(x′)i) −Z(x′)t) −log(2)
where s is the correct classiﬁcation, (e)+ is short-hand for
max(e, 0), softplus(x) = log(1 + exp(x)), and lossF,s(x) is
the cross entropy loss for x.
Notice that we have adjusted some of the above formula by
adding a constant; we have done this only so that the function
respects our deﬁnition. This does not impact the ﬁnal result,
as it just scales the minimization function.
Now, instead of formulating the problem as
minimize D(x, x + δ)
such that f(x + δ) ≤0
x + δ ∈[0, 1]n
we use the alternative formulation:
minimize D(x, x + δ) + c · f(x + δ)
such that x + δ ∈[0, 1]n
6


**Table 6 from page 6**

| 0                      | 1           | 2           | 3                     | 4                                                           | 5                                   | 6               | 7   | 8       | 9           | 10   |
|:-----------------------|:------------|:------------|:----------------------|:------------------------------------------------------------|:------------------------------------|:----------------|:----|:--------|:------------|:-----|
| Layer Type             | MNIST Model | CIFAR Model |                       |                                                             |                                     |                 |     |         |             |      |
|                        |             |             |                       |                                                             | V. OUR APPROACH                     |                 |     |         |             |      |
| Convolution + ReLU     | 3×3×32      | 3×3×64      |                       |                                                             |                                     |                 |     |         |             |      |
|                        |             |             |                       | We now turn to our approach for constructing adversarial    |                                     |                 |     |         |             |      |
| Convolution + ReLU     | 3×3×32      | 3×3×64      |                       |                                                             |                                     |                 |     |         |             |      |
|                        |             |             | examples. To          | begin, we                                                   | rely                                | on              | the | initial | formulation | of   |
| Max Pooling            | 2×2         | 2×2         |                       |                                                             |                                     |                 |     |         |             |      |
| Convolution + ReLU     | 3×3×64      | 3×3×128     |                       | adversarial examples [46] and formally deﬁne the problem of |                                     |                 |     |         |             |      |
| Convolution + ReLU     | 3×3×64      | 3×3×128     |                       |                                                             |                                     |                 |     |         |             |      |
|                        |             |             | ﬁnding an adversarial |                                                             | instance for an image x as follows: |                 |     |         |             |      |
| Max Pooling            | 2×2         | 2×2         |                       |                                                             |                                     |                 |     |         |             |      |
| Fully Connected + ReLU | 200         | 256         |                       |                                                             |                                     |                 |     |         |             |      |
|                        |             |             |                       |                                                             | minimize D(x, x + δ)                |                 |     |         |             |      |
| Fully Connected + ReLU | 200         | 256         |                       |                                                             |                                     |                 |     |         |             |      |
| Softmax                | 10          | 10          |                       |                                                             | such that C(x + δ) = t              |                 |     |         |             |      |
|                        |             |             |                       |                                                             |                                     | x + δ ∈ [0, 1]n |     |         |             |      |

**Table 7 from page 6**

| 0                                                           | 1                       | 2                | 3    | 4                                                              | 5            | 6                               | 7                       | 8   | 9     |
|:------------------------------------------------------------|:------------------------|:-----------------|:-----|:---------------------------------------------------------------|:-------------|:--------------------------------|:------------------------|:----|:------|
|                                                             |                         |                  |      | where x is ﬁxed,                                               | and the goal | to ﬁnd δ                        | that minimizes          |     |       |
|                                                             |                         |                  |      |                                                                | is           |                                 |                         |     |       |
| ARCHITECTURE IS IDENTICAL TO THAT OF THE ORIGINAL DEFENSIVE |                         |                  |      |                                                                |              |                                 |                         |     |       |
|                                                             | DISTILLATION WORK. [39] |                  |      | D(x, x+δ). That                                                | is, we want  | to ﬁnd some small change δ that |                         |     |       |
|                                                             |                         |                  |      | we can make to an image x that will change its classiﬁcation,  |              |                                 |                         |     |       |
|                                                             |                         |                  | but  | so that                                                        |              | still a valid image. Here D is  |                         |     | some  |
|                                                             |                         |                  |      | the result                                                     |              |                                 |                         |     |       |
|                                                             |                         |                  |      | is                                                             |              |                                 |                         |     |       |
| Parameter                                                   | MNIST Model             | CIFAR Model      |      | distance metric;                                               | it will be   |                                 | either L0, L2, or L∞ as |     |       |
|                                                             |                         |                  |      | for us,                                                        |              |                                 |                         |     |       |
|                                                             |                         |                  |      | discussed earlier.                                             |              |                                 |                         |     |       |
| Learning Rate                                               | 0.1                     | 0.01 (decay 0.5) |      |                                                                |              |                                 |                         |     |       |
| Momentum                                                    | 0.9                     | 0.9 (decay 0.5)  |      | We solve this problem by formulating it as an appropriate      |              |                                 |                         |     |       |
| Delay Rate                                                  | -                       | 10 epochs        |      |                                                                |              |                                 |                         |     |       |
|                                                             |                         |                  |      | optimization instance that can be solved by existing optimiza- |              |                                 |                         |     |       |
| Dropout                                                     | 0.5                     | 0.5              |      |                                                                |              |                                 |                         |     |       |
|                                                             |                         |                  | tion | algorithms. There                                              | are many     | possible ways                   | to                      | do  | this; |
| Batch Size                                                  | 128                     | 128              |      |                                                                |              |                                 |                         |     |       |
| Epochs                                                      | 50                      | 50               |      | we explore the space of formulations and empirically identify  |              |                                 |                         |     |       |
|                                                             |                         |                  |      | which ones lead to the most effective attacks.                 |              |                                 |                         |     |       |

**Table 8 from page 6**

| 0                                                               | 1                                                              |
|:----------------------------------------------------------------|:---------------------------------------------------------------|
|                                                                 | to solve directly,                                             |
|                                                                 | as                                                             |
|                                                                 | the                                                            |
|                                                                 | constraint C(x + δ) = t                                        |
|                                                                 | is highly                                                      |
|                                                                 | non-linear. Therefore, we express it                           |
|                                                                 | in a different form that                                       |
|                                                                 | is                                                             |
|                                                                 | better suited for optimization. We deﬁne an objective function |
| We train two networks for                                       | f such that C(x + δ) = t if and only if f (x + δ) ≤ 0. There   |
| the MNIST [28] and CIFAR-10                                     |                                                                |
| [24] classiﬁcation tasks, and use one pre-trained network for   | are many possible choices for f :                              |
| the ImageNet classiﬁcation task [41]. Our models and training   |                                                                |
|                                                                 | f1(x(cid:48)) = −lossF,t(x(cid:48)) + 1                        |
| approaches are identical to those presented in [39]. We achieve |                                                                |
|                                                                 | f2(x(cid:48)) = (max                                           |
|                                                                 | (F (x(cid:48))i) − F (x(cid:48))t)+                            |
| 99.5% accuracy on MNIST,                                        |                                                                |
| comparable                                                      |                                                                |
| to the                                                          |                                                                |
| state of                                                        |                                                                |
| the                                                             |                                                                |
|                                                                 | i                                                              |
|                                                                 | (cid:54)=t                                                     |
| art. On CIFAR-10, we achieve 80% accuracy,                      |                                                                |
| identical                                                       |                                                                |
| to the                                                          |                                                                |
|                                                                 | f3(x(cid:48)) = softplus(max                                   |
|                                                                 | (F (x(cid:48))i) − F (x(cid:48))t) − log(2)                    |
| accuracy given in the distillation work. 6                      | i(cid:54)=t                                                    |
|                                                                 | f4(x(cid:48)) = (0.5 − F (x(cid:48))t)+                        |
| MNIST and CIFAR-10. The model architecture is given in          |                                                                |
|                                                                 | f5(x(cid:48)) = − log(2F (x(cid:48))t − 2)                     |
| Table I and the hyperparameters selected in Table II. We use    |                                                                |
|                                                                 | f6(x(cid:48)) = (max                                           |
|                                                                 | (Z(x(cid:48))i) − Z(x(cid:48))t)+                              |
| a momentum-based SGD optimizer during training.                 |                                                                |
|                                                                 | i(cid:54)=t                                                    |
| The CIFAR-10 model signiﬁcantly overﬁts the training data       |                                                                |
|                                                                 | f7(x(cid:48)) = softplus(max                                   |
|                                                                 | (Z(x(cid:48))i) − Z(x(cid:48))t) − log(2)                      |
|                                                                 | i(cid:54)=t                                                    |
| even with dropout: we obtain a ﬁnal                             |                                                                |
| training cross-entropy                                          |                                                                |
| loss of 0.05 with accuracy 98%,                                 |                                                                |
| compared to a validation                                        |                                                                |
|                                                                 | where s is                                                     |
|                                                                 | the                                                            |
|                                                                 | correct                                                        |
|                                                                 | classiﬁcation,                                                 |
|                                                                 | (e)+ is                                                        |
|                                                                 | short-hand for                                                 |
| loss of 1.2 with validation accuracy 80%. We do not             |                                                                |
| alter                                                           |                                                                |
|                                                                 | max(e, 0),                                                     |
|                                                                 | is                                                             |
|                                                                 | softplus(x) = log(1 + exp(x)), and lossF,s(x)                  |
| the                                                             |                                                                |
| network                                                         |                                                                |
| by                                                              |                                                                |
| performing                                                      |                                                                |
| image                                                           |                                                                |
| augmentation                                                    |                                                                |
| or                                                              |                                                                |
| adding                                                          |                                                                |
|                                                                 | the cross entropy loss for x.                                  |
| additional dropout as that was not done in [39].                | Notice that we have adjusted some of the above formula by      |
|                                                                 | adding a constant; we have done this only so that              |
|                                                                 | the function                                                   |
| ImageNet. Along with                                            |                                                                |
| considering MNIST and CIFAR,                                    |                                                                |
|                                                                 | respects our deﬁnition. This does not                          |
|                                                                 | impact                                                         |
|                                                                 | the ﬁnal                                                       |
|                                                                 | result,                                                        |
| which                                                           |                                                                |
| are                                                             |                                                                |
| both                                                            |                                                                |
| relatively                                                      |                                                                |
| small                                                           |                                                                |
| datasets, we                                                    |                                                                |
| also                                                            |                                                                |
| consider                                                        |                                                                |
|                                                                 | as it                                                          |
|                                                                 | just scales the minimization function.                         |
| the ImageNet dataset.                                           |                                                                |
| Instead of                                                      |                                                                |
| training our own ImageNet                                       |                                                                |
|                                                                 | Now,                                                           |
|                                                                 | instead of                                                     |
|                                                                 | formulating the problem as                                     |
| model, we                                                       |                                                                |
| use                                                             |                                                                |
| the                                                             |                                                                |
| pre-trained                                                     |                                                                |
| Inception                                                       |                                                                |
| v3                                                              |                                                                |
| network                                                         |                                                                |
| [45],                                                           |                                                                |
|                                                                 | minimize D(x, x + δ)                                           |
| which achieves 96% top-5 accuracy (that                         |                                                                |
| is,                                                             |                                                                |
| the probability                                                 |                                                                |
| that                                                            | f (x + δ) ≤ 0                                                  |
| the correct class is one of the ﬁve most                        | such that                                                      |
| likely as reported                                              |                                                                |
| by the network is 96%). Inception takes images as 299×299×3     |                                                                |
|                                                                 | x + δ ∈ [0, 1]n                                                |
| dimensional vectors.                                            |                                                                |
|                                                                 | we use the alternative formulation:                            |
| 6This                                                           | minimize D(x, x + δ) + c · f (x + δ)                           |
| is                                                              |                                                                |
| compared to the                                                 |                                                                |
| state-of-the-art                                                |                                                                |
| result of 95% [12],                                             |                                                                |
| [44],                                                           |                                                                |
| [31].                                                           |                                                                |
| However,                                                        |                                                                |
| in order                                                        |                                                                |
| to provide the most accurate comparison to the original         |                                                                |
|                                                                 | x + δ ∈ [0, 1]n                                                |
|                                                                 | such that                                                      |
| work, we feel                                                   |                                                                |
| it                                                              |                                                                |
| is important                                                    |                                                                |
| to reproduce their model architectures.                         |                                                                |



## Page 7

0.0
0.2
0.4
0.6
0.8
1.0
Success Probability
0
2
4
6
8
10
Mean Adversarial Example Distance
1e−02
1e−01
1e+00
1e+01
1e+02
Constant c used
Fig. 2. Sensitivity on the constant c. We plot the L2 distance of the adversarial
example computed by gradient descent as a function of c, for objective
function f6. When c < .1, the attack rarely succeeds. After c > 1, the
attack becomes less effective, but always succeeds.
where c > 0 is a suitably chosen constant. These two are
equivalent, in the sense that there exists c > 0 such that the
optimal solution to the latter matches the optimal solution to
the former. After instantiating the distance metric D with an
lp norm, the problem becomes: given x, ﬁnd δ that solves
minimize ∥δ∥p + c · f(x + δ)
such that x + δ ∈[0, 1]n
Choosing the constant c.
Empirically, we have found that often the best way to choose
c is to use the smallest value of c for which the resulting
solution x∗has f(x∗) ≤0. This causes gradient descent to
minimize both of the terms simultaneously instead of picking
only one to optimize over ﬁrst.
We verify this by running our f6 formulation (which we
found most effective) for values of c spaced uniformly (on a
log scale) from c = 0.01 to c = 100 on the MNIST dataset.
We plot this line in Figure 2. 7
Further, we have found that if choose the smallest c such
that f(x∗) ≤0, the solution is within 5% of optimal 70% of
the time, and within 30% of optimal 98% of the time, where
“optimal” refers to the solution found using the best value of
c. Therefore, in our implementations we use modiﬁed binary
search to choose c.
7The corresponding ﬁgures for other objective functions are similar; we
omit them for brevity.
B. Box constraints
To ensure the modiﬁcation yields a valid image, we have a
constraint on δ: we must have 0 ≤xi +δi ≤1 for all i. In the
optimization literature, this is known as a “box constraint.”
Previous work uses a particular optimization algorithm, L-
BFGS-B, which supports box constraints natively.
We investigate three different methods of approaching this
problem.
1) Projected gradient descent performs one step of standard
gradient descent, and then clips all the coordinates to be
within the box.
This approach can work poorly for gradient descent
approaches that have a complicated update step (for
example, those with momentum): when we clip the
actual xi, we unexpectedly change the input to the next
iteration of the algorithm.
2) Clipped gradient descent does not clip xi on each
iteration; rather, it incorporates the clipping into the
objective function to be minimized. In other words, we
replace f(x + δ) with f(min(max(x + δ, 0), 1)), with
the min and max taken component-wise.
While solving the main issue with projected gradient de-
scent, clipping introduces a new problem: the algorithm
can get stuck in a ﬂat spot where it has increased some
component xi to be substantially larger than the maxi-
mum allowed. When this happens, the partial derivative
becomes zero, so even if some improvement is possible
by later reducing xi, gradient descent has no way to
detect this.
3) Change of variables introduces a new variable w and
instead of optimizing over the variable δ deﬁned above,
we apply a change-of-variables and optimize over w,
setting
δi = 1
2(tanh(wi) + 1) −xi.
Since −1 ≤tanh(wi) ≤1, it follows that 0 ≤xi +δi ≤
1, so the solution will automatically be valid. 8
We can think of this approach as a smoothing of clipped
gradient descent that eliminates the problem of getting
stuck in extreme regions.
These methods allow us to use other optimization algo-
rithms that don’t natively support box constraints. We use the
Adam [23] optimizer almost exclusively, as we have found it to
be the most effective at quickly ﬁnding adversarial examples.
We tried three solvers — standard gradient descent, gradient
descent with momentum, and Adam — and all three produced
identical-quality solutions. However, Adam converges substan-
tially more quickly than the others.
C. Evaluation of approaches
For each possible objective function f(·) and method to
enforce the box constraint, we evaluate the quality of the
adversarial examples found.
8Instead of scaling by 1
2 we scale by 1
2 + ϵ to avoid dividing by zero.
7


**Table 9 from page 7**

| 0   | 1                        | 2                                                               | 3           | 4            | 5                 |
|:----|:-------------------------|:----------------------------------------------------------------|:------------|:-------------|:------------------|
|     | B. Box constraints       |                                                                 |             |              |                   |
|     |                          | To ensure the modiﬁcation yields a valid image, we have a       |             |              |                   |
|     |                          | constraint on δ: we must have 0 ≤ xi + δi ≤ 1 for all i. In the |             |              |                   |
|     | optimization literature, | this                                                            | is known as | a            | “box constraint.” |
| 10  | Previous work            | particular                                                      |             | optimization | algorithm, L-     |
|     | uses                     |                                                                 |             |              |                   |
|     | a                        |                                                                 |             |              |                   |
|     |                          | BFGS-B, which supports box constraints natively.                |             |              |                   |
|     |                          | We investigate three different methods of approaching this      |             |              |                   |
| 8   |                          |                                                                 |             |              |                   |
|     | problem.                 |                                                                 |             |              |                   |
| 6   |                          |                                                                 |             |              |                   |
|     |                          | 1) Projected gradient descent performs one step of standard     |             |              |                   |

**Table 10 from page 7**

| 0                                                                                 | 1                                                           |
|:----------------------------------------------------------------------------------|:------------------------------------------------------------|
| 0.8                                                                               | BFGS-B, which supports box constraints natively.            |
|                                                                                   | We investigate three different methods of approaching this  |
| 8                                                                                 |                                                             |
| 0.6                                                                               | problem.                                                    |
| 6                                                                                 |                                                             |
| Success Probability                                                               | 1) Projected gradient descent performs one step of standard |
| Mean Adversarial Example Distance                                                 |                                                             |
|                                                                                   | gradient descent, and then clips all                        |
|                                                                                   | the coordinates to be                                       |
| 0.4                                                                               | within the box.                                             |
| 4                                                                                 |                                                             |
|                                                                                   | This                                                        |
|                                                                                   | approach                                                    |
|                                                                                   | can work                                                    |
|                                                                                   | poorly                                                      |
|                                                                                   | for                                                         |
|                                                                                   | gradient                                                    |
|                                                                                   | descent                                                     |
| 0.2                                                                               | approaches                                                  |
| 2                                                                                 | that                                                        |
|                                                                                   | have                                                        |
|                                                                                   | a                                                           |
|                                                                                   | complicated                                                 |
|                                                                                   | update                                                      |
|                                                                                   | step                                                        |
|                                                                                   | (for                                                        |
|                                                                                   | example,                                                    |
|                                                                                   | those with momentum): when we                               |
|                                                                                   | clip                                                        |
|                                                                                   | the                                                         |
| 0.0                                                                               | to the next                                                 |
| 0                                                                                 | actual xi, we unexpectedly change the input                 |
|                                                                                   | iteration of                                                |
|                                                                                   | the algorithm.                                              |
| 1e−02                                                                             | gradient                                                    |
| 1e−01                                                                             | descent                                                     |
| 1e+00                                                                             | 2) Clipped                                                  |
| 1e+01                                                                             | does                                                        |
| 1e+02                                                                             | not                                                         |
|                                                                                   | clip                                                        |
|                                                                                   | on                                                          |
|                                                                                   | each                                                        |
|                                                                                   | xi                                                          |
| Constant c used                                                                   | iteration;                                                  |
|                                                                                   | rather,                                                     |
|                                                                                   | it                                                          |
|                                                                                   | incorporates                                                |
|                                                                                   | the                                                         |
|                                                                                   | clipping                                                    |
|                                                                                   | into                                                        |
|                                                                                   | the                                                         |
|                                                                                   | objective function to be minimized.                         |
|                                                                                   | In other words, we                                          |
|                                                                                   | replace f (x + δ) with f (min(max(x + δ, 0), 1)), with      |
| Fig. 2. Sensitivity on the constant c. We plot the L2 distance of the adversarial | the min and max taken component-wise.                       |
| example                                                                           |                                                             |
| computed                                                                          |                                                             |
| by                                                                                |                                                             |
| gradient                                                                          |                                                             |
| descent                                                                           |                                                             |
| as                                                                                |                                                             |
| a                                                                                 |                                                             |
| function                                                                          |                                                             |
| of                                                                                |                                                             |
| c,                                                                                |                                                             |
| for                                                                               |                                                             |
| objective                                                                         |                                                             |
|                                                                                   | While solving the main issue with projected gradient de-    |
| the                                                                               |                                                             |
| attack rarely succeeds. After                                                     |                                                             |
| c > 1,                                                                            |                                                             |
| the                                                                               |                                                             |
| function f6. When c < .1,                                                         |                                                             |
|                                                                                   | scent, clipping introduces a new problem:                   |
|                                                                                   | the algorithm                                               |
| attack becomes less effective, but always succeeds.                               |                                                             |
|                                                                                   | can get stuck in a ﬂat spot where it has increased some     |
|                                                                                   | to be substantially larger                                  |
|                                                                                   | than the maxi-                                              |
|                                                                                   | component xi                                                |
|                                                                                   | mum allowed. When this happens,                             |
|                                                                                   | the partial derivative                                      |
| where                                                                             |                                                             |
| c > 0 is                                                                          |                                                             |
| a                                                                                 |                                                             |
| suitably chosen constant. These                                                   |                                                             |
| two are                                                                           |                                                             |
|                                                                                   | becomes zero, so even if some improvement                   |
|                                                                                   | is possible                                                 |
| equivalent,                                                                       |                                                             |
| in the sense that                                                                 |                                                             |
| there exists c > 0 such that                                                      |                                                             |
| the                                                                               |                                                             |
|                                                                                   | by later                                                    |
|                                                                                   | reducing xi, gradient descent has no way to                 |
| optimal solution to the latter matches the optimal solution to                    |                                                             |
|                                                                                   | detect                                                      |
|                                                                                   | this.                                                       |
| the former. After                                                                 |                                                             |
| instantiating the distance metric D with an                                       |                                                             |
|                                                                                   | 3) Change of variables                                      |
|                                                                                   | introduces a new variable w and                             |
| the problem becomes: given x, ﬁnd δ that solves                                   |                                                             |
| lp norm,                                                                          |                                                             |
|                                                                                   | instead of optimizing over the variable δ deﬁned above,     |
|                                                                                   | we                                                          |
|                                                                                   | apply a                                                     |
|                                                                                   | change-of-variables                                         |
|                                                                                   | and optimize over w,                                        |
| minimize                                                                          |                                                             |
| (cid:107)δ(cid:107)p + c · f (x + δ)                                              |                                                             |
|                                                                                   | setting                                                     |
| x + δ ∈ [0, 1]n                                                                   |                                                             |
| such that                                                                         |                                                             |
|                                                                                   | 1 2                                                         |
|                                                                                   | (tanh(wi) + 1) − xi.                                        |
|                                                                                   | δi =                                                        |
|                                                                                   | Since −1 ≤ tanh(wi) ≤ 1,                                    |
|                                                                                   | it follows that 0 ≤ xi + δi ≤                               |
| Choosing the constant c.                                                          |                                                             |
|                                                                                   | 1, so the solution will automatically be valid. 8           |
| Empirically, we have found that often the best way to choose                      |                                                             |
|                                                                                   | We can think of this approach as a smoothing of clipped     |
| c                                                                                 |                                                             |
| c                                                                                 |                                                             |
| is                                                                                |                                                             |
| to use                                                                            |                                                             |
| the                                                                               |                                                             |
| smallest value of                                                                 |                                                             |
| for which the                                                                     |                                                             |
| resulting                                                                         |                                                             |
|                                                                                   | gradient descent                                            |
|                                                                                   | that eliminates the problem of getting                      |
| solution x∗ has f (x∗) ≤ 0. This causes gradient descent                          |                                                             |
| to                                                                                |                                                             |
|                                                                                   | stuck in extreme regions.                                   |
| minimize both of the terms simultaneously instead of picking                      |                                                             |
|                                                                                   | These methods                                               |
|                                                                                   | allow us                                                    |
|                                                                                   | to                                                          |
|                                                                                   | use                                                         |
|                                                                                   | other                                                       |
|                                                                                   | optimization                                                |
|                                                                                   | algo-                                                       |



## Page 8

Best Case
Average Case
Worst Case
Change of
Clipped
Projected
Change of
Clipped
Projected
Change of
Clipped
Projected
Variable
Descent
Descent
Variable
Descent
Descent
Variable
Descent
Descent
mean
prob
mean
prob
mean
prob
mean
prob
mean
prob
mean
prob
mean
prob
mean
prob
mean
prob
f1
2.46
100%
2.93
100%
2.31
100%
4.35
100%
5.21
100%
4.11
100%
7.76
100%
9.48
100%
7.37
100%
f2
4.55
80%
3.97
83%
3.49
83%
3.22
44%
8.99
63%
15.06
74%
2.93
18%
10.22
40%
18.90
53%
f3
4.54
77%
4.07
81%
3.76
82%
3.47
44%
9.55
63%
15.84
74%
3.09
17%
11.91
41%
24.01
59%
f4
5.01
86%
6.52
100%
7.53
100%
4.03
55%
7.49
71%
7.60
71%
3.55
24%
4.25
35%
4.10
35%
f5
1.97
100%
2.20
100%
1.94
100%
3.58
100%
4.20
100%
3.47
100%
6.42
100%
7.86
100%
6.12
100%
f6
1.94
100%
2.18
100%
1.95
100%
3.47
100%
4.11
100%
3.41
100%
6.03
100%
7.50
100%
5.89
100%
f7
1.96
100%
2.21
100%
1.94
100%
3.53
100%
4.14
100%
3.43
100%
6.20
100%
7.57
100%
5.94
100%
TABLE III
EVALUATION OF ALL COMBINATIONS OF ONE OF THE SEVEN POSSIBLE OBJECTIVE FUNCTIONS WITH ONE OF THE THREE BOX CONSTRAINT ENCODINGS.
WE SHOW THE AVERAGE L2 DISTORTION, THE STANDARD DEVIATION, AND THE SUCCESS PROBABILITY (FRACTION OF INSTANCES FOR WHICH AN
ADVERSARIAL EXAMPLE CAN BE FOUND). EVALUATED ON 1000 RANDOM INSTANCES. WHEN THE SUCCESS IS NOT 100%, MEAN IS FOR SUCCESSFUL
ATTACKS ONLY.
To choose the optimal c, we perform 20 iterations of binary
search over c. For each selected value of c, we run 10, 000
iterations of gradient descent with the Adam optimizer. 9
The results of this analysis are in Table III. We evaluate
the quality of the adversarial examples found on the MNIST
and CIFAR datasets. The relative ordering of each objective
function is identical between the two datasets, so for brevity
we report only results for MNIST.
There is a factor of three difference in quality between the
best objective function and the worst. The choice of method
for handling box constraints does not impact the quality of
results as signiﬁcantly for the best minimization functions.
In fact, the worst performing objective function, cross
entropy loss, is the approach that was most suggested in the
literature previously [46], [42].
Why are some loss functions better than others? When c =
0, gradient descent will not make any move away from the
initial image. However, a large c often causes the initial steps
of gradient descent to perform in an overly-greedy manner,
only traveling in the direction which can most easily reduce
f and ignoring the D loss — thus causing gradient descent to
ﬁnd sub-optimal solutions.
This means that for loss function f1 and f4, there is no
good constant c that is useful throughout the duration of
the gradient descent search. Since the constant c weights the
relative importance of the distance term and the loss term, in
order for a ﬁxed constant c to be useful, the relative value of
these two terms should remain approximately equal. This is
not the case for these two loss functions.
To explain why this is the case, we will have to take a side
discussion to analyze how adversarial examples exist. Consider
a valid input x and an adversarial example x′ on a network.
What does it look like as we linearly interpolate from x to
x′? That is, let y = αx+(1−α)x′ for α ∈[0, 1]. It turns out the
value of Z(·)t is mostly linear from the input to the adversarial
example, and therefore the F(·)t is a logistic. We verify this
fact empirically by constructing adversarial examples on the
9Adam converges to 95% of optimum within 1, 000 iterations 92% of the
time. For completeness we run it for 10, 000 iterations at each step.
ﬁrst 1, 000 test images on both the MNIST and CIFAR dataset
with our approach, and ﬁnd the Pearson correlation coefﬁcient
r > .9.
Given this, consider loss function f4 (the argument for f1 is
similar). In order for the gradient descent attack to make any
change initially, the constant c will have to be large enough
that
ϵ < c(f1(x + ϵ) −f1(x))
or, as ϵ →0,
1/c < |∇f1(x)|
implying that c must be larger than the inverse of the gradient
to make progress, but the gradient of f1 is identical to F(·)t
so will be tiny around the initial image, meaning c will have
to be extremely large.
However, as soon as we leave the immediate vicinity of
the initial image, the gradient of ∇f1(x + δ) increases at an
exponential rate, making the large constant c cause gradient
descent to perform in an overly greedy manner.
We verify all of this theory empirically. When we run our
attack trying constants chosen from 10−10 to 1010 the average
constant for loss function f4 was 106.
The average gradient of the loss function f1 around the valid
image is 2−20 but 2−1 at the closest adversarial example. This
means c is a million times larger than it has to be, causing
the loss function f4 and f1 to perform worse than any of the
others.
D. Discretization
We model pixel intensities as a (continuous) real number in
the range [0, 1]. However, in a valid image, each pixel intensity
must be a (discrete) integer in the range {0, 1, . . . , 255}. This
additional requirement is not captured in our formulation.
In practice, we ignore the integrality constraints, solve the
continuous optimization problem, and then round to the nearest
integer: the intensity of the ith pixel becomes ⌊255(xi + δi)⌉.
This rounding will slightly degrade the quality of the
adversarial example. If we need to restore the attack quality,
we perform greedy search on the lattice deﬁned by the discrete
8


**Table 11 from page 8**

| 0   | 1    | 2         | 3    | 4         | 5    | 6         | 7    | 8         | 9    | 10           | 11    | 12        | 13   | 14        | 15    | 16         | 17    | 18        |
|:----|:-----|:----------|:-----|:----------|:-----|:----------|:-----|:----------|:-----|:-------------|:------|:----------|:-----|:----------|:------|:-----------|:------|:----------|
|     |      |           |      | Best Case |      |           |      |           |      | Average Case |       |           |      |           |       | Worst Case |       |           |
|     |      | Change of |      | Clipped   |      | Projected |      | Change of |      | Clipped      |       | Projected |      | Change of |       | Clipped    |       | Projected |
|     |      | Variable  |      | Descent   |      | Descent   |      | Variable  |      | Descent      |       | Descent   |      | Variable  |       | Descent    |       | Descent   |
|     | mean | prob      | mean | prob      | mean | prob      | mean | prob      | mean | prob         | mean  | prob      | mean | prob      | mean  | prob       | mean  | prob      |
| f1  | 2.46 | 100%      | 2.93 | 100%      | 2.31 | 100%      | 4.35 | 100%      | 5.21 | 100%         | 4.11  | 100%      | 7.76 | 100%      | 9.48  | 100%       | 7.37  | 100%      |
| f2  | 4.55 | 80%       | 3.97 | 83%       | 3.49 | 83%       | 3.22 | 44%       | 8.99 | 63%          | 15.06 | 74%       | 2.93 | 18%       | 10.22 | 40%        | 18.90 | 53%       |
| f3  | 4.54 | 77%       | 4.07 | 81%       | 3.76 | 82%       | 3.47 | 44%       | 9.55 | 63%          | 15.84 | 74%       | 3.09 | 17%       | 11.91 | 41%        | 24.01 | 59%       |
| f4  | 5.01 | 86%       | 6.52 | 100%      | 7.53 | 100%      | 4.03 | 55%       | 7.49 | 71%          | 7.60  | 71%       | 3.55 | 24%       | 4.25  | 35%        | 4.10  | 35%       |
| f5  | 1.97 | 100%      | 2.20 | 100%      | 1.94 | 100%      | 3.58 | 100%      | 4.20 | 100%         | 3.47  | 100%      | 6.42 | 100%      | 7.86  | 100%       | 6.12  | 100%      |
| f6  | 1.94 | 100%      | 2.18 | 100%      | 1.95 | 100%      | 3.47 | 100%      | 4.11 | 100%         | 3.41  | 100%      | 6.03 | 100%      | 7.50  | 100%       | 5.89  | 100%      |
| f7  | 1.96 | 100%      | 2.21 | 100%      | 1.94 | 100%      | 3.53 | 100%      | 4.14 | 100%         | 3.43  | 100%      | 6.20 | 100%      | 7.57  | 100%       | 5.94  | 100%      |

**Table 12 from page 8**

| 0                                                                              | 1                                                                                                                           |
|:-------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------|
|                                                                                | ADVERSARIAL EXAMPLE CAN BE FOUND). EVALUATED ON 1000 RANDOM INSTANCES. WHEN THE SUCCESS IS NOT 100%, MEAN IS FOR SUCCESSFUL |
| ATTACKS ONLY.                                                                  |                                                                                                                             |
| To choose the optimal c, we perform 20 iterations of binary                    | ﬁrst 1, 000 test images on both the MNIST and CIFAR dataset                                                                 |
| search over c. For each selected value of c, we run 10, 000                    | with our approach, and ﬁnd the Pearson correlation coefﬁcient                                                               |
| iterations of gradient descent with the Adam optimizer. 9                      | r > .9.                                                                                                                     |
| The                                                                            | Given this, consider loss function f4 (the argument for f1 is                                                               |
| results of                                                                     |                                                                                                                             |
| this                                                                           |                                                                                                                             |
| analysis                                                                       |                                                                                                                             |
| are                                                                            |                                                                                                                             |
| in Table                                                                       |                                                                                                                             |
| III. We                                                                        |                                                                                                                             |
| evaluate                                                                       |                                                                                                                             |
| the quality of                                                                 | similar).                                                                                                                   |
| the adversarial examples found on the MNIST                                    | In order                                                                                                                    |
|                                                                                | for                                                                                                                         |
|                                                                                | the gradient descent attack to make any                                                                                     |
| and CIFAR datasets. The relative ordering of each objective                    | change initially,                                                                                                           |
|                                                                                | the constant c will have to be large enough                                                                                 |
| function is identical between the two datasets, so for brevity                 | that                                                                                                                        |
| we report only results for MNIST.                                              |                                                                                                                             |
|                                                                                | (cid:15) < c(f1(x + (cid:15)) − f1(x))                                                                                      |
| There is a factor of                                                           |                                                                                                                             |
| three difference in quality between the                                        |                                                                                                                             |
| best objective function and the worst. The choice of method                    | or, as (cid:15) → 0,                                                                                                        |
| for handling box constraints does not                                          | 1/c < |∇f1(x)|                                                                                                              |
| impact                                                                         |                                                                                                                             |
| the quality of                                                                 |                                                                                                                             |
| results as signiﬁcantly for                                                    |                                                                                                                             |
| the best minimization functions.                                               |                                                                                                                             |
|                                                                                | implying that c must be larger than the inverse of the gradient                                                             |
| In                                                                             |                                                                                                                             |
| fact,                                                                          |                                                                                                                             |
| the worst                                                                      |                                                                                                                             |
| performing                                                                     |                                                                                                                             |
| objective                                                                      |                                                                                                                             |
| function,                                                                      |                                                                                                                             |
| cross                                                                          |                                                                                                                             |
|                                                                                | to make progress, but                                                                                                       |
|                                                                                | is identical                                                                                                                |
|                                                                                | the gradient of f1                                                                                                          |
|                                                                                | to F (·)t                                                                                                                   |
| entropy loss,                                                                  |                                                                                                                             |
| is the approach that was most suggested in the                                 |                                                                                                                             |
|                                                                                | so will be tiny around the initial                                                                                          |
|                                                                                | image, meaning c will have                                                                                                  |
| literature previously [46],                                                    |                                                                                                                             |
| [42].                                                                          |                                                                                                                             |
|                                                                                | to be extremely large.                                                                                                      |
| Why are some loss functions better than others? When c =                       | However,                                                                                                                    |
|                                                                                | as                                                                                                                          |
|                                                                                | soon as we                                                                                                                  |
|                                                                                | leave                                                                                                                       |
|                                                                                | the                                                                                                                         |
|                                                                                | immediate vicinity of                                                                                                       |
| 0, gradient descent will not make any move away from the                       | the initial                                                                                                                 |
|                                                                                | image,                                                                                                                      |
|                                                                                | the gradient of ∇f1(x + δ) increases at an                                                                                  |
| initial                                                                        | exponential                                                                                                                 |
| image. However, a large c often causes the initial steps                       | rate, making the large constant c cause gradient                                                                            |
| of gradient descent                                                            | descent                                                                                                                     |
| to perform in an overly-greedy manner,                                         | to perform in an overly greedy manner.                                                                                      |
| only traveling in the direction which can most easily reduce                   | We verify all of                                                                                                            |
|                                                                                | this theory empirically. When we run our                                                                                    |
| f and ignoring the D loss — thus causing gradient descent                      | attack trying constants chosen from 10−10 to 1010 the average                                                               |
| to                                                                             |                                                                                                                             |
| ﬁnd sub-optimal solutions.                                                     | constant                                                                                                                    |
|                                                                                | for                                                                                                                         |
|                                                                                | loss function f4 was 106.                                                                                                   |
| This means                                                                     |                                                                                                                             |
| that                                                                           |                                                                                                                             |
| for                                                                            |                                                                                                                             |
| loss                                                                           |                                                                                                                             |
| there                                                                          |                                                                                                                             |
| is no                                                                          |                                                                                                                             |
| function f1                                                                    |                                                                                                                             |
| and f4,                                                                        |                                                                                                                             |
|                                                                                | The average gradient of the loss function f1 around the valid                                                               |
| c                                                                              | image is 2−20 but 2−1 at                                                                                                    |
| good                                                                           | the closest adversarial example. This                                                                                       |
| constant                                                                       |                                                                                                                             |
| that                                                                           |                                                                                                                             |
| is                                                                             |                                                                                                                             |
| useful                                                                         |                                                                                                                             |
| throughout                                                                     |                                                                                                                             |
| the                                                                            |                                                                                                                             |
| duration                                                                       |                                                                                                                             |
| of                                                                             |                                                                                                                             |
| the gradient descent search. Since the constant c weights the                  | means c is a million times                                                                                                  |
|                                                                                | larger                                                                                                                      |
|                                                                                | than it has                                                                                                                 |
|                                                                                | to be, causing                                                                                                              |
| relative importance of                                                         |                                                                                                                             |
| the distance term and the loss term,                                           |                                                                                                                             |
| in                                                                             |                                                                                                                             |
|                                                                                | to perform worse than any of                                                                                                |
|                                                                                | the                                                                                                                         |
|                                                                                | the loss function f4 and f1                                                                                                 |
| order                                                                          | others.                                                                                                                     |
| for a ﬁxed constant c to be useful,                                            |                                                                                                                             |
| the relative value of                                                          |                                                                                                                             |
| these two terms                                                                |                                                                                                                             |
| should remain approximately equal. This                                        |                                                                                                                             |
| is                                                                             |                                                                                                                             |
| not                                                                            | D. Discretization                                                                                                           |
| the case for                                                                   |                                                                                                                             |
| these two loss functions.                                                      |                                                                                                                             |
| To explain why this is the case, we will have to take a side                   |                                                                                                                             |
|                                                                                | We model pixel                                                                                                              |
|                                                                                | intensities as a (continuous) real number in                                                                                |
| discussion to analyze how adversarial examples exist. Consider                 |                                                                                                                             |
|                                                                                | the range [0, 1]. However, in a valid image, each pixel intensity                                                           |
| a valid input x and an adversarial example x(cid:48) on a network.             |                                                                                                                             |
|                                                                                | must be a (discrete) integer in the range {0, 1, . . . , 255}. This                                                         |
| What does it                                                                   |                                                                                                                             |
| look like as we linearly interpolate from x to                                 |                                                                                                                             |
|                                                                                | additional                                                                                                                  |
|                                                                                | requirement                                                                                                                 |
|                                                                                | is                                                                                                                          |
|                                                                                | not                                                                                                                         |
|                                                                                | captured                                                                                                                    |
|                                                                                | in                                                                                                                          |
|                                                                                | our                                                                                                                         |
|                                                                                | formulation.                                                                                                                |
| x(cid:48)? That is, let y = αx+(1−α)x(cid:48) for α ∈ [0, 1]. It turns out the |                                                                                                                             |
|                                                                                | In                                                                                                                          |
|                                                                                | practice, we                                                                                                                |
|                                                                                | ignore                                                                                                                      |
|                                                                                | the                                                                                                                         |
|                                                                                | integrality                                                                                                                 |
|                                                                                | constraints,                                                                                                                |
|                                                                                | solve                                                                                                                       |
|                                                                                | the                                                                                                                         |
| is mostly linear from the input to the adversarial                             |                                                                                                                             |
| value of Z(·)t                                                                 |                                                                                                                             |
|                                                                                | continuous optimization problem, and then round to the nearest                                                              |
| is a logistic. We verify this                                                  |                                                                                                                             |
| example, and therefore the F (·)t                                              |                                                                                                                             |
|                                                                                | integer:                                                                                                                    |
|                                                                                | the intensity of the ith pixel becomes (cid:98)255(xi + δi)(cid:101).                                                       |
| fact empirically by constructing adversarial examples on the                   |                                                                                                                             |
|                                                                                | This                                                                                                                        |
|                                                                                | rounding will                                                                                                               |
|                                                                                | slightly                                                                                                                    |
|                                                                                | degrade                                                                                                                     |
|                                                                                | the                                                                                                                         |
|                                                                                | quality                                                                                                                     |
|                                                                                | of                                                                                                                          |
|                                                                                | the                                                                                                                         |
|                                                                                | adversarial example.                                                                                                        |
|                                                                                | If we need to restore the attack quality,                                                                                   |
| 9Adam converges to 95% of optimum within 1, 000 iterations 92% of the          |                                                                                                                             |
| time. For completeness we run it                                               | we perform greedy search on the lattice deﬁned by the discrete                                                              |
| for 10, 000 iterations at each step.                                           |                                                                                                                             |



## Page 9

Target Classiﬁcation (L2)
0
1
2
3
4
5
6
7
8
9
Source Classiﬁcation
9
8
7
6
5
4
3
2
1
0
Fig. 3. Our L2 adversary applied to the MNIST dataset performing a targeted
attack for every source/target pair. Each digit is the ﬁrst image in the dataset
with that label.
solutions by changing one pixel value at a time. This greedy
search never failed for any of our attacks.
Prior work has largely ignored the integrality constraints.10
For instance, when using the fast gradient sign attack with ϵ =
0.1 (i.e., changing pixel values by 10%), discretization rarely
affects the success rate of the attack. In contrast, in our work,
we are able to ﬁnd attacks that make much smaller changes
to the images, so discretization effects cannot be ignored. We
take care to always generate valid images; when reporting the
success rate of our attacks, they always are for attacks that
include the discretization post-processing.
VI. OUR THREE ATTACKS
A. Our L2 Attack
Putting these ideas together, we obtain a method for ﬁnding
adversarial examples that will have low distortion in the L2
metric. Given x, we choose a target class t (such that we have
t ̸= C∗(x)) and then search for w that solves
minimize ∥1
2(tanh(w) + 1) −x∥2
2 + c · f(1
2(tanh(w) + 1)
with f deﬁned as
f(x′) = max(max{Z(x′)i : i ̸= t} −Z(x′)t, −κ).
This f is based on the best objective function found earlier,
modiﬁed slightly so that we can control the conﬁdence with
which the misclassiﬁcation occurs by adjusting κ. The param-
eter κ encourages the solver to ﬁnd an adversarial instance
x′ that will be classiﬁed as class t with high conﬁdence. We
set κ = 0 for our attacks but we note here that a side beneﬁt
10One exception: The JSMA attack [38] handles this by only setting the
output value to either 0 or 255.
Target Classiﬁcation (L0)
0
1
2
3
4
5
6
7
8
9
Source Classiﬁcation
9
8
7
6
5
4
3
2
1
0
Fig. 4. Our L0 adversary applied to the MNIST dataset performing a targeted
attack for every source/target pair. Each digit is the ﬁrst image in the dataset
with that label.
of this formulation is it allows one to control for the desired
conﬁdence. This is discussed further in Section VIII-D.
Figure 3 shows this attack applied to our MNIST model
for each source digit and target digit. Almost all attacks are
visually indistinguishable from the original digit.
A comparable ﬁgure (Figure 12) for CIFAR is in the ap-
pendix. No attack is visually distinguishable from the baseline
image.
Multiple starting-point gradient descent. The main problem
with gradient descent is that its greedy search is not guaranteed
to ﬁnd the optimal solution and can become stuck in a local
minimum. To remedy this, we pick multiple random starting
points close to the original image and run gradient descent
from each of those points for a ﬁxed number of iterations.
We randomly sample points uniformly from the ball of radius
r, where r is the closest adversarial example found so far.
Starting from multiple starting points reduces the likelihood
that gradient descent gets stuck in a bad local minimum.
B. Our L0 Attack
The L0 distance metric is non-differentiable and therefore
is ill-suited for standard gradient descent. Instead, we use an
iterative algorithm that, in each iteration, identiﬁes some pixels
that don’t have much effect on the classiﬁer output and then
ﬁxes those pixels, so their value will never be changed. The
set of ﬁxed pixels grows in each iteration until we have, by
process of elimination, identiﬁed a minimal (but possibly not
minimum) subset of pixels that can be modiﬁed to generate an
adversarial example. In each iteration, we use our L2 attack
to identify which pixels are unimportant.
In more detail, on each iteration, we call the L2 adversary,
restricted to only modify the pixels in the allowed set. Let
9


**Table 13 from page 9**

| 0                                                                           | 1                                                                           |
|:----------------------------------------------------------------------------|:----------------------------------------------------------------------------|
| 8                                                                           | 8                                                                           |
| 9                                                                           | 9                                                                           |
| Fig. 3. Our L2 adversary applied to the MNIST dataset performing a targeted | Fig. 4. Our L0 adversary applied to the MNIST dataset performing a targeted |
| attack for every source/target pair. Each digit                             | attack for every source/target pair. Each digit                             |
| is the ﬁrst                                                                 | is the ﬁrst                                                                 |
| image in the dataset                                                        | image in the dataset                                                        |
| with that                                                                   | with that                                                                   |
| label.                                                                      | label.                                                                      |
| solutions by changing one pixel value at a time. This greedy                | of                                                                          |
|                                                                             | this formulation is it allows one to control                                |
|                                                                             | for                                                                         |
|                                                                             | the desired                                                                 |
| search never                                                                | conﬁdence. This is discussed further                                        |
| failed for any of our attacks.                                              | in Section VIII-D.                                                          |
| Prior work has largely ignored the integrality constraints.10               | Figure 3 shows                                                              |
|                                                                             | this                                                                        |
|                                                                             | attack applied to our MNIST model                                           |
| For instance, when using the fast gradient sign attack with (cid:15) =      | for each source digit and target digit. Almost all attacks are              |
| 0.1 (i.e., changing pixel values by 10%), discretization rarely             | visually indistinguishable from the original digit.                         |
| affects the success rate of the attack. In contrast,                        | A comparable ﬁgure (Figure 12)                                              |
| in our work,                                                                | for CIFAR is                                                                |
|                                                                             | in the ap-                                                                  |
| we are able to ﬁnd attacks                                                  | pendix. No attack is visually distinguishable from the baseline             |
| that make much smaller changes                                              |                                                                             |
| to the images, so discretization effects cannot be ignored. We              | image.                                                                      |
| take care to always generate valid images; when reporting the               |                                                                             |
|                                                                             | Multiple starting-point gradient descent. The main problem                  |
| success                                                                     |                                                                             |
| rate of our attacks,                                                        |                                                                             |
| they always are for attacks                                                 |                                                                             |
| that                                                                        |                                                                             |
|                                                                             | with gradient descent is that its greedy search is not guaranteed           |
| include the discretization post-processing.                                 |                                                                             |
|                                                                             | to ﬁnd the optimal solution and can become stuck in a local                 |
| VI. OUR THREE ATTACKS                                                       | minimum. To remedy this, we pick multiple random starting                   |
|                                                                             | points                                                                      |
|                                                                             | close                                                                       |
|                                                                             | to the original                                                             |
|                                                                             | image                                                                       |
|                                                                             | and run gradient descent                                                    |
| A. Our L2 Attack                                                            |                                                                             |
|                                                                             | from each of                                                                |
|                                                                             | those points                                                                |
|                                                                             | for                                                                         |
|                                                                             | a ﬁxed number of                                                            |
|                                                                             | iterations.                                                                 |
| Putting these ideas together, we obtain a method for ﬁnding                 |                                                                             |
|                                                                             | We randomly sample points uniformly from the ball of radius                 |
| adversarial examples                                                        |                                                                             |
| that will have low distortion in the L2                                     |                                                                             |
|                                                                             | r                                                                           |
|                                                                             | r, where                                                                    |
|                                                                             | is                                                                          |
|                                                                             | the                                                                         |
|                                                                             | closest                                                                     |
|                                                                             | adversarial                                                                 |
|                                                                             | example                                                                     |
|                                                                             | found so far.                                                               |
| metric. Given x, we choose a target class t (such that we have              |                                                                             |
|                                                                             | Starting from multiple starting points                                      |
|                                                                             | reduces                                                                     |
|                                                                             | the likelihood                                                              |
| t (cid:54)= C ∗(x)) and then search for w that solves                       |                                                                             |
|                                                                             | that gradient descent gets stuck in a bad local minimum.                    |
| 1 2                                                                         |                                                                             |
| 1 2                                                                         |                                                                             |
| (cid:107)                                                                   |                                                                             |
| (tanh(w) + 1) − x(cid:107)2                                                 |                                                                             |
| (tanh(w) + 1)                                                               |                                                                             |
| minimize                                                                    |                                                                             |
| 2 + c · f (                                                                 | B. Our L0 Attack                                                            |
|                                                                             | The L0 distance metric is non-differentiable and therefore                  |
| with f deﬁned as                                                            |                                                                             |
|                                                                             | is ill-suited for standard gradient descent.                                |
|                                                                             | Instead, we use an                                                          |
| f (x(cid:48)) = max(max{Z(x(cid:48))i                                       | iterative algorithm that, in each iteration, identiﬁes some pixels          |
| : i (cid:54)= t} − Z(x(cid:48))t, −κ).                                      |                                                                             |
|                                                                             | that don’t have much effect on the classiﬁer output and then                |
| This f                                                                      |                                                                             |
| is based on the best objective function found earlier,                      |                                                                             |
|                                                                             | ﬁxes                                                                        |
|                                                                             | those pixels,                                                               |
|                                                                             | so their value will never be changed. The                                   |
| modiﬁed slightly so that we can control                                     |                                                                             |
| the conﬁdence with                                                          |                                                                             |
|                                                                             | set of ﬁxed pixels grows                                                    |
|                                                                             | in each iteration until we have, by                                         |
| which the misclassiﬁcation occurs by adjusting κ. The param-                |                                                                             |
|                                                                             | process of elimination,                                                     |
|                                                                             | identiﬁed a minimal                                                         |
|                                                                             | (but possibly not                                                           |
| eter κ encourages                                                           |                                                                             |
| the                                                                         |                                                                             |
| solver                                                                      |                                                                             |
| to ﬁnd an adversarial                                                       |                                                                             |
| instance                                                                    |                                                                             |
|                                                                             | minimum) subset of pixels that can be modiﬁed to generate an                |
| x(cid:48)                                                                   |                                                                             |
| that will be classiﬁed as class t with high conﬁdence. We                   |                                                                             |
|                                                                             | adversarial example.                                                        |
|                                                                             | attack                                                                      |
|                                                                             | In each iteration, we use our L2                                            |
| set κ = 0 for our attacks but we note here that a side beneﬁt               |                                                                             |
|                                                                             | to identify which pixels are unimportant.                                   |
|                                                                             | In more detail, on each iteration, we call                                  |
|                                                                             | the L2 adversary,                                                           |
| 10One exception: The JSMA attack [38] handles                               |                                                                             |
| this by only setting the                                                    |                                                                             |
| output value to either 0 or 255.                                            | restricted to only modify the pixels                                        |
|                                                                             | in the                                                                      |
|                                                                             | allowed set. Let                                                            |



## Page 10

δ be the solution returned from the L2 adversary on input
image x, so that x+δ is an adversarial example. We compute
g = ∇f(x + δ) (the gradient of the objective function,
evaluated at the adversarial instance). We then select the pixel
i = arg mini gi · δi and ﬁx i, i.e., remove i from the allowed
set.11 The intuition is that gi·δi tells us how much reduction to
f(·) we obtain from the ith pixel of the image, when moving
from x to x + δ: gi tells us how much reduction in f we
obtain, per unit change to the ith pixel, and we multiply this
by how much the ith pixel has changed. This process repeats
until the L2 adversary fails to ﬁnd an adversarial example.
There is one ﬁnal detail required to achieve strong results:
choosing a constant c to use for the L2 adversary. To do this,
we initially set c to a very low value (e.g., 10−4). We then
run our L2 adversary at this c-value. If it fails, we double c
and try again, until it is successful. We abort the search if c
exceeds a ﬁxed threshold (e.g., 1010).
JSMA grows a set — initially empty — of pixels that are
allowed to be changed and sets the pixels to maximize the total
loss. In contrast, our attack shrinks the set of pixels — initially
containing every pixel — that are allowed to be changed.
Our algorithm is signiﬁcantly more effective than JSMA
(see Section VII for an evaluation). It is also efﬁcient: we
introduce optimizations that make it about as fast as our L2
attack with a single starting point on MNIST and CIFAR; it is
substantially slower on ImageNet. Instead of starting gradient
descent in each iteration from the initial image, we start the
gradient descent from the solution found on the previous
iteration (“warm-start”). This dramatically reduces the number
of rounds of gradient descent needed during each iteration, as
the solution with k pixels held constant is often very similar
to the solution with k + 1 pixels held constant.
Figure 4 shows the L0 attack applied to one digit of each
source class, targeting each target class, on the MNIST dataset.
The attacks are visually noticeable, implying the L0 attack is
more difﬁcult than L2. Perhaps the worst case is that of a 7
being made to classify as a 6; interestingly, this attack for L2
is one of the only visually distinguishable attacks.
A comparable ﬁgure (Figure 11) for CIFAR is in the
appendix.
C. Our L∞Attack
The L∞distance metric is not fully differentiable and
standard gradient descent does not perform well for it. We
experimented with naively optimizing
minimize
c · f(x + δ) + ∥δ∥∞
However, we found that gradient descent produces very poor
results: the ∥δ∥∞term only penalizes the largest (in absolute
value) entry in δ and has no impact on any of the other. As
such, gradient descent very quickly becomes stuck oscillating
between two suboptimal solutions. Consider a case where δi =
0.5 and δj = 0.5 −ϵ. The L∞norm will only penalize δi,
11Selecting the index i that minimizes δi is simpler, but it yields results
with 1.5× higher L0 distortion.
Target Classiﬁcation (L∞)
0
1
2
3
4
5
6
7
8
9
Source Classiﬁcation
9
8
7
6
5
4
3
2
1
0
Fig. 5. Our L∞adversary applied to the MNIST dataset performing a targeted
attack for every source/target pair. Each digit is the ﬁrst image in the dataset
with that label.
not δj, and
∂
∂δj ∥δ∥∞will be zero at this point. Thus, the
gradient imposes no penalty for increasing δj, even though it
is already large. On the next iteration we might move to a
position where δj is slightly larger than δi, say δi = 0.5 −ϵ′
and δj = 0.5 + ϵ′′, a mirror image of where we started. In
other words, gradient descent may oscillate back and forth
across the line δi = δj = 0.5, making it nearly impossible to
make progress.
We resolve this issue using an iterative attack. We replace
the L2 term in the objective function with a penalty for any
terms that exceed τ (initially 1, decreasing in each iteration).
This prevents oscillation, as this loss term penalizes all large
values simultaneously. Speciﬁcally, in each iteration we solve
minimize
c · f(x + δ) + ·
X
i

(δi −τ)+
After each iteration, if δi < τ for all i, we reduce τ by a factor
of 0.9 and repeat; otherwise, we terminate the search.
Again we must choose a good constant c to use for the
L∞adversary. We take the same approach as we do for the
L0 attack: initially set c to a very low value and run the L∞
adversary at this c-value. If it fails, we double c and try again,
until it is successful. We abort the search if c exceeds a ﬁxed
threshold.
Using “warm-start” for gradient descent in each iteration,
this algorithm is about as fast as our L2 algorithm (with a
single starting point).
Figure 5 shows the L∞attack applied to one digit of each
source class, targeting each target class, on the MNSIT dataset.
While most differences are not visually noticeable, a few are.
Again, the worst case is that of a 7 being made to classify as
a 6.
10


**Table 14 from page 10**

| 0                                                                   | 1                                                                           |
|:--------------------------------------------------------------------|:----------------------------------------------------------------------------|
| δ                                                                   | Target Classiﬁcation (L∞)                                                   |
| be                                                                  |                                                                             |
| the                                                                 |                                                                             |
| adversary on input                                                  |                                                                             |
| solution returned from the L2                                       |                                                                             |
| image x, so that x + δ is an adversarial example. We compute        | 0                                                                           |
|                                                                     | 1                                                                           |
|                                                                     | 2                                                                           |
|                                                                     | 3                                                                           |
|                                                                     | 4                                                                           |
|                                                                     | 5                                                                           |
|                                                                     | 6                                                                           |
|                                                                     | 7                                                                           |
|                                                                     | 8                                                                           |
|                                                                     | 9                                                                           |
| g = ∇f (x + δ)                                                      |                                                                             |
| (the                                                                |                                                                             |
| gradient                                                            |                                                                             |
| of                                                                  |                                                                             |
| the                                                                 |                                                                             |
| objective                                                           |                                                                             |
| function,                                                           |                                                                             |
|                                                                     | 0                                                                           |
| evaluated at                                                        |                                                                             |
| the adversarial                                                     |                                                                             |
| instance). We then select                                           |                                                                             |
| the pixel                                                           |                                                                             |
|                                                                     | 1                                                                           |
| i.e.,                                                               |                                                                             |
| remove i                                                            |                                                                             |
| from the allowed                                                    |                                                                             |
| i = arg mini gi · δi and ﬁx i,                                      |                                                                             |
| tells us how much reduction to                                      |                                                                             |
| set.11 The intuition is that gi ·δi                                 |                                                                             |
|                                                                     | 2                                                                           |
| f (·) we obtain from the ith pixel of the image, when moving        |                                                                             |
| tells us how much reduction in f we                                 |                                                                             |
| from x to x + δ: gi                                                 |                                                                             |
|                                                                     | 3                                                                           |
| obtain, per unit change to the ith pixel, and we multiply this      |                                                                             |
|                                                                     | 4                                                                           |
| by how much the ith pixel has changed. This process repeats         | SourceClassiﬁcation                                                         |
| until                                                               | 5                                                                           |
| the L2 adversary fails to ﬁnd an adversarial example.               |                                                                             |
| There is one ﬁnal detail                                            |                                                                             |
| required to achieve strong results:                                 |                                                                             |
|                                                                     | 6                                                                           |
| choosing a constant c to use for the L2 adversary. To do this,      |                                                                             |
| we initially set c to a very low value (e.g., 10−4). We then        |                                                                             |
|                                                                     | 7                                                                           |
| this c-value.                                                       |                                                                             |
| If                                                                  |                                                                             |
| it                                                                  |                                                                             |
| fails, we double c                                                  |                                                                             |
| run our L2 adversary at                                             |                                                                             |
|                                                                     | 8                                                                           |
| and try again, until                                                |                                                                             |
| it                                                                  |                                                                             |
| is successful. We abort                                             |                                                                             |
| the search if c                                                     |                                                                             |
| exceeds a ﬁxed threshold (e.g., 1010).                              |                                                                             |
|                                                                     | 9                                                                           |
| JSMA grows a set — initially empty — of pixels that are             |                                                                             |
| allowed to be changed and sets the pixels to maximize the total     |                                                                             |
| loss. In contrast, our attack shrinks the set of pixels — initially | Fig. 5. Our L∞ adversary applied to the MNIST dataset performing a targeted |
|                                                                     | attack for every source/target pair. Each digit                             |
|                                                                     | is the ﬁrst                                                                 |
|                                                                     | image in the dataset                                                        |
| containing every pixel — that are allowed to be changed.            |                                                                             |
|                                                                     | with that                                                                   |
|                                                                     | label.                                                                      |
| Our                                                                 |                                                                             |
| algorithm is                                                        |                                                                             |
| signiﬁcantly more                                                   |                                                                             |
| effective                                                           |                                                                             |
| than JSMA                                                           |                                                                             |
| (see Section VII                                                    |                                                                             |
| for                                                                 |                                                                             |
| an evaluation).                                                     |                                                                             |
| It                                                                  |                                                                             |
| is                                                                  |                                                                             |
| also efﬁcient: we                                                   |                                                                             |
|                                                                     | ∂                                                                           |
| introduce optimizations                                             |                                                                             |
| that make it about as                                               |                                                                             |
| fast as our L2                                                      |                                                                             |
|                                                                     | not                                                                         |
|                                                                     | and                                                                         |
|                                                                     | zero at                                                                     |
|                                                                     | this point. Thus,                                                           |
|                                                                     | the                                                                         |
|                                                                     | δj,                                                                         |
|                                                                     | (cid:107)δ(cid:107)∞ will be                                                |
|                                                                     | ∂δj                                                                         |
| attack with a single starting point on MNIST and CIFAR;             |                                                                             |
| it                                                                  |                                                                             |
| is                                                                  |                                                                             |
|                                                                     | gradient                                                                    |
|                                                                     | imposes no penalty for                                                      |
|                                                                     | increasing δj, even though it                                               |
| substantially slower on ImageNet. Instead of starting gradient      |                                                                             |
|                                                                     | is                                                                          |
|                                                                     | already large. On the next                                                  |
|                                                                     | iteration we might move                                                     |
|                                                                     | to a                                                                        |
| descent                                                             |                                                                             |
| in each iteration from the initial                                  |                                                                             |
| image, we start                                                     |                                                                             |
| the                                                                 |                                                                             |
|                                                                     | is slightly larger                                                          |
|                                                                     | position where δj                                                           |
|                                                                     | than δi, say δi = 0.5 − (cid:15)(cid:48)                                    |
| gradient                                                            |                                                                             |
| descent                                                             |                                                                             |
| from the                                                            |                                                                             |
| solution                                                            |                                                                             |
| found                                                               |                                                                             |
| on                                                                  |                                                                             |
| the                                                                 |                                                                             |
| previous                                                            |                                                                             |
|                                                                     | image of where we started.                                                  |
|                                                                     | In                                                                          |
|                                                                     | and δj = 0.5 + (cid:15)(cid:48)(cid:48), a mirror                           |
| iteration (“warm-start”). This dramatically reduces the number      |                                                                             |
|                                                                     | other words, gradient descent may oscillate back and forth                  |
| of rounds of gradient descent needed during each iteration, as      |                                                                             |
|                                                                     | across the line δi = δj = 0.5, making it nearly impossible to               |
| the solution with k pixels held constant                            |                                                                             |
| is often very similar                                               |                                                                             |
|                                                                     | make progress.                                                              |
| to the solution with k + 1 pixels held constant.                    |                                                                             |
|                                                                     | We resolve this issue using an iterative attack. We replace                 |
| Figure 4 shows the L0 attack applied to one digit of each           |                                                                             |
|                                                                     | term in the objective function with a penalty for any                       |
|                                                                     | the L2                                                                      |
| source class, targeting each target class, on the MNIST dataset.    |                                                                             |
|                                                                     | terms that exceed τ                                                         |
|                                                                     | (initially 1, decreasing in each iteration).                                |
| The attacks are visually noticeable,                                |                                                                             |
| implying the L0 attack is                                           |                                                                             |
|                                                                     | This prevents oscillation, as this loss term penalizes all                  |
|                                                                     | large                                                                       |
| more difﬁcult                                                       |                                                                             |
| than L2. Perhaps the worst case is that of a 7                      |                                                                             |
|                                                                     | values simultaneously. Speciﬁcally,                                         |
|                                                                     | in each iteration we solve                                                  |
| being made to classify as a 6;                                      |                                                                             |
| interestingly,                                                      |                                                                             |
| this attack for L2                                                  |                                                                             |
| is one of                                                           | (cid:2)(δi − τ )+(cid:3)                                                    |
| the only visually distinguishable attacks.                          |                                                                             |
|                                                                     | (cid:88) i                                                                  |
|                                                                     | c · f (x + δ) + ·                                                           |
|                                                                     | minimize                                                                    |
| A comparable                                                        |                                                                             |
| ﬁgure                                                               |                                                                             |
| (Figure                                                             |                                                                             |
| 11)                                                                 |                                                                             |
| for CIFAR is                                                        |                                                                             |
| in                                                                  |                                                                             |
| the                                                                 |                                                                             |
| appendix.                                                           |                                                                             |
|                                                                     | After each iteration, if δi < τ for all i, we reduce τ by a factor          |
| C. Our L∞ Attack                                                    | of 0.9 and repeat; otherwise, we terminate the search.                      |
|                                                                     | c                                                                           |
|                                                                     | Again we must                                                               |
|                                                                     | choose                                                                      |
|                                                                     | a good constant                                                             |
|                                                                     | to use                                                                      |
|                                                                     | for                                                                         |
|                                                                     | the                                                                         |
| is                                                                  |                                                                             |
| not                                                                 |                                                                             |
| fully                                                               |                                                                             |
| differentiable                                                      |                                                                             |
| and                                                                 |                                                                             |
| The L∞ distance metric                                              |                                                                             |
|                                                                     | the                                                                         |
|                                                                     | L∞ adversary. We take the same approach as we do for                        |
| standard gradient descent does not perform well                     |                                                                             |
| for                                                                 |                                                                             |
| it. We                                                              |                                                                             |
|                                                                     | L0 attack:                                                                  |
|                                                                     | initially set c to a very low value and run the L∞                          |
| experimented with naively optimizing                                |                                                                             |
|                                                                     | adversary at                                                                |
|                                                                     | this c-value. If it fails, we double c and try again,                       |
| minimize                                                            | until                                                                       |
| c · f (x + δ) + (cid:107)δ(cid:107)∞                                | it                                                                          |
|                                                                     | is successful. We abort                                                     |
|                                                                     | the search if c exceeds a ﬁxed                                              |
|                                                                     | threshold.                                                                  |
| However, we found that gradient descent produces very poor          |                                                                             |
|                                                                     | Using “warm-start” for gradient descent                                     |
|                                                                     | in each iteration,                                                          |
| results:                                                            |                                                                             |
| (in absolute                                                        |                                                                             |
| the (cid:107)δ(cid:107)∞ term only penalizes the largest            |                                                                             |
|                                                                     | this                                                                        |
|                                                                     | algorithm is                                                                |
|                                                                     | about                                                                       |
|                                                                     | as                                                                          |
|                                                                     | fast                                                                        |
|                                                                     | algorithm (with a                                                           |
|                                                                     | as our L2                                                                   |
| value) entry in δ and has no impact on any of                       |                                                                             |
| the other. As                                                       |                                                                             |
|                                                                     | single starting point).                                                     |
| such, gradient descent very quickly becomes stuck oscillating       |                                                                             |
|                                                                     | Figure 5 shows the L∞ attack applied to one digit of each                   |
| between two suboptimal solutions. Consider a case where δi =        |                                                                             |
|                                                                     | source class, targeting each target class, on the MNSIT dataset.            |
| 0.5 and δj = 0.5 − (cid:15). The L∞ norm will only penalize δi,     |                                                                             |
|                                                                     | While most differences are not visually noticeable, a few are.              |
|                                                                     | Again,                                                                      |
|                                                                     | the worst case is that of a 7 being made to classify as                     |
| 11Selecting the index i                                             |                                                                             |
| is                                                                  |                                                                             |
| simpler, but                                                        |                                                                             |
| it yields                                                           |                                                                             |
| results                                                             |                                                                             |
| that minimizes δi                                                   |                                                                             |
| with 1.5× higher L0 distortion.                                     | a 6.                                                                        |



## Page 11

A comparable ﬁgure (Figure 13) for CIFAR is in the ap-
pendix. No attack is visually distinguishable from the baseline
image.
VII. ATTACK EVALUATION
We compare our targeted attacks to the best results pre-
viously reported in prior publications, for each of the three
distance metrics.
We re-implement Deepfool, fast gradient sign, and iterative
gradient sign. For fast gradient sign, we search over ϵ to ﬁnd
the smallest distance that generates an adversarial example;
failures is returned if no ϵ produces the target class. Our
iterative gradient sign method is similar: we search over ϵ
(ﬁxing α =
1
256) and return the smallest successful.
For JSMA we use the implementation in CleverHans [35]
with only slight modiﬁcation (we improve performance by
50× with no impact on accuracy).
JSMA is unable to run on ImageNet due to an inherent
signiﬁcant computational cost: recall that JSMA performs
search for a pair of pixels p, q that can be changed together
that make the target class more likely and other classes less
likely. ImageNet represents images as 299 × 299 × 3 vectors,
so searching over all pairs of pixels would require 236 work
on each step of the calculation. If we remove the search over
pairs of pixels, the success of JSMA falls off dramatically. We
therefore report it as failing always on ImageNet.
We report success if the attack produced an adversarial
example with the correct target label, no matter how much
change was required. Failure indicates the case where the
attack was entirely unable to succeed.
We evaluate on the ﬁrst 1, 000 images in the test set on
CIFAR and MNSIT. On ImageNet, we report on 1, 000 images
that were initially classiﬁed correctly by Inception v3 12. On
ImageNet we approximate the best-case and worst-case results
by choosing 100 target classes (10%) at random.
The results are found in Table IV for MNIST and CIFAR,
and Table V for ImageNet. 13
For each distance metric, across all three datasets, our
attacks ﬁnd closer adversarial examples than the previous
state-of-the-art attacks, and our attacks never fail to ﬁnd an
adversarial example. Our L0 and L2 attacks ﬁnd adversarial
examples with 2× to 10× lower distortion than the best pre-
viously published attacks, and succeed with 100% probability.
Our L∞attacks are comparable in quality to prior work, but
their success rate is higher. Our L∞attacks on ImageNet are so
successful that we can change the classiﬁcation of an image
to any desired label by only ﬂipping the lowest bit of each
pixel, a change that would be impossible to detect visually.
As the learning task becomes increasingly more difﬁcult, the
previous attacks produce worse results, due to the complexity
of the model. In contrast, our attacks perform even better as
the task complexity increases. We have found JSMA is unable
12Otherwise the best-case attack results would appear to succeed extremely
often artiﬁcially low due to the relatively low top-1 accuracy
13The complete code to reproduce these tables and ﬁgures is available
online at http://nicholas.carlini.com/code/nn robust attacks.
Target Classiﬁcation
0
1
2
3
4
5
6
7
8
9
Distance Metric
L∞
L2
L0
Fig. 6. Targeted attacks for each of the 10 MNIST digits where the starting
image is totally black for each of the three distance metrics.
Target Classiﬁcation
0
1
2
3
4
5
6
7
8
9
Distance Metric
L∞
L2
L0
Fig. 7. Targeted attacks for each of the 10 MNIST digits where the starting
image is totally white for each of the three distance metrics.
to ﬁnd targeted L0 adversarial examples on ImageNet, whereas
ours is able to with 100% success.
It is important to realize that the results between models
are not directly comparable. For example, even though a L0
adversary must change 10 times as many pixels to switch an
ImageNet classiﬁcation compared to a MNIST classiﬁcation,
ImageNet has 114× as many pixels and so the fraction of
pixels that must change is signiﬁcantly smaller.
Generating synthetic digits. With our targeted adversary,
we can start from any image we want and ﬁnd adversarial
examples of each given target. Using this, in Figure 6 we
show the minimum perturbation to an entirely-black image
required to make it classify as each digit, for each of the
distance metrics.
This experiment was performed for the L0 task previously
[38], however when mounting their attack, “for classes 0, 2,
3 and 5 one can clearly recognize the target digit.” With our
more powerful attacks, none of the digits are recognizable.
Figure 7 performs the same analysis starting from an all-white
image.
Notice that the all-black image requires no change to
become a digit 1 because it is initially classiﬁed as a 1, and
the all-white image requires no change to become a 8 because
the initial image is already an 8.
Runtime Analysis. We believe there are two reasons why one
may consider the runtime performance of adversarial example
generation algorithms important: ﬁrst, to understand if the
performance would be prohibitive for an adversary to actually
mount the attacks, and second, to be used as an inner loop in
11


**Table 15 from page 11**

| 0                                                                          | 1                                                              |
|:---------------------------------------------------------------------------|:---------------------------------------------------------------|
| A comparable ﬁgure (Figure 13)                                             | Target Classiﬁcation                                           |
| for CIFAR is                                                               |                                                                |
| in the ap-                                                                 |                                                                |
| pendix. No attack is visually distinguishable from the baseline            | 0                                                              |
|                                                                            | 1                                                              |
|                                                                            | 2                                                              |
|                                                                            | 3                                                              |
|                                                                            | 4                                                              |
|                                                                            | 5                                                              |
|                                                                            | 6                                                              |
|                                                                            | 7                                                              |
|                                                                            | 8                                                              |
|                                                                            | 9                                                              |
| image.                                                                     | L0                                                             |
| VII. ATTACK EVALUATION                                                     |                                                                |
| We                                                                         | DistanceMetric                                                 |
| compare our                                                                | L2                                                             |
| targeted attacks                                                           |                                                                |
| to the best                                                                |                                                                |
| results pre-                                                               |                                                                |
| viously reported in prior publications,                                    |                                                                |
| for                                                                        |                                                                |
| each of                                                                    |                                                                |
| the                                                                        |                                                                |
| three                                                                      |                                                                |
| distance metrics.                                                          | L∞                                                             |
| We re-implement Deepfool, fast gradient sign, and iterative                |                                                                |
| gradient sign. For                                                         |                                                                |
| fast gradient sign, we search over (cid:15) to ﬁnd                         |                                                                |
|                                                                            | Fig. 6.                                                        |
|                                                                            | Targeted attacks for each of                                   |
|                                                                            | the 10 MNIST digits where the starting                         |
| the                                                                        |                                                                |
| smallest distance                                                          |                                                                |
| that generates                                                             |                                                                |
| an adversarial                                                             |                                                                |
| example;                                                                   |                                                                |
|                                                                            | image is totally black for each of                             |
|                                                                            | the three distance metrics.                                    |
| (cid:15)                                                                   |                                                                |
| failures                                                                   |                                                                |
| is                                                                         |                                                                |
| returned                                                                   |                                                                |
| if                                                                         |                                                                |
| no                                                                         |                                                                |
| produces                                                                   |                                                                |
| the                                                                        |                                                                |
| target                                                                     |                                                                |
| class. Our                                                                 |                                                                |
| (cid:15)                                                                   |                                                                |
| iterative gradient                                                         |                                                                |
| sign method is                                                             |                                                                |
| similar: we                                                                |                                                                |
| search over                                                                |                                                                |
| 1                                                                          | Target Classiﬁcation                                           |
| (ﬁxing α =                                                                 |                                                                |
| 256 ) and return the smallest successful.                                  |                                                                |
|                                                                            | 0                                                              |
|                                                                            | 1                                                              |
|                                                                            | 2                                                              |
|                                                                            | 3                                                              |
|                                                                            | 4                                                              |
|                                                                            | 5                                                              |
|                                                                            | 6                                                              |
|                                                                            | 7                                                              |
|                                                                            | 8                                                              |
|                                                                            | 9                                                              |
| For                                                                        |                                                                |
| JSMA we use the implementation in CleverHans                               |                                                                |
| [35]                                                                       |                                                                |
| with                                                                       | L0                                                             |
| only                                                                       |                                                                |
| slight modiﬁcation                                                         |                                                                |
| (we                                                                        |                                                                |
| improve                                                                    |                                                                |
| performance                                                                |                                                                |
| by                                                                         |                                                                |
| 50× with no impact on accuracy).                                           |                                                                |
| JSMA is unable                                                             | DistanceMetric                                                 |
| to run on ImageNet due                                                     | L2                                                             |
| to an inherent                                                             |                                                                |
| signiﬁcant                                                                 |                                                                |
| computational                                                              |                                                                |
| cost:                                                                      |                                                                |
| recall                                                                     |                                                                |
| that                                                                       |                                                                |
| JSMA performs                                                              |                                                                |
| search for a pair of pixels p, q that can be changed together              | L∞                                                             |
| that make the target class more likely and other classes                   |                                                                |
| less                                                                       |                                                                |
| likely.                                                                    |                                                                |
| ImageNet                                                                   |                                                                |
| represents images as 299 × 299 × 3 vectors,                                |                                                                |
| so searching over all pairs of pixels would require 236 work               |                                                                |
|                                                                            | Fig. 7.                                                        |
|                                                                            | Targeted attacks for each of                                   |
|                                                                            | the 10 MNIST digits where the starting                         |
| on each step of                                                            | image is totally white for each of                             |
| the calculation.                                                           | the three distance metrics.                                    |
| If we remove the search over                                               |                                                                |
| pairs of pixels,                                                           |                                                                |
| the success of JSMA falls off dramatically. We                             |                                                                |
| therefore report                                                           |                                                                |
| it as failing always on ImageNet.                                          |                                                                |
|                                                                            | to ﬁnd targeted L0 adversarial examples on ImageNet, whereas   |
| We                                                                         |                                                                |
| report                                                                     |                                                                |
| success                                                                    |                                                                |
| if                                                                         |                                                                |
| the                                                                        |                                                                |
| attack                                                                     |                                                                |
| produced                                                                   |                                                                |
| an                                                                         |                                                                |
| adversarial                                                                |                                                                |
|                                                                            | ours is able to with 100% success.                             |
| example with the                                                           |                                                                |
| correct                                                                    |                                                                |
| target                                                                     |                                                                |
| label, no matter how much                                                  |                                                                |
|                                                                            | It                                                             |
|                                                                            | is                                                             |
|                                                                            | important                                                      |
|                                                                            | to realize                                                     |
|                                                                            | that                                                           |
|                                                                            | the                                                            |
|                                                                            | results between models                                         |
| change was                                                                 |                                                                |
| required. Failure                                                          |                                                                |
| indicates                                                                  |                                                                |
| the                                                                        |                                                                |
| case where                                                                 |                                                                |
| the                                                                        |                                                                |
|                                                                            | are not directly comparable. For example, even though a L0     |
| attack was entirely unable to succeed.                                     |                                                                |
|                                                                            | adversary must change 10 times as many pixels to switch an     |
| We                                                                         |                                                                |
| evaluate on the ﬁrst 1, 000 images                                         |                                                                |
| in the                                                                     |                                                                |
| test                                                                       |                                                                |
| set on                                                                     |                                                                |
|                                                                            | ImageNet classiﬁcation compared to a MNIST classiﬁcation,      |
| CIFAR and MNSIT. On ImageNet, we report on 1, 000 images                   |                                                                |
|                                                                            | fraction of                                                    |
|                                                                            | ImageNet has 114× as many pixels                               |
|                                                                            | and so the                                                     |
| that were initially classiﬁed correctly by Inception v3 12. On             |                                                                |
|                                                                            | pixels that must change is signiﬁcantly smaller.               |
| ImageNet we approximate the best-case and worst-case results               |                                                                |
| by choosing 100 target classes (10%) at                                    |                                                                |
| random.                                                                    |                                                                |
|                                                                            | Generating                                                     |
|                                                                            | synthetic                                                      |
|                                                                            | digits. With                                                   |
|                                                                            | our                                                            |
|                                                                            | targeted                                                       |
|                                                                            | adversary,                                                     |
| The results are found in Table IV for MNIST and CIFAR,                     |                                                                |
|                                                                            | we                                                             |
|                                                                            | can start                                                      |
|                                                                            | from any                                                       |
|                                                                            | image we want                                                  |
|                                                                            | and ﬁnd adversarial                                            |
| 13                                                                         |                                                                |
| and Table V for                                                            |                                                                |
| ImageNet.                                                                  |                                                                |
|                                                                            | examples of                                                    |
|                                                                            | each given target. Using this,                                 |
|                                                                            | in Figure 6 we                                                 |
| For                                                                        |                                                                |
| each                                                                       |                                                                |
| distance metric,                                                           |                                                                |
| across                                                                     |                                                                |
| all                                                                        |                                                                |
| three                                                                      |                                                                |
| datasets,                                                                  |                                                                |
| our                                                                        |                                                                |
|                                                                            | show the minimum perturbation to an entirely-black image       |
| attacks                                                                    |                                                                |
| ﬁnd                                                                        |                                                                |
| closer                                                                     |                                                                |
| adversarial                                                                |                                                                |
| examples                                                                   |                                                                |
| than                                                                       |                                                                |
| the                                                                        |                                                                |
| previous                                                                   |                                                                |
|                                                                            | required                                                       |
|                                                                            | to make                                                        |
|                                                                            | it                                                             |
|                                                                            | classify                                                       |
|                                                                            | as                                                             |
|                                                                            | each                                                           |
|                                                                            | digit,                                                         |
|                                                                            | for                                                            |
|                                                                            | each                                                           |
|                                                                            | of                                                             |
|                                                                            | the                                                            |
| state-of-the-art attacks, and our attacks never                            |                                                                |
| fail                                                                       |                                                                |
| to ﬁnd an                                                                  |                                                                |
|                                                                            | distance metrics.                                              |
| attacks ﬁnd adversarial                                                    |                                                                |
| adversarial example. Our L0                                                |                                                                |
| and L2                                                                     |                                                                |
|                                                                            | This experiment was performed for                              |
|                                                                            | task previously                                                |
|                                                                            | the L0                                                         |
| examples with 2× to 10× lower distortion than the best pre-                |                                                                |
|                                                                            | [38], however when mounting their attack, “for classes 0, 2,   |
| viously published attacks, and succeed with 100% probability.              |                                                                |
|                                                                            | 3 and 5 one can clearly recognize the target digit.” With our  |
| Our L∞ attacks are comparable in quality to prior work, but                |                                                                |
|                                                                            | more powerful                                                  |
|                                                                            | attacks, none of                                               |
|                                                                            | the digits                                                     |
|                                                                            | are                                                            |
|                                                                            | recognizable.                                                  |
| their success rate is higher. Our L∞ attacks on ImageNet are so            |                                                                |
|                                                                            | Figure 7 performs the same analysis starting from an all-white |
| successful                                                                 |                                                                |
| that we can change the classiﬁcation of an image                           |                                                                |
|                                                                            | image.                                                         |
| to any desired label by only ﬂipping the lowest bit of each                |                                                                |
|                                                                            | Notice                                                         |
|                                                                            | that                                                           |
|                                                                            | the                                                            |
|                                                                            | all-black                                                      |
|                                                                            | image                                                          |
|                                                                            | requires                                                       |
|                                                                            | no                                                             |
|                                                                            | change                                                         |
|                                                                            | to                                                             |
| pixel, a change that would be impossible to detect visually.               |                                                                |
|                                                                            | become a digit 1 because it                                    |
|                                                                            | is initially classiﬁed as a 1, and                             |
| As the learning task becomes increasingly more difﬁcult, the               |                                                                |
|                                                                            | the all-white image requires no change to become a 8 because   |
| previous attacks produce worse results, due to the complexity              |                                                                |
|                                                                            | the initial                                                    |
|                                                                            | image is already an 8.                                         |
| of                                                                         |                                                                |
| the model.                                                                 |                                                                |
| In contrast, our attacks perform even better as                            |                                                                |
| the task complexity increases. We have found JSMA is unable                | Runtime Analysis. We believe there are two reasons why one     |
|                                                                            | may consider the runtime performance of adversarial example    |
| 12Otherwise the best-case attack results would appear to succeed extremely |                                                                |
|                                                                            | generation                                                     |
|                                                                            | algorithms                                                     |
|                                                                            | important: ﬁrst,                                               |
|                                                                            | to                                                             |
|                                                                            | understand                                                     |
|                                                                            | if                                                             |
|                                                                            | the                                                            |
| often artiﬁcially low due to the relatively low top-1 accuracy             |                                                                |
|                                                                            | performance would be prohibitive for an adversary to actually  |
| 13The                                                                      |                                                                |
| complete                                                                   |                                                                |
| code                                                                       |                                                                |
| to                                                                         |                                                                |
| reproduce                                                                  |                                                                |
| these                                                                      |                                                                |
| tables                                                                     |                                                                |
| and ﬁgures                                                                 |                                                                |
| is                                                                         |                                                                |
| available                                                                  |                                                                |
| attacks.                                                                   | mount                                                          |
| online at http://nicholas.carlini.com/code/nn robust                       | the attacks, and second,                                       |
|                                                                            | to be used as an inner loop in                                 |



## Page 12

Best Case
Average Case
Worst Case
MNIST
CIFAR
MNIST
CIFAR
MNIST
CIFAR
mean
prob
mean
prob
mean
prob
mean
prob
mean
prob
mean
prob
Our L0
8.5
100%
5.9
100%
16
100%
13
100%
33
100%
24
100%
JSMA-Z
20
100%
20
100%
56
100%
58
100%
180
98%
150
100%
JSMA-F
17
100%
25
100%
45
100%
110
100%
100
100%
240
100%
Our L2
1.36
100%
0.17
100%
1.76
100%
0.33
100%
2.60
100%
0.51
100%
Deepfool
2.11
100%
0.85
100%
−
-
−
-
−
-
−
-
Our L∞
0.13
100%
0.0092 100%
0.16
100%
0.013
100%
0.23
100%
0.019
100%
Fast Gradient Sign
0.22
100%
0.015
99%
0.26
42%
0.029
51%
−
0%
0.34
1%
Iterative Gradient Sign
0.14
100%
0.0078 100%
0.19
100%
0.014
100%
0.26
100%
0.023
100%
TABLE IV
COMPARISON OF THE THREE VARIANTS OF TARGETED ATTACK TO PREVIOUS WORK FOR OUR MNIST AND CIFAR MODELS. WHEN SUCCESS RATE IS
NOT 100%, THE MEAN IS ONLY OVER SUCCESSES.
Untargeted
Average Case
Least Likely
mean
prob
mean
prob
mean
prob
Our L0
48
100%
410
100%
5200
100%
JSMA-Z
-
0%
-
0%
-
0%
JSMA-F
-
0%
-
0%
-
0%
Our L2
0.32
100%
0.96
100%
2.22
100%
Deepfool
0.91
100%
-
-
-
-
Our L∞
0.004
100%
0.006
100%
0.01
100%
FGS
0.004
100%
0.064
2%
-
0%
IGS
0.004
100%
0.01
99%
0.03
98%
TABLE V
COMPARISON OF THE THREE VARIANTS OF TARGETED ATTACK TO
PREVIOUS WORK FOR THE INCEPTION V3 MODEL ON IMAGENET. WHEN
SUCCESS RATE IS NOT 100%, THE MEAN IS ONLY OVER SUCCESSES.
adversarial re-training [11].
Comparing the exact runtime of attacks can be misleading.
For example, we have parallelized the implementation of
our L2 adversary allowing it to run hundreds of attacks
simultaneously on a GPU, increasing performance from 10×
to 100×. However, we did not parallelize our L0 or L∞
attacks. Similarly, our implementation of fast gradient sign
is parallelized, but JSMA is not. We therefore refrain from
giving exact performance numbers because we believe an
unfair comparison is worse than no comparison.
All of our attacks, and all previous attacks, are plenty
efﬁcient to be used by an adversary. No attack takes longer
than a few minutes to run on any given instance.
When compared to L0, our attacks are 2 × −10× slower
than our optimized JSMA algorithm (and signiﬁcantly faster
than the un-optimized version). Our attacks are typically 10×
−100× slower than previous attacks for L2 and L∞, with
exception of iterative gradient sign which we are 10× slower.
VIII. EVALUATING DEFENSIVE DISTILLATION
Distillation was initially proposed as an approach to reduce
a large model (the teacher) down to a smaller distilled model
[19]. At a high level, distillation works by ﬁrst training the
teacher model on the training set in a standard manner. Then,
we use the teacher to label each instance in the training set with
soft labels (the output vector from the teacher network). For
example, while the hard label for an image of a hand-written
digit 7 will say it is classiﬁed as a seven, the soft labels might
say it has a 80% chance of being a seven and a 20% chance
of being a one. Then, we train the distilled model on the soft
labels from the teacher, rather than on the hard labels from
the training set. Distillation can potentially increase accuracy
on the test set as well as the rate at which the smaller model
learns to predict the hard labels [19], [30].
Defensive distillation uses distillation in order to increase
the robustness of a neural network, but with two signiﬁcant
changes. First, both the teacher model and the distilled model
are identical in size — defensive distillation does not result
in smaller models. Second, and more importantly, defensive
distillation uses a large distillation temperature (described
below) to force the distilled model to become more conﬁdent
in its predictions.
Recall that, the softmax function is the last layer of a neural
network. Defensive distillation modiﬁes the softmax function
to also include a temperature constant T:
softmax(x, T)i =
exi/T
P
j exj/T
It is easy to see that softmax(x, T) = softmax(x/T, 1). Intu-
itively, increasing the temperature causes a “softer” maximum,
and decreasing it causes a “harder” maximum. As the limit
of the temperature goes to 0, softmax approaches max; as
the limit goes to inﬁnity, softmax(x) approaches a uniform
distribution.
Defensive distillation proceeds in four steps:
1) Train a network, the teacher network, by setting the
temperature of the softmax to T during the training
phase.
2) Compute soft labels by apply the teacher network to
each instance in the training set, again evaluating the
softmax at temperature T.
3) Train the distilled network (a network with the same
shape as the teacher network) on the soft labels, using
softmax at temperature T.
12


**Table 16 from page 12**

| 0                       | 1     | 2         | 3     | 4           | 5     | 6            | 7     | 8    | 9     | 10         | 11    | 12   |
|:------------------------|:------|:----------|:------|:------------|:------|:-------------|:------|:-----|:------|:-----------|:------|:-----|
|                         |       | Best Case |       |             |       | Average Case |       |      |       | Worst Case |       |      |
|                         | MNIST |           | CIFAR |             | MNIST |              | CIFAR |      | MNIST |            | CIFAR |      |
|                         | mean  | prob      | mean  | prob        | mean  | prob         | mean  | prob | mean  | prob       | mean  | prob |
| Our L0                  | 8.5   | 100%      | 5.9   | 100%        | 16    | 100%         | 13    | 100% | 33    | 100%       | 24    | 100% |
| JSMA-Z                  | 20    | 100%      | 20    | 100%        | 56    | 100%         | 58    | 100% | 180   | 98%        | 150   | 100% |
| JSMA-F                  | 17    | 100%      | 25    | 100%        | 45    | 100%         | 110   | 100% | 100   | 100%       | 240   | 100% |
| Our L2                  | 1.36  | 100%      | 0.17  | 100%        | 1.76  | 100%         | 0.33  | 100% | 2.60  | 100%       | 0.51  | 100% |
| Deepfool                | 2.11  | 100%      | 0.85  | 100%        | −     | -            | −     | -    | −     | -          | −     | -    |
| Our L∞                  | 0.13  | 100%      |       | 0.0092 100% | 0.16  | 100%         | 0.013 | 100% | 0.23  | 100%       | 0.019 | 100% |
| Fast Gradient Sign      | 0.22  | 100%      | 0.015 | 99%         | 0.26  | 42%          | 0.029 | 51%  | −     | 0%         | 0.34  | 1%   |
| Iterative Gradient Sign | 0.14  | 100%      |       | 0.0078 100% | 0.19  | 100%         | 0.014 | 100% | 0.26  | 100%       | 0.023 | 100% |

**Table 17 from page 12**

| 0                                                                  | 1                                                                                                                         | 2                                                                | 3                                         | 4                        | 5                   | 6                              | 7                    |
|:-------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------|:------------------------------------------|:-------------------------|:--------------------|:-------------------------------|:---------------------|
|                                                                    | COMPARISON OF THE THREE VARIANTS OF TARGETED ATTACK TO PREVIOUS WORK FOR OUR MNIST AND CIFAR MODELS. WHEN SUCCESS RATE IS |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    | NOT 100%, THE MEAN IS ONLY OVER SUCCESSES.                                                                                |                                                                  |                                           |                          |                     |                                |                      |
| Untargeted                                                         |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| Average Case                                                       |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| Least Likely                                                       |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    | soft                                                                                                                      |                                                                  | (the output vector                        |                          |                     | from the teacher network). For |                      |
|                                                                    | labels                                                                                                                    |                                                                  |                                           |                          |                     |                                |                      |
| mean                                                               |                                                                                                                           | example, while the hard label                                    |                                           |                          |                     | for an image of a hand-written |                      |
| prob                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| mean                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| prob                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| mean                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| prob                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 100%                                                               |                                                                                                                           | digit 7 will say it                                              |                                           | is classiﬁed as a seven, |                     | the soft                       | labels might         |
| 100%                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 100%                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 48                                                                 |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 410                                                                |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 5200                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| Our L0                                                             |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 0%                                                                 |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 0%                                                                 |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 0%                                                                 |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| JSMA-Z                                                             |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| -                                                                  |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| -                                                                  |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| -                                                                  |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    |                                                                                                                           | say it has a 80% chance of being a seven and a 20% chance        |                                           |                          |                     |                                |                      |
| 0%                                                                 |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 0%                                                                 |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 0%                                                                 |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| JSMA-F                                                             |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| -                                                                  |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| -                                                                  |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| -                                                                  |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    |                                                                                                                           | of being a one. Then, we train the distilled model on the soft   |                                           |                          |                     |                                |                      |
| 100%                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 100%                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 100%                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 0.32                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 0.96                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 2.22                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| Our L2                                                             |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    | labels                                                                                                                    | from the teacher,                                                |                                           |                          | rather              | than on the hard labels        | from                 |
| 100%                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| Deepfool                                                           |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 0.91                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| -                                                                  |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| -                                                                  |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| -                                                                  |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| -                                                                  |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    |                                                                                                                           | the training set. Distillation can potentially increase accuracy |                                           |                          |                     |                                |                      |
| 100%                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 100%                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 100%                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 0.004                                                              |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 0.006                                                              |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 0.01                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| Our L∞                                                             |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    |                                                                                                                           | on the test set as well as the rate at which the smaller model   |                                           |                          |                     |                                |                      |
| 100%                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 2%                                                                 |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 0%                                                                 |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| FGS                                                                |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 0.004                                                              |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 0.064                                                              |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| -                                                                  |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 100%                                                               |                                                                                                                           | learns to predict                                                |                                           | the hard labels [19],    |                     | [30].                          |                      |
| 99%                                                                |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 98%                                                                |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| IGS                                                                |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 0.004                                                              |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 0.01                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 0.03                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    |                                                                                                                           | Defensive distillation uses distillation in order                |                                           |                          |                     |                                | to increase          |
| TABLE V                                                            |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    |                                                                                                                           | the robustness of a neural network, but with two signiﬁcant      |                                           |                          |                     |                                |                      |
| COMPARISON OF THE THREE VARIANTS OF TARGETED ATTACK TO             |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| PREVIOUS WORK FOR THE INCEPTION V3 MODEL ON IMAGENET. WHEN         |                                                                                                                           | changes. First, both the teacher model and the distilled model   |                                           |                          |                     |                                |                      |
| SUCCESS RATE IS NOT 100%, THE MEAN IS ONLY OVER SUCCESSES.         |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    | are identical                                                                                                             |                                                                  | in size — defensive distillation does not |                          |                     |                                | result               |
|                                                                    |                                                                                                                           | in smaller models. Second, and more importantly, defensive       |                                           |                          |                     |                                |                      |
|                                                                    | distillation                                                                                                              | uses                                                             | a                                         | large                    | distillation        | temperature                    | (described           |
|                                                                    | below)                                                                                                                    | to force the distilled model                                     |                                           |                          |                     | to become more conﬁdent        |                      |
| adversarial                                                        |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| re-training [11].                                                  |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    |                                                                                                                           | in its predictions.                                              |                                           |                          |                     |                                |                      |
| Comparing the exact                                                |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| runtime of attacks can be misleading.                              |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    | Recall                                                                                                                    | that,                                                            | the softmax function is the last          |                          |                     |                                | layer of a neural    |
| For                                                                |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| example, we                                                        |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| have                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| parallelized                                                       |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| the                                                                |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| implementation                                                     |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| of                                                                 |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    |                                                                                                                           | network. Defensive distillation modiﬁes the softmax function     |                                           |                          |                     |                                |                      |
| adversary                                                          |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| allowing                                                           |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| it                                                                 |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| to                                                                 |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| run                                                                |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| hundreds                                                           |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| of                                                                 |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| attacks                                                            |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| our L2                                                             |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    |                                                                                                                           | to also include a temperature constant T :                       |                                           |                          |                     |                                |                      |
| simultaneously on a GPU,                                           |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| increasing performance from 10×                                    |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| to                                                                 |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| 100×. However, we                                                  |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| did                                                                |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| not                                                                |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| parallelize                                                        |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| our L0                                                             |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| or L∞                                                              |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    |                                                                                                                           |                                                                  |                                           |                          |                     | exi/T                          |                      |
| attacks. Similarly, our                                            |                                                                                                                           |                                                                  | softmax(x, T )i =                         |                          |                     |                                |                      |
| implementation of                                                  |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| fast gradient                                                      |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| sign                                                               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    |                                                                                                                           |                                                                  |                                           |                          | (cid:80)            |                                |                      |
| is parallelized, but                                               |                                                                                                                           |                                                                  |                                           |                          |                     | j exj /T                       |                      |
| JSMA is not. We                                                    |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| therefore                                                          |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| refrain from                                                       |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| giving                                                             |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| exact                                                              |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| performance                                                        |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| numbers                                                            |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| because we                                                         |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| believe                                                            |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| an                                                                 |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    | It                                                                                                                        | is easy to see that softmax(x, T ) = softmax(x/T, 1).            |                                           |                          |                     |                                | Intu-                |
| unfair comparison is worse than no comparison.                     |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    |                                                                                                                           | itively, increasing the temperature causes a “softer” maximum,   |                                           |                          |                     |                                |                      |
| All                                                                |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| of                                                                 |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| our                                                                |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| attacks,                                                           |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| and                                                                |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| all                                                                |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| previous                                                           |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| attacks,                                                           |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| are                                                                |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| plenty                                                             |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    |                                                                                                                           | and decreasing it causes a “harder” maximum. As                  |                                           |                          |                     |                                | the limit            |
| efﬁcient                                                           |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| to be used by an adversary. No attack takes                        |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| longer                                                             |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    | of                                                                                                                        | temperature goes                                                 |                                           |                          | to 0,               | softmax approaches max;        | as                   |
|                                                                    | the                                                                                                                       |                                                                  |                                           |                          |                     |                                |                      |
| than a few minutes to run on any given instance.                   |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    | the                                                                                                                       | limit goes                                                       | to inﬁnity,                               |                          | softmax(x)          | approaches                     | a uniform            |
| When compared to L0, our attacks are 2 × −10× slower               |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    | distribution.                                                                                                             |                                                                  |                                           |                          |                     |                                |                      |
| than our optimized JSMA algorithm (and signiﬁcantly faster         |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    |                                                                                                                           | Defensive distillation proceeds in four steps:                   |                                           |                          |                     |                                |                      |
| than the un-optimized version). Our attacks are typically 10 ×     |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| −100× slower                                                       | 1) Train                                                                                                                  | a                                                                | network,                                  |                          | the                 | network,                       | setting              |
| than previous                                                      |                                                                                                                           |                                                                  |                                           |                          | teacher             | by                             | the                  |
| attacks                                                            |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| for L2                                                             |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| and L∞, with                                                       |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| exception of iterative gradient sign which we are 10× slower.      |                                                                                                                           | temperature                                                      |                                           | of                       | softmax             | to T                           | the                  |
|                                                                    |                                                                                                                           |                                                                  |                                           | the                      |                     | during                         | training             |
|                                                                    | phase.                                                                                                                    |                                                                  |                                           |                          |                     |                                |                      |
| VIII. EVALUATING DEFENSIVE DISTILLATION                            |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
|                                                                    | 2) Compute                                                                                                                |                                                                  | soft                                      |                          | labels by apply the |                                | teacher network to   |
| Distillation was initially proposed as an approach to reduce       |                                                                                                                           | each instance                                                    |                                           | in the                   | training set,       |                                | again evaluating the |
| a large model (the teacher) down to a smaller distilled model      |                                                                                                                           | softmax at                                                       |                                           | temperature T .          |                     |                                |                      |
| [19]. At a high level, distillation works by ﬁrst                  |                                                                                                                           | 3) Train the distilled network (a network with the               |                                           |                          |                     |                                | same                 |
| training the                                                       |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| teacher model on the training set                                  |                                                                                                                           | shape as the teacher network) on the soft                        |                                           |                          |                     |                                | labels, using        |
| in a standard manner. Then,                                        |                                                                                                                           |                                                                  |                                           |                          |                     |                                |                      |
| we use the teacher to label each instance in the training set with |                                                                                                                           | softmax at                                                       |                                           | temperature T .          |                     |                                |                      |



## Page 13

4) Finally, when running the distilled network at test time
(to classify new inputs), use temperature 1.
A. Fragility of existing attacks
We brieﬂy investigate the reason that existing attacks fail
on distilled networks, and ﬁnd that existing attacks are very
fragile and can easily fail to ﬁnd adversarial examples even
when they exist.
L-BFGS and Deepfool fail due to the fact that the gradient
of F(·) is zero almost always, which prohibits the use of the
standard objective function.
When we train a distilled network at temperature T and
then test it at temperature 1, we effectively cause the inputs to
the softmax to become larger by a factor of T. By minimizing
the cross entropy during training, the output of the softmax
is forced to be close to 1.0 for the correct class and 0.0 for
all others. Since Z(·) is divided by T, the distilled network
will learn to make the Z(·) values T times larger than they
otherwise would be. (Positive values are forced to become
about T times larger; negative values are multiplied by a
factor of about T and thus become even more negative.)
Experimentally, we veriﬁed this fact: the mean value of the
L1 norm of Z(·) (the logits) on the undistilled network is
5.8 with standard deviation 6.4; on the distilled network (with
T = 100), the mean is 482 with standard deviation 457.
Because the values of Z(·) are 100 times larger, when
we test at temperature 1, the output of F becomes ϵ in all
components except for the output class which has conﬁdence
1−9ϵ for some very small ϵ (for tasks with 10 classes). In fact,
in most cases, ϵ is so small that the 32-bit ﬂoating-point value
is rounded to 0. For similar reasons, the gradient is so small
that it becomes 0 when expressed as a 32-bit ﬂoating-point
value.
This causes the L-BFGS minimization procedure to fail to
make progress and terminate. If instead we run L-BFGS with
our stable objective function identiﬁed earlier, rather than the
objective function lossF,l(·) suggested by Szegedy et al. [46],
L-BFGS does not fail. An alternate approach to ﬁxing the
attack would be to set
F ′(x) = softmax(Z(x)/T)
where T is the distillation temperature chosen. Then mini-
mizing lossF ′,l(·) will not fail, as now the gradients do not
vanish due to ﬂoating-point arithmetic rounding. This clearly
demonstrates the fragility of using the loss function as the
objective to minimize.
JSMA-F (whereby we mean the attack uses the output of
the ﬁnal layer F(·)) fails for the same reason that L-BFGS
fails: the output of the Z(·) layer is very large and so softmax
becomes essentially a hard maximum. This is the version of the
attack that Papernot et al. use to attack defensive distillation
in their paper [39].
JSMA-Z (the attack that uses the logits) fails for a com-
pletely different reason. Recall that in the Z(·) version of
the attack, we use the input to the softmax for computing
the gradient instead of the ﬁnal output of the network. This
removes any potential issues with the gradient vanishing,
however this introduces new issues. This version of the attack
is introduced by Papernot et al. [38] but it is not used to attack
distillation; we provide here an analysis of why it fails.
Since this attack uses the Z values, it is important to realize
the differences in relative impact. If the smallest input to
the softmax layer is −100, then, after the softmax layer, the
corresponding output becomes practically zero. If this input
changes from −100 to −90, the output will still be practically
zero. However, if the largest input to the softmax layer is 10,
and it changes to 0, this will have a massive impact on the
softmax output.
Relating this to parameters used in their attack, α and β
represent the size of the change at the input to the softmax
layer. It is perhaps surprising that JSMA-Z works on un-
distilled networks, as it treats all changes as being of equal
importance, regardless of how much they change the softmax
output. If changing a single pixel would increase the target
class by 10, but also increase the least likely class by 15, the
attack will not increase that pixel.
Recall that distillation at temperature T causes the value of
the logits to be T times larger. In effect, this magniﬁes the sub-
optimality noted above as logits that are extremely unlikely but
have slight variation can cause the attack to refuse to make
any changes.
Fast Gradient Sign fails at ﬁrst for the same reason L-
BFGS fails: the gradients are almost always zero. However,
something interesting happens if we attempt the same division
trick and divide the logits by T before feeding them to the
softmax function: distillation still remains effective [36]. We
are unable to explain this phenomenon.
B. Applying Our Attacks
When we apply our attacks to defensively distilled net-
works, we ﬁnd distillation provides only marginal value. We
re-implement defensive distillation on MNIST and CIFAR-10
as described [39] using the same model we used for our eval-
uation above. We train our distilled model with temperature
T = 100, the value found to be most effective [39].
Table VI shows our attacks when applied to distillation. All
of the previous attacks fail to ﬁnd adversarial examples. In
contrast, our attack succeeds with 100% success probability
for each of the three distance metrics.
When compared to Table IV, distillation has added almost
no value: our L0 and L2 attacks perform slightly worse, and
our L∞attack performs approximately equally. All of our
attacks succeed with 100% success.
C. Effect of Temperature
In the original work, increasing the temperature was found
to consistently reduce attack success rate. On MNIST, this
goes from a 91% success rate at T = 1 to a 24% success rate
for T = 5 and ﬁnally 0.5% success at T = 100.
13


**Table 18 from page 13**

| 0                                                                              | 1                                                                  |
|:-------------------------------------------------------------------------------|:-------------------------------------------------------------------|
| 4)                                                                             | the                                                                |
| Finally, when running the distilled network at                                 | attack, we use                                                     |
| test                                                                           | the                                                                |
| time                                                                           | input                                                              |
|                                                                                | to the                                                             |
|                                                                                | softmax for                                                        |
|                                                                                | computing                                                          |
| (to classify new inputs), use temperature 1.                                   | the gradient                                                       |
|                                                                                | instead of                                                         |
|                                                                                | the ﬁnal output of                                                 |
|                                                                                | the network. This                                                  |
|                                                                                | removes                                                            |
|                                                                                | any                                                                |
|                                                                                | potential                                                          |
|                                                                                | issues with                                                        |
|                                                                                | the                                                                |
|                                                                                | gradient                                                           |
|                                                                                | vanishing,                                                         |
| A. Fragility of existing attacks                                               |                                                                    |
|                                                                                | however this introduces new issues. This version of the attack     |
| We brieﬂy investigate the reason that existing attacks                         | is introduced by Papernot et al. [38] but it is not used to attack |
| fail                                                                           |                                                                    |
| on distilled networks, and ﬁnd that existing attacks are very                  | distillation; we provide here an analysis of why it                |
|                                                                                | fails.                                                             |
| fragile and can easily fail                                                    | Since this attack uses the Z values,                               |
| to ﬁnd adversarial examples even                                               | it                                                                 |
|                                                                                | is important                                                       |
|                                                                                | to realize                                                         |
| when they exist.                                                               | the                                                                |
|                                                                                | differences                                                        |
|                                                                                | in                                                                 |
|                                                                                | relative                                                           |
|                                                                                | impact.                                                            |
|                                                                                | If                                                                 |
|                                                                                | the                                                                |
|                                                                                | smallest                                                           |
|                                                                                | input                                                              |
|                                                                                | to                                                                 |
|                                                                                | the softmax layer                                                  |
|                                                                                | is −100,                                                           |
|                                                                                | then, after                                                        |
|                                                                                | the softmax layer,                                                 |
|                                                                                | the                                                                |
| L-BFGS and Deepfool                                                            |                                                                    |
| fail due to the fact                                                           |                                                                    |
| that                                                                           |                                                                    |
| the gradient                                                                   |                                                                    |
|                                                                                | corresponding output becomes practically zero.                     |
|                                                                                | If                                                                 |
|                                                                                | this                                                               |
|                                                                                | input                                                              |
| of F (·) is zero almost always, which prohibits the use of                     |                                                                    |
| the                                                                            |                                                                    |
|                                                                                | changes from −100 to −90,                                          |
|                                                                                | the output will still be practically                               |
| standard objective function.                                                   |                                                                    |
|                                                                                | zero. However,                                                     |
|                                                                                | if                                                                 |
|                                                                                | the largest                                                        |
|                                                                                | input                                                              |
|                                                                                | to the softmax layer                                               |
|                                                                                | is 10,                                                             |
| When we                                                                        |                                                                    |
| train a distilled network at                                                   |                                                                    |
| temperature T                                                                  |                                                                    |
| and                                                                            |                                                                    |
|                                                                                | and it changes                                                     |
|                                                                                | to 0,                                                              |
|                                                                                | this will have a massive impact on the                             |
| then test                                                                      |                                                                    |
| it at                                                                          |                                                                    |
| temperature 1, we effectively cause the inputs to                              |                                                                    |
|                                                                                | softmax output.                                                    |
| the softmax to become larger by a factor of T . By minimizing                  |                                                                    |
|                                                                                | Relating this                                                      |
|                                                                                | to parameters used in their attack, α and β                        |
| the cross entropy during training,                                             |                                                                    |
| the output of                                                                  |                                                                    |
| the softmax                                                                    |                                                                    |
|                                                                                | represent                                                          |
|                                                                                | the size of                                                        |
|                                                                                | the change at                                                      |
|                                                                                | the input                                                          |
|                                                                                | to the softmax                                                     |
| is                                                                             |                                                                    |
| forced to be close to 1.0 for                                                  |                                                                    |
| the correct class and 0.0 for                                                  |                                                                    |
|                                                                                | layer.                                                             |
|                                                                                | It                                                                 |
|                                                                                | is                                                                 |
|                                                                                | perhaps                                                            |
|                                                                                | surprising                                                         |
|                                                                                | that                                                               |
|                                                                                | JSMA-Z works                                                       |
|                                                                                | on                                                                 |
|                                                                                | un-                                                                |
| all others. Since Z(·)                                                         |                                                                    |
| is divided by T ,                                                              |                                                                    |
| the distilled network                                                          |                                                                    |
|                                                                                | distilled networks, as                                             |
|                                                                                | it                                                                 |
|                                                                                | treats all changes as being of equal                               |
| will                                                                           |                                                                    |
| learn to make the Z(·) values T times                                          |                                                                    |
| larger                                                                         |                                                                    |
| than they                                                                      |                                                                    |
|                                                                                | importance,                                                        |
|                                                                                | regardless of how much they change the softmax                     |
| otherwise would be.                                                            |                                                                    |
| (Positive values                                                               |                                                                    |
| are                                                                            |                                                                    |
| forced to become                                                               |                                                                    |
|                                                                                | output.                                                            |
|                                                                                | If                                                                 |
|                                                                                | changing a                                                         |
|                                                                                | single pixel would increase                                        |
|                                                                                | the                                                                |
|                                                                                | target                                                             |
| about T                                                                        |                                                                    |
| times                                                                          |                                                                    |
| larger;                                                                        |                                                                    |
| negative                                                                       |                                                                    |
| values                                                                         |                                                                    |
| are multiplied                                                                 |                                                                    |
| by                                                                             |                                                                    |
| a                                                                              |                                                                    |
|                                                                                | class by 10, but also increase the least                           |
|                                                                                | likely class by 15,                                                |
|                                                                                | the                                                                |
| factor                                                                         |                                                                    |
| of                                                                             |                                                                    |
| about T                                                                        |                                                                    |
| and                                                                            |                                                                    |
| thus                                                                           |                                                                    |
| become                                                                         |                                                                    |
| even more                                                                      |                                                                    |
| negative.)                                                                     |                                                                    |
|                                                                                | attack will not                                                    |
|                                                                                | increase that pixel.                                               |
| Experimentally, we veriﬁed this                                                |                                                                    |
| fact:                                                                          |                                                                    |
| the mean value of                                                              |                                                                    |
| the                                                                            |                                                                    |
|                                                                                | Recall                                                             |
|                                                                                | that distillation at                                               |
|                                                                                | temperature T causes the value of                                  |
| norm of Z(·)                                                                   |                                                                    |
| (the                                                                           |                                                                    |
| logits) on the undistilled network is                                          |                                                                    |
| L1                                                                             |                                                                    |
|                                                                                | the logits to be T times larger. In effect, this magniﬁes the sub- |
| 5.8 with standard deviation 6.4; on the distilled network (with                |                                                                    |
|                                                                                | optimality noted above as logits that are extremely unlikely but   |
| T = 100),                                                                      |                                                                    |
| the mean is 482 with standard deviation 457.                                   |                                                                    |
|                                                                                | have slight variation can cause the attack to refuse to make       |
| Because                                                                        |                                                                    |
| the                                                                            |                                                                    |
| values                                                                         |                                                                    |
| of Z(·)                                                                        |                                                                    |
| are                                                                            |                                                                    |
| 100                                                                            |                                                                    |
| times                                                                          |                                                                    |
| larger, when                                                                   |                                                                    |
|                                                                                | any changes.                                                       |
| (cid:15)                                                                       |                                                                    |
| we test                                                                        |                                                                    |
| at                                                                             |                                                                    |
| temperature 1,                                                                 |                                                                    |
| the output of F becomes                                                        |                                                                    |
| in all                                                                         |                                                                    |
| components except                                                              |                                                                    |
| for                                                                            |                                                                    |
| the output class which has conﬁdence                                           |                                                                    |
|                                                                                | Fast Gradient                                                      |
|                                                                                | Sign                                                               |
|                                                                                | fails                                                              |
|                                                                                | at ﬁrst                                                            |
|                                                                                | for                                                                |
|                                                                                | the                                                                |
|                                                                                | same                                                               |
|                                                                                | reason L-                                                          |
| 1−9(cid:15) for some very small (cid:15) (for tasks with 10 classes). In fact, |                                                                    |
|                                                                                | BFGS fails:                                                        |
|                                                                                | the gradients are almost always zero. However,                     |
| in most cases, (cid:15) is so small                                            |                                                                    |
| that                                                                           |                                                                    |
| the 32-bit ﬂoating-point value                                                 |                                                                    |
|                                                                                | something interesting happens if we attempt                        |
|                                                                                | the same division                                                  |
| is rounded to 0. For similar                                                   |                                                                    |
| reasons,                                                                       |                                                                    |
| the gradient                                                                   |                                                                    |
| is so small                                                                    |                                                                    |
|                                                                                | trick and divide the logits by T before feeding them to the        |
| that                                                                           |                                                                    |
| it becomes 0 when expressed as a 32-bit ﬂoating-point                          |                                                                    |
|                                                                                | softmax function: distillation still                               |
|                                                                                | remains effective [36]. We                                         |
| value.                                                                         |                                                                    |
|                                                                                | are unable to explain this phenomenon.                             |
| This causes the L-BFGS minimization procedure to fail                          |                                                                    |
| to                                                                             |                                                                    |
| make progress and terminate. If instead we run L-BFGS with                     | B. Applying Our Attacks                                            |
| our stable objective function identiﬁed earlier,                               |                                                                    |
| rather                                                                         |                                                                    |
| than the                                                                       |                                                                    |
|                                                                                | When we                                                            |
|                                                                                | apply                                                              |
|                                                                                | our                                                                |
|                                                                                | attacks                                                            |
|                                                                                | to                                                                 |
|                                                                                | defensively                                                        |
|                                                                                | distilled                                                          |
|                                                                                | net-                                                               |
| objective function lossF,l(·) suggested by Szegedy et al. [46],                |                                                                    |
|                                                                                | works, we ﬁnd distillation provides only marginal value. We        |
| L-BFGS does not                                                                |                                                                    |
| fail. An alternate                                                             |                                                                    |
| approach to ﬁxing the                                                          |                                                                    |
|                                                                                | re-implement defensive distillation on MNIST and CIFAR-10          |
| attack would be to set                                                         |                                                                    |
|                                                                                | as described [39] using the same model we used for our eval-       |
|                                                                                | uation above. We train our distilled model with temperature        |
| F (cid:48)(x) = softmax(Z(x)/T )                                               |                                                                    |
|                                                                                | T = 100,                                                           |
|                                                                                | the value found to be most effective [39].                         |
| where T                                                                        |                                                                    |
| is                                                                             |                                                                    |
| the distillation temperature                                                   |                                                                    |
| chosen. Then mini-                                                             |                                                                    |
|                                                                                | Table VI shows our attacks when applied to distillation. All       |
| fail, as now the gradients do not                                              |                                                                    |
| mizing lossF (cid:48),l(·) will not                                            |                                                                    |
|                                                                                | of                                                                 |
|                                                                                | the previous                                                       |
|                                                                                | attacks                                                            |
|                                                                                | fail                                                               |
|                                                                                | to ﬁnd adversarial                                                 |
|                                                                                | examples.                                                          |
|                                                                                | In                                                                 |
| vanish due to ﬂoating-point arithmetic rounding. This clearly                  |                                                                    |
|                                                                                | contrast, our attack succeeds with 100% success probability        |
| demonstrates                                                                   |                                                                    |
| the                                                                            |                                                                    |
| fragility of using the                                                         |                                                                    |
| loss                                                                           |                                                                    |
| function as                                                                    |                                                                    |
| the                                                                            |                                                                    |
|                                                                                | for each of                                                        |
|                                                                                | the three distance metrics.                                        |
| objective to minimize.                                                         |                                                                    |
|                                                                                | When compared to Table IV, distillation has added almost           |
|                                                                                | no value: our L0 and L2 attacks perform slightly worse, and        |
| JSMA-F (whereby we mean the attack uses                                        |                                                                    |
| the output of                                                                  |                                                                    |
|                                                                                | approximately equally. All of our                                  |
|                                                                                | our L∞ attack performs                                             |
| the ﬁnal                                                                       |                                                                    |
| layer F (·))                                                                   |                                                                    |
| fails                                                                          |                                                                    |
| for                                                                            |                                                                    |
| the same reason that L-BFGS                                                    |                                                                    |
|                                                                                | attacks succeed with 100% success.                                 |
| fails:                                                                         |                                                                    |
| the output of the Z(·) layer is very large and so softmax                      |                                                                    |
| becomes essentially a hard maximum. This is the version of the                 |                                                                    |
|                                                                                | C. Effect of Temperature                                           |
| attack that Papernot et al. use to attack defensive distillation               |                                                                    |
|                                                                                | In the original work,                                              |
|                                                                                | increasing the temperature was found                               |
| in their paper                                                                 |                                                                    |
| [39].                                                                          |                                                                    |
|                                                                                | to consistently reduce                                             |
|                                                                                | attack success                                                     |
|                                                                                | rate. On MNIST,                                                    |
|                                                                                | this                                                               |
| JSMA-Z (the                                                                    | goes from a 91% success rate at T = 1 to a 24% success rate        |
| attack that uses                                                               |                                                                    |
| the                                                                            |                                                                    |
| logits)                                                                        |                                                                    |
| fails                                                                          |                                                                    |
| for                                                                            |                                                                    |
| a                                                                              |                                                                    |
| com-                                                                           |                                                                    |
| pletely                                                                        | for T = 5 and ﬁnally 0.5% success at T = 100.                      |
| different                                                                      |                                                                    |
| reason. Recall                                                                 |                                                                    |
| that                                                                           |                                                                    |
| in                                                                             |                                                                    |
| the Z(·)                                                                       |                                                                    |
| version                                                                        |                                                                    |
| of                                                                             |                                                                    |



## Page 14

Best Case
Average Case
Worst Case
MNIST
CIFAR
MNIST
CIFAR
MNIST
CIFAR
mean
prob
mean
prob
mean
prob
mean
prob
mean
prob
mean
prob
Our L0
10
100%
7.4
100%
19
100%
15
100%
36
100%
29
100%
Our L2
1.7
100%
0.36
100%
2.2
100%
0.60
100%
2.9
100%
0.92
100%
Our L∞
0.14
100%
0.002
100%
0.18
100%
0.023
100%
0.25
100%
0.038
100%
TABLE VI
COMPARISON OF OUR ATTACKS WHEN APPLIED TO DEFENSIVELY DISTILLED NETWORKS. COMPARE TO TABLE IV FOR UNDISTILLED NETWORKS.
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0
20
40
60
80
100
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Distillation Temperature
Mean Adversarial Distance
Fig. 8. Mean distance to targeted (with random target) adversarial examples
for different distillation temperatures on MNIST. Temperature is uncorrelated
with mean adversarial example distance.
We re-implement this experiment with our improved attacks
to understand how the choice of temperature impacts robust-
ness. We train models with the temperature varied from t = 1
to t = 100.
When we re-run our implementation of JSMA, we observe
the same effect: attack success rapidly decreases. However,
with our improved L2 attack, we see no effect of temperature
on the mean distance to adversarial examples: the correlation
coefﬁcient is ρ = −0.05. This clearly demonstrates the fact
that increasing the distillation temperature does not increase
the robustness of the neural network, it only causes existing
attacks to fail more often.
D. Transferability
Recent work has shown that an adversarial example for one
model will often transfer to be an adversarial on a different
model, even if they are trained on different sets of training data
[46], [11], and even if they use entirely different algorithms
(i.e., adversarial examples on neural networks transfer to
random forests [37]).
0
10
20
30
40
0.0
0.2
0.4
0.6
0.8
1.0
Value of k
Probability Adversarial Example Transfers, Baseline
Untargetted
Targetted
Fig. 9.
Probability that adversarial examples transfer from one model to
another, for both targeted (the adversarial class remains the same) and
untargeted (the image is not the correct class).
Therefore, any defense that is able to provide robust-
ness against adversarial examples must somehow break this
transferability property; otherwise, we could run our attack
algorithm on an easy-to-attack model, and then transfer those
adversarial examples to the hard-to-attack model.
Even though defensive distillation is not robust to our
stronger attacks, we demonstrate a second break of distillation
by transferring attacks from a standard model to a defensively
distilled model.
We accomplish this by ﬁnding high-conﬁdence adversar-
ial examples, which we deﬁne as adversarial examples that
are strongly misclassiﬁed by the original model. Instead of
looking for an adversarial example that just barely changes
the classiﬁcation from the source to the target, we want one
where the target is much more likely than any other label.
Recall the loss function deﬁned earlier for L2 attacks:
f(x′) = max(max{Z(x′)i : i ̸= t} −Z(x′)t, −κ).
The purpose of the parameter κ is to control the strength of
adversarial examples: the larger κ, the stronger the classiﬁ-
14


**Table 19 from page 14**

| 0                                                                             | 1                                                               |
|:------------------------------------------------------------------------------|:----------------------------------------------------------------|
| Distillation Temperature                                                      | Value of k                                                      |
| Fig. 8. Mean distance to targeted (with random target) adversarial examples   | Fig. 9.                                                         |
|                                                                               | Probability that                                                |
|                                                                               | adversarial                                                     |
|                                                                               | examples                                                        |
|                                                                               | transfer                                                        |
|                                                                               | from one model                                                  |
|                                                                               | to                                                              |
| for different distillation temperatures on MNIST. Temperature is uncorrelated | another,                                                        |
|                                                                               | for                                                             |
|                                                                               | both                                                            |
|                                                                               | targeted                                                        |
|                                                                               | (the                                                            |
|                                                                               | adversarial                                                     |
|                                                                               | class                                                           |
|                                                                               | remains                                                         |
|                                                                               | the                                                             |
|                                                                               | same)                                                           |
|                                                                               | and                                                             |
| with mean adversarial example distance.                                       | untargeted (the image is not                                    |
|                                                                               | the correct class).                                             |
| We re-implement this experiment with our improved attacks                     | Therefore,                                                      |
|                                                                               | any                                                             |
|                                                                               | defense                                                         |
|                                                                               | that                                                            |
|                                                                               | is                                                              |
|                                                                               | able                                                            |
|                                                                               | to                                                              |
|                                                                               | provide                                                         |
|                                                                               | robust-                                                         |
| to understand how the choice of                                               | ness                                                            |
| temperature impacts robust-                                                   | against                                                         |
|                                                                               | adversarial                                                     |
|                                                                               | examples must                                                   |
|                                                                               | somehow break this                                              |
| ness. We train models with the temperature varied from t = 1                  | transferability property; otherwise, we                         |
|                                                                               | could run our                                                   |
|                                                                               | attack                                                          |
| to t = 100.                                                                   | algorithm on an easy-to-attack model, and then transfer         |
|                                                                               | those                                                           |
| When we re-run our                                                            | adversarial examples to the hard-to-attack model.               |
| implementation of JSMA, we observe                                            |                                                                 |
| the                                                                           | Even                                                            |
| same                                                                          | though                                                          |
| effect:                                                                       | defensive                                                       |
| attack success                                                                | distillation                                                    |
| rapidly decreases. However,                                                   | is                                                              |
|                                                                               | not                                                             |
|                                                                               | robust                                                          |
|                                                                               | to                                                              |
|                                                                               | our                                                             |
| with our improved L2 attack, we see no effect of temperature                  | stronger attacks, we demonstrate a second break of distillation |
| on the mean distance to adversarial examples:                                 | by transferring attacks from a standard model                   |
| the correlation                                                               | to a defensively                                                |
| coefﬁcient                                                                    | distilled model.                                                |
| is ρ = −0.05. This clearly demonstrates                                       |                                                                 |
| the fact                                                                      |                                                                 |
| that                                                                          | We                                                              |
| increasing the distillation temperature does not                              | accomplish this by ﬁnding high-conﬁdence adversar-              |
| increase                                                                      |                                                                 |
| the robustness of                                                             | ial                                                             |
| the neural network,                                                           | examples, which we deﬁne                                        |
| it only causes existing                                                       | as                                                              |
|                                                                               | adversarial                                                     |
|                                                                               | examples                                                        |
|                                                                               | that                                                            |
| attacks to fail more often.                                                   | are                                                             |
|                                                                               | strongly misclassiﬁed by the original model.                    |
|                                                                               | Instead of                                                      |
|                                                                               | looking for                                                     |
|                                                                               | an adversarial                                                  |
|                                                                               | example                                                         |
|                                                                               | that                                                            |
|                                                                               | just barely changes                                             |
| D. Transferability                                                            |                                                                 |
|                                                                               | the classiﬁcation from the source to the target, we want one    |
| Recent work has shown that an adversarial example for one                     | where the target                                                |
|                                                                               | is much more likely than any other                              |
|                                                                               | label.                                                          |
| model will often transfer                                                     | Recall                                                          |
| to be an adversarial on a different                                           | the loss function deﬁned earlier                                |
|                                                                               | for L2 attacks:                                                 |
| model, even if they are trained on different sets of training data            |                                                                 |
|                                                                               | f (x(cid:48)) = max(max{Z(x(cid:48))i                           |
|                                                                               | : i (cid:54)= t} − Z(x(cid:48))t, −κ).                          |
| [46],                                                                         |                                                                 |
| [11], and even if                                                             |                                                                 |
| they use entirely different algorithms                                        |                                                                 |
| (i.e.,                                                                        | The purpose of                                                  |
| adversarial                                                                   | the parameter κ is                                              |
| examples                                                                      | to control                                                      |
| on                                                                            | the strength of                                                 |
| neural                                                                        |                                                                 |
| networks                                                                      |                                                                 |
| transfer                                                                      |                                                                 |
| to                                                                            |                                                                 |
| random forests [37]).                                                         | adversarial                                                     |
|                                                                               | examples:                                                       |
|                                                                               | the                                                             |
|                                                                               | larger κ,                                                       |
|                                                                               | the                                                             |
|                                                                               | stronger                                                        |
|                                                                               | the                                                             |
|                                                                               | classiﬁ-                                                        |



## Page 15

0
10
20
30
40
0.0
0.2
0.4
0.6
0.8
Value of k
Probability Adversarial Example Transfers, Distilled
Untargetted
Targetted
Fig. 10. Probability that adversarial examples transfer from the baseline model
to a model trained with defensive distillation at temperature 100.
cation of the adversarial example. This allows us to generate
high-conﬁdence adversarial examples by increasing κ.
We ﬁrst investigate if our hypothesis is true that the stronger
the classiﬁcation on the ﬁrst model, the more likely it will
transfer. We do this by varying κ from 0 to 40.
Our baseline experiment uses two models trained on MNIST
as described in Section IV, with each model trained on half of
the training data. We ﬁnd that the transferability success rate
increases linearly from κ = 0 to κ = 20 and then plateaus
at near-100% success for κ ≈20, so clearly increasing κ
increases the probability of a successful transferable attack.
We then run this same experiment only instead we train
the second model with defensive distillation, and ﬁnd that
adversarial examples do transfer. This gives us another at-
tack technique for ﬁnding adversarial examples on distilled
networks.
However, interestingly, the transferability success rate be-
tween the unsecured model and the distilled model only
reaches 100% success at κ = 40, in comparison to the previous
approach that only required κ = 20.
We believe that this approach can be used in general to
evaluate the robustness of defenses, even if the defense is able
to completely block ﬂow of gradients to cause our gradient-
descent based approaches from succeeding.
IX. CONCLUSION
The existence of adversarial examples limits the areas in
which deep learning can be applied. It is an open problem
to construct defenses that are robust to adversarial examples.
In an attempt to solve this problem, defensive distillation
was proposed as a general-purpose procedure to increase the
robustness of an arbitrary neural network.
In this paper, we propose powerful attacks that defeat
defensive distillation, demonstrating that our attacks more
generally can be used to evaluate the efﬁcacy of potential
defenses. By systematically evaluating many possible attack
approaches, we settle on one that can consistently ﬁnd better
adversarial examples than all existing approaches. We use this
evaluation as the basis of our three L0, L2, and L∞attacks.
We encourage those who create defenses to perform the two
evaluation approaches we use in this paper:
• Use a powerful attack (such as the ones proposed in this
paper) to evaluate the robustness of the secured model
directly. Since a defense that prevents our L2 attack will
prevent our other attacks, defenders should make sure to
establish robustness against the L2 distance metric.
• Demonstrate that transferability fails by constructing
high-conﬁdence adversarial examples on a unsecured
model and showing they fail to transfer to the secured
model.
ACKNOWLEDGEMENTS
We would like to thank Nicolas Papernot discussing our
defensive distillation implementation, and the anonymous re-
viewers for their helpful feedback. This work was supported
by Intel through the ISTC for Secure Computing, Qualcomm,
Cisco, the AFOSR under MURI award FA9550-12-1-0040,
and the Hewlett Foundation through the Center for Long-Term
Cybersecurity.
REFERENCES
[1] ANDOR, D., ALBERTI, C., WEISS, D., SEVERYN, A., PRESTA, A.,
GANCHEV, K., PETROV, S., AND COLLINS, M. Globally normalized
transition-based neural networks.
arXiv preprint arXiv:1603.06042
(2016).
[2] BASTANI, O., IOANNOU, Y., LAMPROPOULOS, L., VYTINIOTIS, D.,
NORI, A., AND CRIMINISI, A. Measuring neural net robustness with
constraints. arXiv preprint arXiv:1605.07262 (2016).
[3] BOJARSKI, M., DEL TESTA, D., DWORAKOWSKI, D., FIRNER, B.,
FLEPP, B., GOYAL, P., JACKEL, L. D., MONFORT, M., MULLER, U.,
ZHANG, J., ET AL. End to end learning for self-driving cars. arXiv
preprint arXiv:1604.07316 (2016).
[4] BOURZAC,
K.
Bringing
big
neural
networks
to
self-driving
cars,
smartphones,
and
drones.
http:
//spectrum.ieee.org/computing/embedded-systems/
bringing-big-neural-networks-to-selfdriving-cars-smartphones-and-drones,
2016.
[5] CARLINI, N., MISHRA, P., VAIDYA, T., ZHANG, Y., SHERR, M.,
SHIELDS, C., WAGNER, D., AND ZHOU, W. Hidden voice commands.
In 25th USENIX Security Symposium (USENIX Security 16), Austin, TX
(2016).
[6] CHANDOLA, V., BANERJEE, A., AND KUMAR, V. Anomaly detection:
A survey. ACM computing surveys (CSUR) 41, 3 (2009), 15.
[7] CLEVERT, D.-A., UNTERTHINER, T., AND HOCHREITER, S. Fast and
accurate deep network learning by exponential linear units (ELUs).
arXiv preprint arXiv:1511.07289 (2015).
[8] DAHL, G. E., STOKES, J. W., DENG, L., AND YU, D.
Large-scale
malware classiﬁcation using random projections and neural networks. In
2013 IEEE International Conference on Acoustics, Speech and Signal
Processing (2013), IEEE, pp. 3422–3426.
[9] DENG, J., DONG, W., SOCHER, R., LI, L.-J., LI, K., AND FEI-FEI,
L. Imagenet: A large-scale hierarchical image database. In Computer
Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference
on (2009), IEEE, pp. 248–255.
15


**Table 20 from page 15**

| 0           | 1   | 2                                                              | 3             | 4             | 5                             | 6       | 7            | 8         |
|:------------|:----|:---------------------------------------------------------------|:--------------|:--------------|:------------------------------|:--------|:-------------|:----------|
|             | In  | this                                                           | paper, we     | propose       | powerful                      | attacks | that         | defeat    |
|             |     | defensive                                                      | distillation, | demonstrating | that                          | our     | attacks more |           |
|             |     | generally                                                      | can           | to            | the                           | efﬁcacy | of           | potential |
|             |     |                                                                | be            | evaluate      |                               |         |              |           |
|             |     |                                                                | used          |               |                               |         |              |           |
|             |     | defenses. By systematically evaluating many possible attack    |               |               |                               |         |              |           |
|             |     | approaches, we settle on one that can consistently ﬁnd better  |               |               |                               |         |              |           |
| Untargetted |     |                                                                |               |               |                               |         |              |           |
|             |     | adversarial examples than all existing approaches. We use this |               |               |                               |         |              |           |
| Targetted   |     |                                                                |               |               |                               |         |              |           |
|             |     | evaluation as the basis of our                                 |               |               | three L0, L2, and L∞ attacks. |         |              |           |
|             |     | We encourage those who create defenses to perform the two      |               |               |                               |         |              |           |
|             |     | evaluation approaches we use in this paper:                    |               |               |                               |         |              |           |
|             |     | • Use a powerful attack (such as the ones proposed in this     |               |               |                               |         |              |           |

**Table 21 from page 15**

| 0   | 1   | 2          | 3   | 4   | 5                                                          |
|:----|:----|:-----------|:----|:----|:-----------------------------------------------------------|
|     |     |            |     |     | evaluation as the basis of our                             |
|     |     |            |     |     | three L0, L2, and L∞ attacks.                              |
|     |     |            |     |     | We encourage those who create defenses to perform the two  |
|     |     |            |     |     | evaluation approaches we use in this paper:                |
|     |     |            |     |     | • Use a powerful attack (such as the ones proposed in this |
|     |     |            |     |     | paper)                                                     |
|     |     |            |     |     | to evaluate the robustness of                              |
|     |     |            |     |     | the secured model                                          |
|     |     |            |     |     | directly. Since a defense that prevents our L2 attack will |
|     |     |            |     |     | prevent our other attacks, defenders should make sure to   |
|     |     |            |     |     | establish robustness against                               |
|     |     |            |     |     | the L2 distance metric.                                    |
|     |     |            |     |     | • Demonstrate that                                         |
|     |     |            |     |     | transferability fails by constructing                      |
|     |     |            |     |     | high-conﬁdence                                             |
|     |     |            |     |     | adversarial                                                |
|     |     |            |     |     | examples                                                   |
|     |     |            |     |     | on                                                         |
|     |     |            |     |     | a                                                          |
|     |     |            |     |     | unsecured                                                  |
|     |     |            |     |     | model and showing they fail                                |
|     |     |            |     |     | to transfer                                                |
|     |     |            |     |     | to the secured                                             |
| 0   | 10  | 20         | 30  | 40  |                                                            |
|     |     |            |     |     | model.                                                     |
|     |     | Value of k |     |     |                                                            |

**Table 22 from page 15**

| 0                                                                               | 1                                                                        |
|:--------------------------------------------------------------------------------|:-------------------------------------------------------------------------|
| 0                                                                               |                                                                          |
| 10                                                                              |                                                                          |
| 20                                                                              |                                                                          |
| 30                                                                              |                                                                          |
| 40                                                                              |                                                                          |
|                                                                                 | model.                                                                   |
| Value of k                                                                      |                                                                          |
|                                                                                 | ACKNOWLEDGEMENTS                                                         |
| Fig. 10. Probability that adversarial examples transfer from the baseline model |                                                                          |
|                                                                                 | We would like                                                            |
|                                                                                 | to thank Nicolas Papernot discussing our                                 |
| to a model                                                                      |                                                                          |
| trained with defensive distillation at                                          |                                                                          |
| temperature 100.                                                                |                                                                          |
|                                                                                 | defensive distillation implementation, and the anonymous re-             |
|                                                                                 | viewers                                                                  |
|                                                                                 | for                                                                      |
|                                                                                 | their helpful                                                            |
|                                                                                 | feedback. This work was                                                  |
|                                                                                 | supported                                                                |
| cation of                                                                       | by Intel                                                                 |
| the adversarial example. This allows us to generate                             | through the ISTC for Secure Computing, Qualcomm,                         |
| high-conﬁdence adversarial examples by increasing κ.                            | Cisco,                                                                   |
|                                                                                 | the AFOSR under MURI                                                     |
|                                                                                 | award FA9550-12-1-0040,                                                  |
| We ﬁrst investigate if our hypothesis is true that the stronger                 | and the Hewlett Foundation through the Center for Long-Term              |
| the                                                                             | Cybersecurity.                                                           |
| classiﬁcation on the ﬁrst model,                                                |                                                                          |
| the more                                                                        |                                                                          |
| likely it will                                                                  |                                                                          |
| transfer. We do this by varying κ from 0 to 40.                                 |                                                                          |
| Our baseline experiment uses two models trained on MNIST                        | REFERENCES                                                               |
| as described in Section IV, with each model                                     |                                                                          |
| trained on half of                                                              |                                                                          |
|                                                                                 | [1] ANDOR, D., ALBERTI, C., WEISS, D., SEVERYN, A., PRESTA, A.,          |
| the training data. We ﬁnd that                                                  | GANCHEV, K., PETROV, S., AND COLLINS, M.                                 |
| the transferability success rate                                                | Globally normalized                                                      |
| increases                                                                       | arXiv                                                                    |
| linearly from κ = 0 to κ = 20 and then plateaus                                 | preprint                                                                 |
|                                                                                 | arXiv:1603.06042                                                         |
|                                                                                 | transition-based                                                         |
|                                                                                 | neural                                                                   |
|                                                                                 | networks.                                                                |
|                                                                                 | (2016).                                                                  |
| at near-100% success                                                            |                                                                          |
| for κ ≈ 20,                                                                     |                                                                          |
| so clearly increasing κ                                                         |                                                                          |
|                                                                                 | [2] BASTANI, O.,                                                         |
|                                                                                 | IOANNOU, Y., LAMPROPOULOS, L., VYTINIOTIS, D.,                           |
| increases the probability of a successful                                       |                                                                          |
| transferable attack.                                                            |                                                                          |
|                                                                                 | NORI, A., AND CRIMINISI, A. Measuring neural net                         |
|                                                                                 | robustness with                                                          |
| We                                                                              | constraints. arXiv preprint arXiv:1605.07262 (2016).                     |
| then run this                                                                   |                                                                          |
| same                                                                            |                                                                          |
| experiment only instead we                                                      |                                                                          |
| train                                                                           |                                                                          |
|                                                                                 | [3] BOJARSKI, M., DEL TESTA, D., DWORAKOWSKI, D., FIRNER, B.,            |
| the                                                                             |                                                                          |
| second model with                                                               |                                                                          |
| defensive                                                                       |                                                                          |
| distillation,                                                                   |                                                                          |
| and ﬁnd                                                                         |                                                                          |
| that                                                                            |                                                                          |
|                                                                                 | FLEPP, B., GOYAL, P., JACKEL, L. D., MONFORT, M., MULLER, U.,            |
| do                                                                              |                                                                          |
| adversarial                                                                     |                                                                          |
| examples                                                                        |                                                                          |
| transfer. This                                                                  |                                                                          |
| gives                                                                           |                                                                          |
| us                                                                              |                                                                          |
| another                                                                         |                                                                          |
| at-                                                                             |                                                                          |
|                                                                                 | arXiv                                                                    |
|                                                                                 | ZHANG,                                                                   |
|                                                                                 | J., ET AL.                                                               |
|                                                                                 | End to end learning for                                                  |
|                                                                                 | self-driving cars.                                                       |
| tack technique                                                                  | preprint arXiv:1604.07316 (2016).                                        |
| for ﬁnding adversarial                                                          |                                                                          |
| examples on distilled                                                           |                                                                          |
|                                                                                 | [4] BOURZAC,                                                             |
|                                                                                 | K.                                                                       |
|                                                                                 | Bringing                                                                 |
|                                                                                 | big                                                                      |
|                                                                                 | neural                                                                   |
|                                                                                 | networks                                                                 |
|                                                                                 | to                                                                       |
| networks.                                                                       |                                                                          |
|                                                                                 | self-driving                                                             |
|                                                                                 | cars,                                                                    |
|                                                                                 | smartphones,                                                             |
|                                                                                 | and                                                                      |
|                                                                                 | drones.                                                                  |
|                                                                                 | http:                                                                    |
| However,                                                                        |                                                                          |
| interestingly,                                                                  |                                                                          |
| the transferability success                                                     |                                                                          |
| rate be-                                                                        |                                                                          |
|                                                                                 | //spectrum.ieee.org/computing/embedded-systems/                          |
| tween                                                                           | bringing-big-neural-networks-to-selfdriving-cars-smartphones-and-drones, |
| the                                                                             |                                                                          |
| unsecured model                                                                 |                                                                          |
| and                                                                             |                                                                          |
| the                                                                             |                                                                          |
| distilled model                                                                 |                                                                          |
| only                                                                            |                                                                          |
|                                                                                 | 2016.                                                                    |
| reaches 100% success at κ = 40, in comparison to the previous                   |                                                                          |
|                                                                                 | [5] CARLINI, N., MISHRA, P., VAIDYA, T., ZHANG, Y., SHERR, M.,           |
| approach that only required κ = 20.                                             |                                                                          |
|                                                                                 | SHIELDS, C., WAGNER, D., AND ZHOU, W. Hidden voice commands.             |
| We believe                                                                      | In 25th USENIX Security Symposium (USENIX Security 16), Austin, TX       |
| that                                                                            |                                                                          |
| this                                                                            |                                                                          |
| approach can be used in general                                                 |                                                                          |
| to                                                                              |                                                                          |
| evaluate the robustness of defenses, even if the defense is able                | (2016).                                                                  |
|                                                                                 | [6] CHANDOLA, V., BANERJEE, A., AND KUMAR, V. Anomaly detection:         |
| to completely block ﬂow of gradients                                            |                                                                          |
| to cause our gradient-                                                          |                                                                          |
|                                                                                 | A survey. ACM computing surveys (CSUR) 41, 3 (2009), 15.                 |
| descent based approaches from succeeding.                                       | [7] CLEVERT, D.-A., UNTERTHINER, T., AND HOCHREITER, S. Fast and         |
|                                                                                 | accurate                                                                 |
|                                                                                 | deep                                                                     |
|                                                                                 | network                                                                  |
|                                                                                 | learning                                                                 |
|                                                                                 | by                                                                       |
|                                                                                 | exponential                                                              |
|                                                                                 | linear                                                                   |
|                                                                                 | units                                                                    |
|                                                                                 | (ELUs).                                                                  |
| IX. CONCLUSION                                                                  | arXiv preprint arXiv:1511.07289 (2015).                                  |
|                                                                                 | [8] DAHL, G. E., STOKES,                                                 |
|                                                                                 | J. W., DENG, L., AND YU, D.                                              |
|                                                                                 | Large-scale                                                              |
| The                                                                             |                                                                          |
| existence of                                                                    |                                                                          |
| adversarial                                                                     |                                                                          |
| examples                                                                        |                                                                          |
| limits                                                                          |                                                                          |
| the                                                                             |                                                                          |
| areas                                                                           |                                                                          |
| in                                                                              |                                                                          |
|                                                                                 | malware classiﬁcation using random projections and neural networks. In   |
| which deep learning can be                                                      | 2013 IEEE International Conference on Acoustics, Speech and Signal       |
| applied.                                                                        |                                                                          |
| It                                                                              |                                                                          |
| is                                                                              |                                                                          |
| an open problem                                                                 |                                                                          |
| to construct defenses that are robust                                           | Processing (2013),                                                       |
| to adversarial examples.                                                        | IEEE, pp. 3422–3426.                                                     |
|                                                                                 | [9] DENG,                                                                |
|                                                                                 | J., DONG, W., SOCHER, R., LI, L.-J., LI, K., AND FEI-FEI,                |
| In                                                                              |                                                                          |
| an                                                                              |                                                                          |
| attempt                                                                         |                                                                          |
| to                                                                              |                                                                          |
| solve                                                                           |                                                                          |
| this                                                                            |                                                                          |
| problem,                                                                        |                                                                          |
| defensive                                                                       |                                                                          |
| distillation                                                                    |                                                                          |
|                                                                                 | L.                                                                       |
|                                                                                 | Imagenet: A large-scale hierarchical                                     |
|                                                                                 | image database.                                                          |
|                                                                                 | In Computer                                                              |
| was proposed as a general-purpose procedure to increase the                     |                                                                          |
|                                                                                 | Vision and Pattern Recognition, 2009. CVPR 2009.                         |
|                                                                                 | IEEE Conference                                                          |
| robustness of an arbitrary neural network.                                      | on (2009),                                                               |
|                                                                                 | IEEE, pp. 248–255.                                                       |



## Page 16

[10] GIUSTI, A., GUZZI, J., CIRES¸AN, D. C., HE, F.-L., RODR´IGUEZ,
J. P., FONTANA, F., FAESSLER, M., FORSTER, C., SCHMIDHUBER, J.,
DI CARO, G., ET AL. A machine learning approach to visual perception
of forest trails for mobile robots. IEEE Robotics and Automation Letters
1, 2 (2016), 661–667.
[11] GOODFELLOW, I. J., SHLENS, J., AND SZEGEDY, C.
Explaining
and harnessing adversarial examples. arXiv preprint arXiv:1412.6572
(2014).
[12] GRAHAM, B. Fractional max-pooling. arXiv preprint arXiv:1412.6071
(2014).
[13] GRAVES, A., MOHAMED, A.-R., AND HINTON, G. Speech recognition
with deep recurrent neural networks.
In 2013 IEEE international
conference on acoustics, speech and signal processing (2013), IEEE,
pp. 6645–6649.
[14] GROSSE, K., PAPERNOT, N., MANOHARAN, P., BACKES, M., AND
MCDANIEL, P. Adversarial perturbations against deep neural networks
for malware classiﬁcation. arXiv preprint arXiv:1606.04435 (2016).
[15] GU, S., AND RIGAZIO, L. Towards deep neural network architectures
robust to adversarial examples. arXiv preprint arXiv:1412.5068 (2014).
[16] HE, K., ZHANG, X., REN, S., AND SUN, J. Deep residual learning for
image recognition. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (2016), pp. 770–778.
[17] HINTON, G., DENG, L., YU, D., DAHL, G., RAHMAN MOHAMED, A.,
JAITLY, N., SENIOR, A., VANHOUCKE, V., NGUYEN, P., SAINATH, T.,
AND KINGSBURY, B. Deep neural networks for acoustic modeling in
speech recognition. Signal Processing Magazine (2012).
[18] HINTON, G., DENG, L., YU, D., DAHL, G. E., MOHAMED, A.-R.,
JAITLY, N., SENIOR, A., VANHOUCKE, V., NGUYEN, P., SAINATH,
T. N., ET AL. Deep neural networks for acoustic modeling in speech
recognition: The shared views of four research groups. IEEE Signal
Processing Magazine 29, 6 (2012), 82–97.
[19] HINTON, G., VINYALS, O., AND DEAN, J. Distilling the knowledge in
a neural network. arXiv preprint arXiv:1503.02531 (2015).
[20] HUANG, R., XU, B., SCHUURMANS, D., AND SZEPESV ´ARI, C. Learn-
ing with a strong adversary. CoRR, abs/1511.03034 (2015).
[21] HUANG, X., KWIATKOWSKA, M., WANG, S., AND WU, M.
Safety
veriﬁcation of deep neural networks. arXiv preprint arXiv:1610.06940
(2016).
[22] JANGLOV ´A, D. Neural networks in mobile robot motion. Cutting Edge
Robotics 1, 1 (2005), 243.
[23] KINGMA, D., AND BA, J. Adam: A method for stochastic optimization.
arXiv preprint arXiv:1412.6980 (2014).
[24] KRIZHEVSKY, A., AND HINTON, G.
Learning multiple layers of
features from tiny images.
[25] KRIZHEVSKY, A., SUTSKEVER, I., AND HINTON, G. E.
ImageNet
classiﬁcation with deep convolutional neural networks.
In Advances
in neural information processing systems (2012), pp. 1097–1105.
[26] KURAKIN, A., GOODFELLOW, I., AND BENGIO, S. Adversarial exam-
ples in the physical world. arXiv preprint arXiv:1607.02533 (2016).
[27] LECUN, Y., BOTTOU, L., BENGIO, Y., AND HAFFNER, P. Gradient-
based learning applied to document recognition.
Proceedings of the
IEEE 86, 11 (1998), 2278–2324.
[28] LECUN, Y., CORTES, C., AND BURGES, C. J. The mnist database of
handwritten digits, 1998.
[29] MAAS, A. L., HANNUN, A. Y., AND NG, A. Y. Rectiﬁer nonlinearities
improve neural network acoustic models. In Proc. ICML (2013), vol. 30.
[30] MELICHER, W., UR, B., SEGRETI, S. M., KOMANDURI, S., BAUER,
L., CHRISTIN, N., AND CRANOR, L. F.
Fast, lean and accurate:
Modeling password guessability using neural networks. In Proceedings
of USENIX Security (2016).
[31] MISHKIN, D., AND MATAS, J.
All you need is a good init.
arXiv
preprint arXiv:1511.06422 (2015).
[32] MNIH,
V.,
KAVUKCUOGLU,
K.,
SILVER,
D.,
GRAVES,
A.,
ANTONOGLOU, I., WIERSTRA, D., AND RIEDMILLER, M.
Playing
Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602
(2013).
[33] MNIH, V., KAVUKCUOGLU, K., SILVER, D., RUSU, A. A., VENESS,
J., BELLEMARE, M. G., GRAVES, A., RIEDMILLER, M., FIDJELAND,
A. K., OSTROVSKI, G., ET AL.
Human-level control through deep
reinforcement learning. Nature 518, 7540 (2015), 529–533.
[34] MOOSAVI-DEZFOOLI, S.-M., FAWZI, A., AND FROSSARD, P. Deep-
fool: a simple and accurate method to fool deep neural networks. arXiv
preprint arXiv:1511.04599 (2015).
[35] PAPERNOT, N., GOODFELLOW, I., SHEATSLEY, R., FEINMAN, R., AND
MCDANIEL, P.
cleverhans v1.0.0: an adversarial machine learning
library. arXiv preprint arXiv:1610.00768 (2016).
[36] PAPERNOT, N., AND MCDANIEL, P. On the effectiveness of defensive
distillation. arXiv preprint arXiv:1607.05113 (2016).
[37] PAPERNOT, N., MCDANIEL, P., AND GOODFELLOW, I. Transferabil-
ity in machine learning: from phenomena to black-box attacks using
adversarial samples. arXiv preprint arXiv:1605.07277 (2016).
[38] PAPERNOT, N., MCDANIEL, P., JHA, S., FREDRIKSON, M., CELIK,
Z. B., AND SWAMI, A. The limitations of deep learning in adversarial
settings. In 2016 IEEE European Symposium on Security and Privacy
(EuroS&P) (2016), IEEE, pp. 372–387.
[39] PAPERNOT, N., MCDANIEL, P., WU, X., JHA, S., AND SWAMI, A.
Distillation as a defense to adversarial perturbations against deep neural
networks. IEEE Symposium on Security and Privacy (2016).
[40] PASCANU, R., STOKES, J. W., SANOSSIAN, H., MARINESCU, M.,
AND THOMAS, A. Malware classiﬁcation with recurrent networks. In
2015 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP) (2015), IEEE, pp. 1916–1920.
[41] RUSSAKOVSKY, O., DENG, J., SU, H., KRAUSE, J., SATHEESH, S.,
MA, S., HUANG, Z., KARPATHY, A., KHOSLA, A., BERNSTEIN, M.,
BERG, A. C., AND FEI-FEI, L. ImageNet Large Scale Visual Recogni-
tion Challenge. International Journal of Computer Vision (IJCV) 115,
3 (2015), 211–252.
[42] SHAHAM, U., YAMADA, Y., AND NEGAHBAN, S.
Understanding
adversarial training: Increasing local stability of neural nets through
robust optimization. arXiv preprint arXiv:1511.05432 (2015).
[43] SILVER, D., HUANG, A., MADDISON, C. J., GUEZ, A., SIFRE, L.,
VAN DEN DRIESSCHE, G., SCHRITTWIESER, J., ANTONOGLOU, I.,
PANNEERSHELVAM, V., LANCTOT, M., ET AL.
Mastering the game
of Go with deep neural networks and tree search. Nature 529, 7587
(2016), 484–489.
[44] SPRINGENBERG, J. T., DOSOVITSKIY, A., BROX, T., AND RIED-
MILLER, M. Striving for simplicity: The all convolutional net. arXiv
preprint arXiv:1412.6806 (2014).
[45] SZEGEDY, C., VANHOUCKE, V., IOFFE, S., SHLENS, J., AND WOJNA,
Z. Rethinking the Inception architecture for computer vision. arXiv
preprint arXiv:1512.00567 (2015).
[46] SZEGEDY, C., ZAREMBA, W., SUTSKEVER, I., BRUNA, J., ERHAN,
D., GOODFELLOW, I., AND FERGUS, R. Intriguing properties of neural
networks. ICLR (2013).
[47] WARDE-FARLEY, D., AND GOODFELLOW, I. Adversarial perturbations
of deep neural networks. Advanced Structured Prediction, T. Hazan, G.
Papandreou, and D. Tarlow, Eds (2016).
[48] YUAN, Z., LU, Y., WANG, Z., AND XUE, Y. Droid-sec: Deep learning
in android malware detection. In ACM SIGCOMM Computer Commu-
nication Review (2014), vol. 44, ACM, pp. 371–372.
APPENDIX
16


**Table 23 from page 16**

| 0                                                                       | 1                                                                          |
|:------------------------------------------------------------------------|:---------------------------------------------------------------------------|
| [10] GIUSTI, A., GUZZI,                                                 | [32] MNIH,                                                                 |
| J., CIRES¸ AN, D. C., HE, F.-L., RODR´IGUEZ,                            | V.,                                                                        |
|                                                                         | KAVUKCUOGLU,                                                               |
|                                                                         | K.,                                                                        |
|                                                                         | SILVER,                                                                    |
|                                                                         | D.,                                                                        |
|                                                                         | GRAVES,                                                                    |
|                                                                         | A.,                                                                        |
| J. P., FONTANA, F., FAESSLER, M., FORSTER, C., SCHMIDHUBER, J.,         | ANTONOGLOU,                                                                |
|                                                                         | I., WIERSTRA, D., AND RIEDMILLER, M.                                       |
|                                                                         | Playing                                                                    |
| DI CARO, G., ET AL. A machine learning approach to visual perception    | Atari with deep reinforcement                                              |
|                                                                         | learning. arXiv preprint arXiv:1312.5602                                   |
| IEEE Robotics and Automation Letters                                    | (2013).                                                                    |
| of forest trails for mobile robots.                                     |                                                                            |
| 1, 2 (2016), 661–667.                                                   | [33] MNIH, V., KAVUKCUOGLU, K., SILVER, D., RUSU, A. A., VENESS,           |
| [11] GOODFELLOW,                                                        | J., BELLEMARE, M. G., GRAVES, A., RIEDMILLER, M., FIDJELAND,               |
| I.                                                                      |                                                                            |
| J., SHLENS,                                                             |                                                                            |
| J., AND SZEGEDY, C.                                                     |                                                                            |
| Explaining                                                              |                                                                            |
| arXiv preprint arXiv:1412.6572                                          | A. K., OSTROVSKI, G., ET AL.                                               |
| and harnessing adversarial examples.                                    | Human-level                                                                |
|                                                                         | control                                                                    |
|                                                                         | through                                                                    |
|                                                                         | deep                                                                       |
| (2014).                                                                 | reinforcement                                                              |
|                                                                         | learning. Nature 518, 7540 (2015), 529–533.                                |
| [12] GRAHAM, B. Fractional max-pooling. arXiv preprint arXiv:1412.6071  | [34] MOOSAVI-DEZFOOLI, S.-M., FAWZI, A., AND FROSSARD, P.                  |
|                                                                         | Deep-                                                                      |
| (2014).                                                                 | fool: a simple and accurate method to fool deep neural networks. arXiv     |
| [13] GRAVES, A., MOHAMED, A.-R., AND HINTON, G. Speech recognition      | preprint arXiv:1511.04599 (2015).                                          |
| 2013                                                                    | [35]                                                                       |
| IEEE international                                                      | PAPERNOT, N., GOODFELLOW, I., SHEATSLEY, R., FEINMAN, R., AND              |
| with                                                                    |                                                                            |
| deep                                                                    |                                                                            |
| recurrent                                                               |                                                                            |
| neural                                                                  |                                                                            |
| networks.                                                               |                                                                            |
| In                                                                      |                                                                            |
| conference on acoustics,                                                | MCDANIEL, P.                                                               |
| speech and signal processing (2013),                                    | cleverhans                                                                 |
| IEEE,                                                                   | v1.0.0:                                                                    |
|                                                                         | an                                                                         |
|                                                                         | adversarial machine                                                        |
|                                                                         | learning                                                                   |
| pp. 6645–6649.                                                          | library. arXiv preprint arXiv:1610.00768 (2016).                           |
| [14] GROSSE, K., PAPERNOT, N., MANOHARAN, P., BACKES, M., AND           | [36]                                                                       |
|                                                                         | PAPERNOT, N., AND MCDANIEL, P. On the effectiveness of defensive           |
| MCDANIEL, P. Adversarial perturbations against deep neural networks     | distillation. arXiv preprint arXiv:1607.05113 (2016).                      |
| for malware classiﬁcation. arXiv preprint arXiv:1606.04435 (2016).      | [37]                                                                       |
|                                                                         | PAPERNOT, N., MCDANIEL, P., AND GOODFELLOW,                                |
|                                                                         | I.                                                                         |
|                                                                         | Transferabil-                                                              |
| [15] GU, S., AND RIGAZIO, L. Towards deep neural network architectures  | ity in machine                                                             |
|                                                                         | learning:                                                                  |
|                                                                         | from phenomena                                                             |
|                                                                         | to black-box attacks using                                                 |
| robust                                                                  | adversarial samples. arXiv preprint arXiv:1605.07277 (2016).               |
| to adversarial examples. arXiv preprint arXiv:1412.5068 (2014).         |                                                                            |
| [16] HE, K., ZHANG, X., REN, S., AND SUN, J. Deep residual              | [38]                                                                       |
| learning for                                                            | PAPERNOT, N., MCDANIEL, P.,                                                |
|                                                                         | JHA, S., FREDRIKSON, M., CELIK,                                            |
| image recognition. In Proceedings of the IEEE Conference on Computer    | Z. B., AND SWAMI, A. The limitations of deep learning in adversarial       |
| Vision and Pattern Recognition (2016), pp. 770–778.                     | settings.                                                                  |
|                                                                         | In 2016 IEEE European Symposium on Security and Privacy                    |
| [17] HINTON, G., DENG, L., YU, D., DAHL, G., RAHMAN MOHAMED, A.,        | (EuroS&P)                                                                  |
|                                                                         | (2016),                                                                    |
|                                                                         | IEEE, pp. 372–387.                                                         |
| JAITLY, N., SENIOR, A., VANHOUCKE, V., NGUYEN, P., SAINATH, T.,         |                                                                            |
|                                                                         | [39]                                                                       |
|                                                                         | PAPERNOT, N., MCDANIEL, P., WU, X.,                                        |
|                                                                         | JHA, S., AND SWAMI, A.                                                     |
| AND KINGSBURY, B. Deep neural networks                                  |                                                                            |
| for acoustic modeling in                                                |                                                                            |
|                                                                         | Distillation as a defense to adversarial perturbations against deep neural |
| speech recognition. Signal Processing Magazine (2012).                  |                                                                            |
|                                                                         | networks.                                                                  |
|                                                                         | IEEE Symposium on Security and Privacy (2016).                             |
| [18] HINTON, G., DENG, L., YU, D., DAHL, G. E., MOHAMED, A.-R.,         |                                                                            |
|                                                                         | [40]                                                                       |
|                                                                         | PASCANU, R., STOKES,                                                       |
|                                                                         | J. W., SANOSSIAN, H., MARINESCU, M.,                                       |
| JAITLY, N., SENIOR, A., VANHOUCKE, V., NGUYEN, P., SAINATH,             |                                                                            |
|                                                                         | AND THOMAS, A. Malware classiﬁcation with recurrent networks.              |
|                                                                         | In                                                                         |
| T. N., ET AL. Deep neural networks                                      |                                                                            |
| for acoustic modeling in speech                                         |                                                                            |
|                                                                         | 2015 IEEE International Conference on Acoustics, Speech and Signal         |
| IEEE Signal                                                             |                                                                            |
| recognition: The                                                        |                                                                            |
| shared views of                                                         |                                                                            |
| four                                                                    |                                                                            |
| research groups.                                                        |                                                                            |
|                                                                         | Processing (ICASSP)                                                        |
|                                                                         | (2015),                                                                    |
|                                                                         | IEEE, pp. 1916–1920.                                                       |
| Processing Magazine 29, 6 (2012), 82–97.                                |                                                                            |
|                                                                         | [41] RUSSAKOVSKY, O., DENG,                                                |
|                                                                         | J., SU, H., KRAUSE,                                                        |
|                                                                         | J., SATHEESH, S.,                                                          |
| [19] HINTON, G., VINYALS, O., AND DEAN, J. Distilling the knowledge in  |                                                                            |
|                                                                         | MA, S., HUANG, Z., KARPATHY, A., KHOSLA, A., BERNSTEIN, M.,                |
| a neural network. arXiv preprint arXiv:1503.02531 (2015).               |                                                                            |
|                                                                         | BERG, A. C., AND FEI-FEI, L.                                               |
|                                                                         | ImageNet Large Scale Visual Recogni-                                       |
| [20] HUANG, R., XU, B., SCHUURMANS, D., AND SZEPESV ´ARI, C. Learn-     |                                                                            |
|                                                                         | tion Challenge.                                                            |
|                                                                         | International Journal of Computer Vision (IJCV) 115,                       |
| ing with a strong adversary. CoRR, abs/1511.03034 (2015).               |                                                                            |
|                                                                         | 3 (2015), 211–252.                                                         |
| [21] HUANG, X., KWIATKOWSKA, M., WANG, S., AND WU, M.                   |                                                                            |
| Safety                                                                  |                                                                            |
|                                                                         | [42]                                                                       |
|                                                                         | SHAHAM, U., YAMADA, Y., AND NEGAHBAN, S.                                   |
|                                                                         | Understanding                                                              |
| arXiv preprint arXiv:1610.06940                                         |                                                                            |
| veriﬁcation of deep neural networks.                                    |                                                                            |
|                                                                         | adversarial                                                                |
|                                                                         | training:                                                                  |
|                                                                         | Increasing                                                                 |
|                                                                         | local                                                                      |
|                                                                         | stability                                                                  |
|                                                                         | of                                                                         |
|                                                                         | neural                                                                     |
|                                                                         | nets                                                                       |
|                                                                         | through                                                                    |
| (2016).                                                                 |                                                                            |
|                                                                         | robust optimization. arXiv preprint arXiv:1511.05432 (2015).               |
| [22]                                                                    |                                                                            |
| JANGLOV ´A, D. Neural networks in mobile robot motion. Cutting Edge     |                                                                            |
|                                                                         | [43]                                                                       |
|                                                                         | SILVER, D., HUANG, A., MADDISON, C.                                        |
|                                                                         | J., GUEZ, A., SIFRE, L.,                                                   |
| Robotics 1, 1 (2005), 243.                                              |                                                                            |
|                                                                         | VAN DEN DRIESSCHE, G., SCHRITTWIESER,                                      |
|                                                                         | J., ANTONOGLOU,                                                            |
|                                                                         | I.,                                                                        |
| [23] KINGMA, D., AND BA, J. Adam: A method for stochastic optimization. |                                                                            |
|                                                                         | PANNEERSHELVAM, V., LANCTOT, M., ET AL. Mastering the game                 |
| arXiv preprint arXiv:1412.6980 (2014).                                  |                                                                            |
|                                                                         | of Go with deep neural networks and tree search.                           |
|                                                                         | Nature 529, 7587                                                           |
| [24] KRIZHEVSKY, A., AND HINTON, G.                                     |                                                                            |
| Learning multiple                                                       |                                                                            |
| layers                                                                  |                                                                            |
| of                                                                      |                                                                            |
|                                                                         | (2016), 484–489.                                                           |
| features from tiny images.                                              |                                                                            |
|                                                                         | [44]                                                                       |
|                                                                         | SPRINGENBERG,                                                              |
|                                                                         | J. T., DOSOVITSKIY, A., BROX, T., AND RIED-                                |
| [25] KRIZHEVSKY, A., SUTSKEVER,                                         |                                                                            |
| I., AND HINTON, G. E.                                                   |                                                                            |
| ImageNet                                                                |                                                                            |
|                                                                         | arXiv                                                                      |
|                                                                         | MILLER, M.                                                                 |
|                                                                         | Striving for                                                               |
|                                                                         | simplicity: The all convolutional net.                                     |
| classiﬁcation with deep convolutional neural networks.                  |                                                                            |
| In Advances                                                             |                                                                            |
|                                                                         | preprint arXiv:1412.6806 (2014).                                           |
| in neural                                                               |                                                                            |
| information processing systems (2012), pp. 1097–1105.                   |                                                                            |
|                                                                         | [45]                                                                       |
|                                                                         | SZEGEDY, C., VANHOUCKE, V., IOFFE, S., SHLENS, J., AND WOJNA,              |
| [26] KURAKIN, A., GOODFELLOW, I., AND BENGIO, S. Adversarial exam-      |                                                                            |
|                                                                         | arXiv                                                                      |
|                                                                         | Z.                                                                         |
|                                                                         | Rethinking the                                                             |
|                                                                         | Inception architecture                                                     |
|                                                                         | for                                                                        |
|                                                                         | computer vision.                                                           |
| ples in the physical world. arXiv preprint arXiv:1607.02533 (2016).     |                                                                            |
|                                                                         | preprint arXiv:1512.00567 (2015).                                          |
| [27]                                                                    |                                                                            |
| LECUN, Y., BOTTOU, L., BENGIO, Y., AND HAFFNER, P.                      |                                                                            |
| Gradient-                                                               |                                                                            |
|                                                                         | [46]                                                                       |
|                                                                         | SZEGEDY, C., ZAREMBA, W., SUTSKEVER,                                       |
|                                                                         | I., BRUNA,                                                                 |
|                                                                         | J., ERHAN,                                                                 |
| Proceedings of                                                          |                                                                            |
| the                                                                     |                                                                            |
| based learning applied to document                                      |                                                                            |
| recognition.                                                            |                                                                            |
|                                                                         | D., GOODFELLOW, I., AND FERGUS, R.                                         |
|                                                                         | Intriguing properties of neural                                            |
| IEEE 86, 11 (1998), 2278–2324.                                          |                                                                            |
|                                                                         | networks.                                                                  |
|                                                                         | ICLR (2013).                                                               |
| [28]                                                                    |                                                                            |
| LECUN, Y., CORTES, C., AND BURGES, C. J.                                |                                                                            |
| The mnist database of                                                   |                                                                            |
|                                                                         | [47] WARDE-FARLEY, D., AND GOODFELLOW, I. Adversarial perturbations        |
| handwritten digits, 1998.                                               |                                                                            |
|                                                                         | of deep neural networks. Advanced Structured Prediction, T. Hazan, G.      |
| [29] MAAS, A. L., HANNUN, A. Y., AND NG, A. Y. Rectiﬁer nonlinearities  |                                                                            |
|                                                                         | Papandreou, and D. Tarlow, Eds (2016).                                     |
| improve neural network acoustic models. In Proc. ICML (2013), vol. 30.  |                                                                            |
|                                                                         | [48] YUAN, Z., LU, Y., WANG, Z., AND XUE, Y. Droid-sec: Deep learning      |
| [30] MELICHER, W., UR, B., SEGRETI, S. M., KOMANDURI, S., BAUER,        |                                                                            |
|                                                                         | in android malware detection.                                              |
|                                                                         | In ACM SIGCOMM Computer Commu-                                             |
| L., CHRISTIN, N., AND CRANOR, L. F.                                     |                                                                            |
| Fast,                                                                   |                                                                            |
| lean                                                                    |                                                                            |
| and                                                                     |                                                                            |
| accurate:                                                               |                                                                            |
|                                                                         | nication Review (2014), vol. 44, ACM, pp. 371–372.                         |
| Modeling password guessability using neural networks.                   |                                                                            |
| In Proceedings                                                          |                                                                            |
| of USENIX Security (2016).                                              |                                                                            |
| arXiv                                                                   |                                                                            |
| [31] MISHKIN, D., AND MATAS,                                            |                                                                            |
| J.                                                                      |                                                                            |
| All you need is                                                         |                                                                            |
| a good init.                                                            |                                                                            |
|                                                                         | APPENDIX                                                                   |
| preprint arXiv:1511.06422 (2015).                                       |                                                                            |



## Page 17

Target Classiﬁcation (L0)
0
1
2
3
4
5
6
7
8
9
Source Classiﬁcation
0
1
2
3
4
5
6
7
8
9
Fig. 11. Our L0 adversary applied to the CIFAR dataset performing a targeted attack for every source/target pair. Each image is the ﬁrst image in the dataset
with that label.
17


**Table 24 from page 17**

| 0                                                                                                                                        | 1                    |
|:-----------------------------------------------------------------------------------------------------------------------------------------|:---------------------|
| Target Classiﬁcation (L0)                                                                                                                |                      |
| 0                                                                                                                                        | 9                    |
| 1                                                                                                                                        |                      |
| 2                                                                                                                                        |                      |
| 3                                                                                                                                        |                      |
| 4                                                                                                                                        |                      |
| 5                                                                                                                                        |                      |
| 6                                                                                                                                        |                      |
| 7                                                                                                                                        |                      |
| 8                                                                                                                                        |                      |
| 9                                                                                                                                        |                      |
| 8                                                                                                                                        |                      |
| 7                                                                                                                                        |                      |
| 6                                                                                                                                        |                      |
| 5                                                                                                                                        |                      |
| SourceClassiﬁcation                                                                                                                      |                      |
| 4                                                                                                                                        |                      |
| 3                                                                                                                                        |                      |
| 2                                                                                                                                        |                      |
| 1                                                                                                                                        |                      |
| 0                                                                                                                                        |                      |
| Fig. 11. Our L0 adversary applied to the CIFAR dataset performing a targeted attack for every source/target pair. Each image is the ﬁrst | image in the dataset |
| with that                                                                                                                                |                      |
| label.                                                                                                                                   |                      |
| 17                                                                                                                                       |                      |



## Page 18

Target Classiﬁcation (L2)
0
1
2
3
4
5
6
7
8
9
Source Classiﬁcation
0
1
2
3
4
5
6
7
8
9
Fig. 12. Our L2 adversary applied to the CIFAR dataset performing a targeted attack for every source/target pair. Each image is the ﬁrst image in the dataset
with that label.
18


**Table 25 from page 18**

| 0                                                                                                                                        | 1                    |
|:-----------------------------------------------------------------------------------------------------------------------------------------|:---------------------|
| Target Classiﬁcation (L2)                                                                                                                |                      |
| 0                                                                                                                                        | 9                    |
| 1                                                                                                                                        |                      |
| 2                                                                                                                                        |                      |
| 3                                                                                                                                        |                      |
| 4                                                                                                                                        |                      |
| 5                                                                                                                                        |                      |
| 6                                                                                                                                        |                      |
| 7                                                                                                                                        |                      |
| 8                                                                                                                                        |                      |
| 9                                                                                                                                        |                      |
| 8                                                                                                                                        |                      |
| 7                                                                                                                                        |                      |
| 6                                                                                                                                        |                      |
| 5                                                                                                                                        |                      |
| SourceClassiﬁcation                                                                                                                      |                      |
| 4                                                                                                                                        |                      |
| 3                                                                                                                                        |                      |
| 2                                                                                                                                        |                      |
| 1                                                                                                                                        |                      |
| 0                                                                                                                                        |                      |
| Fig. 12. Our L2 adversary applied to the CIFAR dataset performing a targeted attack for every source/target pair. Each image is the ﬁrst | image in the dataset |
| with that                                                                                                                                |                      |
| label.                                                                                                                                   |                      |
| 18                                                                                                                                       |                      |



## Page 19

Target Classiﬁcation (L∞)
0
1
2
3
4
5
6
7
8
9
Source Classiﬁcation
0
1
2
3
4
5
6
7
8
9
Fig. 13. Our L∞adversary applied to the CIFAR dataset performing a targeted attack for every source/target pair. Each image is the ﬁrst image in the dataset
with that label.
19


**Table 26 from page 19**

| 0                                                                                                                                        | 1                    |
|:-----------------------------------------------------------------------------------------------------------------------------------------|:---------------------|
| Target Classiﬁcation (L∞)                                                                                                                |                      |
| 0                                                                                                                                        | 9                    |
| 1                                                                                                                                        |                      |
| 2                                                                                                                                        |                      |
| 3                                                                                                                                        |                      |
| 4                                                                                                                                        |                      |
| 5                                                                                                                                        |                      |
| 6                                                                                                                                        |                      |
| 7                                                                                                                                        |                      |
| 8                                                                                                                                        |                      |
| 9                                                                                                                                        |                      |
| 8                                                                                                                                        |                      |
| 7                                                                                                                                        |                      |
| 6                                                                                                                                        |                      |
| 5                                                                                                                                        |                      |
| SourceClassiﬁcation                                                                                                                      |                      |
| 4                                                                                                                                        |                      |
| 3                                                                                                                                        |                      |
| 2                                                                                                                                        |                      |
| 1                                                                                                                                        |                      |
| 0                                                                                                                                        |                      |
| Fig. 13. Our L∞ adversary applied to the CIFAR dataset performing a targeted attack for every source/target pair. Each image is the ﬁrst | image in the dataset |
| with that                                                                                                                                |                      |
| label.                                                                                                                                   |                      |
| 19                                                                                                                                       |                      |

